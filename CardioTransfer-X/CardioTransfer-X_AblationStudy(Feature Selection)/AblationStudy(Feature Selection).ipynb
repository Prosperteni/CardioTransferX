{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4bcfb71-06cb-4875-a056-d0c6e7a1e03f",
   "metadata": {},
   "source": [
    "# <font size=5> <strong>Heart Disease Prediction\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f549498-ec6a-486b-a9fa-8bc07bd6aa5b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35d3eb7e-461a-4535-821a-ef64396d461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import(accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve,\n",
    "                            auc, confusion_matrix, classification_report, make_scorer)\n",
    "\n",
    "# Modeling – XGBoost and TabNet\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "# Hyperparameter optimization\n",
    "import optuna\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Explainability\n",
    "import shap\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
    "import keras_tuner as kt\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b206114-e6e3-4019-81d2-2341db5e6a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a94f5b-532a-4c85-b045-0754856c43e1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed5e14-1c39-4cd1-b6a0-f7175bdf4f3d",
   "metadata": {},
   "source": [
    "### Importing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cae88cdd-d479-4855-837e-85e887c863f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A = pd.read_csv(\"Cleveland+Hungary+VA_long_beach+Switzerland.csv\") # Source domain = multi-hospital dataset\n",
    "df_B = pd.read_csv(\"Heart_disease_cleveland.csv\")                     # Target domain = original Cleveland dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4042b1b-b974-4412-9a46-ad17daa5779f",
   "metadata": {},
   "source": [
    "### Exploring and Inspecting Data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "154cc2bf-576d-4b87-9108-5892e1736f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 919 entries, 0 to 918\n",
      "Data columns (total 16 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   id        919 non-null    int64  \n",
      " 1   age       919 non-null    int64  \n",
      " 2   sex       919 non-null    object \n",
      " 3   dataset   919 non-null    object \n",
      " 4   cp        919 non-null    object \n",
      " 5   trestbps  919 non-null    float64\n",
      " 6   chol      919 non-null    float64\n",
      " 7   fbs       919 non-null    bool   \n",
      " 8   restecg   919 non-null    object \n",
      " 9   thalch    919 non-null    float64\n",
      " 10  exang     919 non-null    bool   \n",
      " 11  oldpeak   919 non-null    float64\n",
      " 12  slope     919 non-null    object \n",
      " 13  ca        919 non-null    float64\n",
      " 14  thal      919 non-null    object \n",
      " 15  num       919 non-null    int64  \n",
      "dtypes: bool(2), float64(5), int64(3), object(6)\n",
      "memory usage: 102.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_A.info() # Displays concise summary of DataFrame A: index range, column names, non-null counts, and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4569bfa4-2b5a-4e51-b930-0695fb312a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       303 non-null    int64  \n",
      " 1   sex       303 non-null    int64  \n",
      " 2   cp        303 non-null    int64  \n",
      " 3   trestbps  303 non-null    int64  \n",
      " 4   chol      303 non-null    int64  \n",
      " 5   fbs       303 non-null    int64  \n",
      " 6   restecg   303 non-null    int64  \n",
      " 7   thalach   303 non-null    int64  \n",
      " 8   exang     303 non-null    int64  \n",
      " 9   oldpeak   303 non-null    float64\n",
      " 10  slope     303 non-null    int64  \n",
      " 11  ca        303 non-null    int64  \n",
      " 12  thal      303 non-null    int64  \n",
      " 13  target    303 non-null    int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 33.3 KB\n"
     ]
    }
   ],
   "source": [
    "df_B.info() # Displays concise summary of DataFrame B: index range, column names, non-null counts, and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2df8ede-ec12-435d-96b3-69ea3f3ba6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Cleveland data from df_A before pretraining\n",
    "df_A = df_A[df_A['dataset'] != 'Cleveland']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2f871dd-ca19-4a0c-a2f0-5bf65455002d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'age', 'sex', 'dataset', 'cp', 'trestbps', 'chol', 'fbs',\n",
      "       'restecg', 'thalch', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num'],\n",
      "      dtype='object')\n",
      "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
      "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_A.columns) # Prints the list of column names in DataFrame A.\n",
    "print(df_B.columns) # Prints the list of column names in DataFrame B."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3edb2754-fbd5-44da-a9a9-310d6f34d472",
   "metadata": {},
   "source": [
    "1. We inspected the datasets using df.info(), df.columns, and df.head().\n",
    "#    Observations from Dataset A (combined multi-hospital dataset):\n",
    "- Many categorical columns (sex, cp, fbs, restecg, slope, thal, exang) are strings/booleans.\n",
    "- Some numeric columns have missing values (trestbps, chol, thalach, oldpeak, ca).\n",
    "- Column names differ from Dataset B (e.g., 'thalch' vs 'thalach', 'num' vs 'target').\n",
    "- Extra columns exist (e.g., 'id', 'dataset') that are irrelevant for modeling.\n",
    "- The target column uses 0–4 scale instead of 0/1 like Dataset B.\n",
    "\n",
    "#    Observations from Dataset B (Cleveland dataset):\n",
    "- All categorical columns are already numeric (int64), matching expected encoding.\n",
    "- No missing values.\n",
    "- Target column is binary (0=no disease, 1=disease).\n",
    "\n",
    "2. Based on these observations, the following harmonization steps are necessary:\n",
    "- Rename columns in Dataset A to match Dataset B.\n",
    "- Drop irrelevant columns in Dataset A (id, dataset).\n",
    "- Map all categorical strings/booleans in Dataset A to numeric codes matching Dataset B.\n",
    "- Fill missing values: \n",
    "#           • Categorical → mode of column\n",
    "#           • Numeric → median of column\n",
    "- Convert target column in Dataset A to binary (0=no disease, 1=disease)\n",
    "- Align features so both datasets have identical column names and encodings.\n",
    "\n",
    "3. Purpose:\n",
    "- Ensure that Dataset A (pretraining) and Dataset B (fine-tuning) are fully compatible.\n",
    "- Prevent encoding mismatches that would break transfer learning.\n",
    "- Guarantee that the XGBoost model receives numeric input for all features.\n",
    "- Maintain consistency for feature importance and SHAP explainability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "856b1cf3-d252-4c47-a02e-1450c084d0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A = df_A.rename(columns={'thalch': 'thalach','num': 'target'}) #Rename columns in Dataset A to match Dataset B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a0bc1c1-cea8-40ab-b12e-5e694f524027",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A = df_A.drop(columns=['id', 'dataset']) #Drop irrelevant columns in Dataset A (id, dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1915aeb1-8cdf-4f58-a0a6-20c303b2a66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target column in Dataset A to binary (0=no disease, 1=disease)\n",
    "df_A['target'] = df_A['target'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ae38850-7e49-4efc-b2e2-ccb20ca3da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map all categorical strings/booleans in Dataset A to numeric codes matching the target domain.\n",
    "sex_map = {'Male': 1, 'Female': 0}\n",
    "\n",
    "cp_map = {\n",
    "    'typical angina': 0,\n",
    "    'atypical angina': 1,\n",
    "    'non-anginal': 2,\n",
    "    'asymptomatic': 3\n",
    "}\n",
    "\n",
    "fbs_map = {True: 1, False: 0}\n",
    "\n",
    "restecg_map = {\n",
    "    'normal': 0,\n",
    "    'st-t abnormality': 1,\n",
    "    'lv hypertrophy': 2\n",
    "}\n",
    "\n",
    "exang_map = {True: 1, False: 0}\n",
    "\n",
    "slope_map = {\n",
    "    'upsloping': 0,\n",
    "    'flat': 1,\n",
    "    'downsloping': 2\n",
    "}\n",
    "\n",
    "thal_map = {\n",
    "    'normal': 1,\n",
    "    'fixed defect': 2,\n",
    "    'reversable defect': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a2ac638-c3d6-4ad2-b138-6de8b87454a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align features so both datasets have identical column names and encodings.\n",
    "df_A['sex'] = df_A['sex'].map(sex_map)\n",
    "df_A['cp'] = df_A['cp'].map(cp_map)\n",
    "df_A['fbs'] = df_A['fbs'].map(fbs_map)\n",
    "df_A['restecg'] = df_A['restecg'].map(restecg_map)\n",
    "df_A['exang'] = df_A['exang'].map(exang_map)\n",
    "df_A['slope'] = df_A['slope'].map(slope_map)\n",
    "df_A['thal'] = df_A['thal'].map(thal_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e875d2bd-fdd7-4f55-98d3-b73e4b58589c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age         0\n",
      "sex         0\n",
      "cp          0\n",
      "trestbps    0\n",
      "chol        0\n",
      "fbs         0\n",
      "restecg     0\n",
      "thalach     0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "slope       0\n",
      "ca          0\n",
      "thal        0\n",
      "target      0\n",
      "dtype: int64\n",
      "age         0\n",
      "sex         0\n",
      "cp          0\n",
      "trestbps    0\n",
      "chol        0\n",
      "fbs         0\n",
      "restecg     0\n",
      "thalach     0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "slope       0\n",
      "ca          0\n",
      "thal        0\n",
      "target      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Final Check (Both should return zero missing values.)\n",
    "print(df_A.isnull().sum())\n",
    "print(df_B.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ed95ab7-b7dd-4f15-be3c-fb4b6bf2c5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 615 entries, 304 to 918\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       615 non-null    int64  \n",
      " 1   sex       615 non-null    int64  \n",
      " 2   cp        615 non-null    int64  \n",
      " 3   trestbps  615 non-null    float64\n",
      " 4   chol      615 non-null    float64\n",
      " 5   fbs       615 non-null    int64  \n",
      " 6   restecg   615 non-null    int64  \n",
      " 7   thalach   615 non-null    float64\n",
      " 8   exang     615 non-null    int64  \n",
      " 9   oldpeak   615 non-null    float64\n",
      " 10  slope     615 non-null    int64  \n",
      " 11  ca        615 non-null    float64\n",
      " 12  thal      615 non-null    int64  \n",
      " 13  target    615 non-null    int64  \n",
      "dtypes: float64(5), int64(9)\n",
      "memory usage: 72.1 KB\n"
     ]
    }
   ],
   "source": [
    "#Do a final check for the target domain to ensure all columns are non-null.\n",
    "df_A.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5bd4404-68aa-497c-8c21-7b57d80c56a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       303 non-null    int64  \n",
      " 1   sex       303 non-null    int64  \n",
      " 2   cp        303 non-null    int64  \n",
      " 3   trestbps  303 non-null    int64  \n",
      " 4   chol      303 non-null    int64  \n",
      " 5   fbs       303 non-null    int64  \n",
      " 6   restecg   303 non-null    int64  \n",
      " 7   thalach   303 non-null    int64  \n",
      " 8   exang     303 non-null    int64  \n",
      " 9   oldpeak   303 non-null    float64\n",
      " 10  slope     303 non-null    int64  \n",
      " 11  ca        303 non-null    int64  \n",
      " 12  thal      303 non-null    int64  \n",
      " 13  target    303 non-null    int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 33.3 KB\n"
     ]
    }
   ],
   "source": [
    "#Do a final check for the source domain to ensure all columns are non-null. \n",
    "df_B.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d65ebbf-ecdc-4821-9ae1-fd383ac64e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDatasets after preprocessing:\u001b[0m\n",
      "\u001b[1mMulti-hospital Dataset:\u001b[0m\n",
      "\u001b[1m            304     305    306\n",
      "age        29.0   29.00   30.0\n",
      "sex         1.0    1.00    0.0\n",
      "cp          1.0    1.00    0.0\n",
      "trestbps  120.0  140.00  170.0\n",
      "chol      243.0  240.48  237.0\n",
      "fbs         0.0    0.00    0.0\n",
      "restecg     0.0    0.00    1.0\n",
      "thalach   160.0  170.00  170.0\n",
      "exang       0.0    0.00    0.0\n",
      "oldpeak     0.0    0.00    0.0\n",
      "slope       0.0    0.00    0.0\n",
      "ca          0.0    0.00    0.0\n",
      "thal        1.0    1.00    2.0\n",
      "target      0.0    0.00    0.0\u001b[0m\n",
      "\n",
      "\u001b[1mCleveland Dataset:\u001b[0m\n",
      "\u001b[1m              0      1      2\n",
      "age        63.0   67.0   67.0\n",
      "sex         1.0    1.0    1.0\n",
      "cp          0.0    3.0    3.0\n",
      "trestbps  145.0  160.0  120.0\n",
      "chol      233.0  286.0  229.0\n",
      "fbs         1.0    0.0    0.0\n",
      "restecg     2.0    2.0    2.0\n",
      "thalach   150.0  108.0  129.0\n",
      "exang       0.0    1.0    1.0\n",
      "oldpeak     2.3    1.5    2.6\n",
      "slope       2.0    1.0    1.0\n",
      "ca          0.0    3.0    2.0\n",
      "thal        2.0    1.0    3.0\n",
      "target      0.0    1.0    1.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "BOLD = '\\033[1m'\n",
    "END = '\\033[0m'\n",
    "\n",
    "print(f\"{BOLD}Datasets after preprocessing:{END}\")\n",
    "\n",
    "print(f\"{BOLD}Multi-hospital Dataset:{END}\")\n",
    "print(f\"{BOLD}{df_A.head(3).T.to_string(line_width=1000)}{END}\")\n",
    "\n",
    "print(f\"\\n{BOLD}Cleveland Dataset:{END}\")\n",
    "print(f\"{BOLD}{df_B.head(3).T.to_string(line_width=1000)}{END}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3159229f-e4d9-4664-8d8f-e1580b957da5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d69b4881-6507-4e0d-97c5-f4669664b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pretrain = df_A.drop('target', axis=1) \n",
    "y_pretrain = df_A['target']               \n",
    "\n",
    "# Split the source domain into 80% train and 20% temp (for val+test)\n",
    "X_pretrain_train, X_pretrain_temp, y_pretrain_train, y_pretrain_temp = train_test_split(\n",
    "    X_pretrain, y_pretrain,\n",
    "    test_size=0.2,          # 20% goes to temp (val+test)\n",
    "    random_state=42,\n",
    "    stratify=y_pretrain\n",
    ")\n",
    "\n",
    "# Split temp into 50% validation and 50% test → each 10% of total\n",
    "X_pretrain_val, X_pretrain_test, y_pretrain_val, y_pretrain_test = train_test_split(\n",
    "    X_pretrain_temp, y_pretrain_temp,\n",
    "    test_size=0.5,          # half of temp = 10% of total\n",
    "    random_state=42,\n",
    "    stratify=y_pretrain_temp\n",
    ")\n",
    "\n",
    "# Target domain: Cleveland dataset for FINE-TUNING and evaluation\n",
    "X_finetune = df_B.drop('target', axis=1)\n",
    "y_finetune = df_B['target']\n",
    "\n",
    "\n",
    "# Split target domain into 80% train and 20% temp (for val+test)\n",
    "X_finetune_train, X_finetune_temp, y_finetune_train, y_finetune_temp = train_test_split(\n",
    "    X_finetune, y_finetune,\n",
    "    test_size=0.2,          # 20% goes to temp (val+test) \n",
    "    random_state=42,        # reproducible splits\n",
    "    stratify=y_finetune     # preserve class distribution\n",
    ")\n",
    "\n",
    "# Split temp into 50% validation and 50% test → each 10% of total\n",
    "X_finetune_val, X_finetune_test, y_finetune_val, y_finetune_test = train_test_split(\n",
    "    X_finetune_temp, y_finetune_temp,\n",
    "    test_size=0.5,          # 20% of training data will be used as the validation set\n",
    "    random_state=42,        # reproducible splits\n",
    "    stratify=y_finetune_temp  # preserve class distribution\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89752ddc-973a-4834-bceb-b8f093ec5653",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. Class imbalance handling using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19949afb-01fc-4657-96ce-2da3f9388d73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled Dataset A training data shape: (590, 10)\n",
      "Resampled Dataset B training data shape: (262, 10)\n",
      "Class distribution before SMOTE (Source domain): \n",
      "target\n",
      "1    295\n",
      "0    197\n",
      "Name: count, dtype: int64\n",
      "Class distribution after SMOTE (Source domain): \n",
      "target\n",
      "1    295\n",
      "0    295\n",
      "Name: count, dtype: int64\n",
      "Class distribution before SMOTE (Target domain): \n",
      "target\n",
      "0    131\n",
      "1    111\n",
      "Name: count, dtype: int64\n",
      "Class distribution after SMOTE (Target domain): \n",
      "target\n",
      "1    131\n",
      "0    131\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Apply SMOTE to source domain data (X_pretrain_train, y_pretrain_train)\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_pretrain_train_res, y_pretrain_train_res = smote.fit_resample(X_pretrain_train, y_pretrain_train)\n",
    "\n",
    "# Apply SMOTE to target domain data (X_finetune_train, y_finetune_train)\n",
    "X_finetune_train_res, y_finetune_train_res = smote.fit_resample(X_finetune_train, y_finetune_train)\n",
    "\n",
    "# Check the resampled data shapes and class distributions\n",
    "print(f\"Resampled Dataset A training data shape: {X_pretrain_train_res.shape}\")\n",
    "print(f\"Resampled Dataset B training data shape: {X_finetune_train_res.shape}\")\n",
    "\n",
    "# Optionally, you can print the class distribution before and after SMOTE\n",
    "print(f\"Class distribution before SMOTE (Source domain): \\n{y_pretrain_train.value_counts()}\")\n",
    "print(f\"Class distribution after SMOTE (Source domain): \\n{y_pretrain_train_res.value_counts()}\")\n",
    "\n",
    "print(f\"Class distribution before SMOTE (Target domain): \\n{y_finetune_train.value_counts()}\")\n",
    "print(f\"Class distribution after SMOTE (Target domain): \\n{y_finetune_train_res.value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21018af5-37a7-4e67-878e-39dfb70cb439",
   "metadata": {},
   "source": [
    "## 5. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6a6b5e-ed72-44d9-afb4-d1c9a756ceb7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Pretraining on the source domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c54eed7-39a9-4f06-b602-a9d87b20ce50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Hyperparameter tuning for Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dba63425-db18-4657-a165-ae6c1274011c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 13824 candidates, totalling 69120 fits\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8],\n",
    "    'learning_rate': [0.001, 0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1],\n",
    "    'reg_lambda': [0.5, 1, 1.5],\n",
    "}\n",
    "\n",
    "# Create XGBoost with balanced class weight\n",
    "xgb_pretrain_base = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=50,\n",
    "    n_jobs=1,                 # single-threaded = deterministic\n",
    "    scale_pos_weight=1\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_pretrain_base,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',\n",
    "    cv=5,\n",
    "    n_jobs=1,                 # ALSO single-threaded\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(\n",
    "    X_pretrain_train_res,\n",
    "    y_pretrain_train_res,\n",
    "    eval_set=[(X_pretrain_val, y_pretrain_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Use best estimator \n",
    "xgb_pretrain_best = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92489723-5656-47ea-b9bd-ad1951e3e359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters for pretraining: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 8, 'n_estimators': 200, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.7}\n",
      "Best F1-score: 0.8965\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nBest parameters for pretraining: {grid_search.best_params_}\")\n",
    "print(f\"Best F1-score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# UPDATED\n",
    "# Best parameters for pretraining: {'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100,\n",
    "#                                   'reg_alpha': 0, 'reg_lambda': 0.5, 'subsample': 0.8}\n",
    "# Best F1-score: 0.8994"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f6c852-ebe7-41bf-a42d-43da34fc8e2a",
   "metadata": {},
   "source": [
    "#### Pre-training on the source domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb972dad-3d65-4aa5-9a35-17ef70094585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the best parameters found during the Grid Search for pretraining on Dataset A......DO NOT RUN THIS CODE IF YOU DO HYPERPARAMETER TUNING\n",
    "best_pretrain_params = {\n",
    "    'colsample_bytree': 0.7,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 8,\n",
    "    'n_estimators': 300,\n",
    "    'reg_alpha': 0.1, \n",
    "    'reg_lambda': 0.5, \n",
    "    'subsample': 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ff71334-005f-4c43-85e7-ad7ef8d86ef8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  colsample_bytree: 0.7\n",
      "  learning_rate: 0.01\n",
      "  max_depth: 8\n",
      "  n_estimators: 300\n",
      "  reg_alpha: 0.1\n",
      "  reg_lambda: 0.5\n",
      "  subsample: 1.0\n",
      "[0]\tvalidation_0-logloss:0.68962\n",
      "[1]\tvalidation_0-logloss:0.68509\n",
      "[2]\tvalidation_0-logloss:0.68114\n",
      "[3]\tvalidation_0-logloss:0.67682\n",
      "[4]\tvalidation_0-logloss:0.67239\n",
      "[5]\tvalidation_0-logloss:0.66902\n",
      "[6]\tvalidation_0-logloss:0.66489\n",
      "[7]\tvalidation_0-logloss:0.66074\n",
      "[8]\tvalidation_0-logloss:0.65727\n",
      "[9]\tvalidation_0-logloss:0.65349\n",
      "[10]\tvalidation_0-logloss:0.64910\n",
      "[11]\tvalidation_0-logloss:0.64457\n",
      "[12]\tvalidation_0-logloss:0.64099\n",
      "[13]\tvalidation_0-logloss:0.63783\n",
      "[14]\tvalidation_0-logloss:0.63401\n",
      "[15]\tvalidation_0-logloss:0.63069\n",
      "[16]\tvalidation_0-logloss:0.62735\n",
      "[17]\tvalidation_0-logloss:0.62396\n",
      "[18]\tvalidation_0-logloss:0.62049\n",
      "[19]\tvalidation_0-logloss:0.61672\n",
      "[20]\tvalidation_0-logloss:0.61361\n",
      "[21]\tvalidation_0-logloss:0.61123\n",
      "[22]\tvalidation_0-logloss:0.60757\n",
      "[23]\tvalidation_0-logloss:0.60468\n",
      "[24]\tvalidation_0-logloss:0.60166\n",
      "[25]\tvalidation_0-logloss:0.59836\n",
      "[26]\tvalidation_0-logloss:0.59571\n",
      "[27]\tvalidation_0-logloss:0.59241\n",
      "[28]\tvalidation_0-logloss:0.59012\n",
      "[29]\tvalidation_0-logloss:0.58731\n",
      "[30]\tvalidation_0-logloss:0.58469\n",
      "[31]\tvalidation_0-logloss:0.58194\n",
      "[32]\tvalidation_0-logloss:0.57916\n",
      "[33]\tvalidation_0-logloss:0.57642\n",
      "[34]\tvalidation_0-logloss:0.57369\n",
      "[35]\tvalidation_0-logloss:0.57082\n",
      "[36]\tvalidation_0-logloss:0.56811\n",
      "[37]\tvalidation_0-logloss:0.56509\n",
      "[38]\tvalidation_0-logloss:0.56261\n",
      "[39]\tvalidation_0-logloss:0.55991\n",
      "[40]\tvalidation_0-logloss:0.55735\n",
      "[41]\tvalidation_0-logloss:0.55482\n",
      "[42]\tvalidation_0-logloss:0.55323\n",
      "[43]\tvalidation_0-logloss:0.55112\n",
      "[44]\tvalidation_0-logloss:0.54887\n",
      "[45]\tvalidation_0-logloss:0.54704\n",
      "[46]\tvalidation_0-logloss:0.54499\n",
      "[47]\tvalidation_0-logloss:0.54306\n",
      "[48]\tvalidation_0-logloss:0.54112\n",
      "[49]\tvalidation_0-logloss:0.53850\n",
      "[50]\tvalidation_0-logloss:0.53663\n",
      "[51]\tvalidation_0-logloss:0.53425\n",
      "[52]\tvalidation_0-logloss:0.53203\n",
      "[53]\tvalidation_0-logloss:0.52991\n",
      "[54]\tvalidation_0-logloss:0.52794\n",
      "[55]\tvalidation_0-logloss:0.52623\n",
      "[56]\tvalidation_0-logloss:0.52426\n",
      "[57]\tvalidation_0-logloss:0.52253\n",
      "[58]\tvalidation_0-logloss:0.52044\n",
      "[59]\tvalidation_0-logloss:0.51848\n",
      "[60]\tvalidation_0-logloss:0.51684\n",
      "[61]\tvalidation_0-logloss:0.51456\n",
      "[62]\tvalidation_0-logloss:0.51305\n",
      "[63]\tvalidation_0-logloss:0.51142\n",
      "[64]\tvalidation_0-logloss:0.50971\n",
      "[65]\tvalidation_0-logloss:0.50772\n",
      "[66]\tvalidation_0-logloss:0.50669\n",
      "[67]\tvalidation_0-logloss:0.50526\n",
      "[68]\tvalidation_0-logloss:0.50375\n",
      "[69]\tvalidation_0-logloss:0.50268\n",
      "[70]\tvalidation_0-logloss:0.50059\n",
      "[71]\tvalidation_0-logloss:0.49898\n",
      "[72]\tvalidation_0-logloss:0.49716\n",
      "[73]\tvalidation_0-logloss:0.49536\n",
      "[74]\tvalidation_0-logloss:0.49402\n",
      "[75]\tvalidation_0-logloss:0.49258\n",
      "[76]\tvalidation_0-logloss:0.49137\n",
      "[77]\tvalidation_0-logloss:0.48926\n",
      "[78]\tvalidation_0-logloss:0.48815\n",
      "[79]\tvalidation_0-logloss:0.48679\n",
      "[80]\tvalidation_0-logloss:0.48514\n",
      "[81]\tvalidation_0-logloss:0.48359\n",
      "[82]\tvalidation_0-logloss:0.48226\n",
      "[83]\tvalidation_0-logloss:0.48124\n",
      "[84]\tvalidation_0-logloss:0.48005\n",
      "[85]\tvalidation_0-logloss:0.47890\n",
      "[86]\tvalidation_0-logloss:0.47833\n",
      "[87]\tvalidation_0-logloss:0.47714\n",
      "[88]\tvalidation_0-logloss:0.47613\n",
      "[89]\tvalidation_0-logloss:0.47511\n",
      "[90]\tvalidation_0-logloss:0.47395\n",
      "[91]\tvalidation_0-logloss:0.47249\n",
      "[92]\tvalidation_0-logloss:0.47169\n",
      "[93]\tvalidation_0-logloss:0.47087\n",
      "[94]\tvalidation_0-logloss:0.47000\n",
      "[95]\tvalidation_0-logloss:0.46881\n",
      "[96]\tvalidation_0-logloss:0.46754\n",
      "[97]\tvalidation_0-logloss:0.46648\n",
      "[98]\tvalidation_0-logloss:0.46593\n",
      "[99]\tvalidation_0-logloss:0.46420\n",
      "[100]\tvalidation_0-logloss:0.46373\n",
      "[101]\tvalidation_0-logloss:0.46273\n",
      "[102]\tvalidation_0-logloss:0.46164\n",
      "[103]\tvalidation_0-logloss:0.46132\n",
      "[104]\tvalidation_0-logloss:0.46070\n",
      "[105]\tvalidation_0-logloss:0.45985\n",
      "[106]\tvalidation_0-logloss:0.45901\n",
      "[107]\tvalidation_0-logloss:0.45797\n",
      "[108]\tvalidation_0-logloss:0.45739\n",
      "[109]\tvalidation_0-logloss:0.45666\n",
      "[110]\tvalidation_0-logloss:0.45560\n",
      "[111]\tvalidation_0-logloss:0.45471\n",
      "[112]\tvalidation_0-logloss:0.45390\n",
      "[113]\tvalidation_0-logloss:0.45335\n",
      "[114]\tvalidation_0-logloss:0.45253\n",
      "[115]\tvalidation_0-logloss:0.45126\n",
      "[116]\tvalidation_0-logloss:0.45065\n",
      "[117]\tvalidation_0-logloss:0.45009\n",
      "[118]\tvalidation_0-logloss:0.44902\n",
      "[119]\tvalidation_0-logloss:0.44828\n",
      "[120]\tvalidation_0-logloss:0.44775\n",
      "[121]\tvalidation_0-logloss:0.44691\n",
      "[122]\tvalidation_0-logloss:0.44623\n",
      "[123]\tvalidation_0-logloss:0.44536\n",
      "[124]\tvalidation_0-logloss:0.44501\n",
      "[125]\tvalidation_0-logloss:0.44431\n",
      "[126]\tvalidation_0-logloss:0.44342\n",
      "[127]\tvalidation_0-logloss:0.44299\n",
      "[128]\tvalidation_0-logloss:0.44258\n",
      "[129]\tvalidation_0-logloss:0.44139\n",
      "[130]\tvalidation_0-logloss:0.44073\n",
      "[131]\tvalidation_0-logloss:0.43956\n",
      "[132]\tvalidation_0-logloss:0.43908\n",
      "[133]\tvalidation_0-logloss:0.43831\n",
      "[134]\tvalidation_0-logloss:0.43794\n",
      "[135]\tvalidation_0-logloss:0.43738\n",
      "[136]\tvalidation_0-logloss:0.43720\n",
      "[137]\tvalidation_0-logloss:0.43674\n",
      "[138]\tvalidation_0-logloss:0.43580\n",
      "[139]\tvalidation_0-logloss:0.43527\n",
      "[140]\tvalidation_0-logloss:0.43514\n",
      "[141]\tvalidation_0-logloss:0.43450\n",
      "[142]\tvalidation_0-logloss:0.43417\n",
      "[143]\tvalidation_0-logloss:0.43356\n",
      "[144]\tvalidation_0-logloss:0.43259\n",
      "[145]\tvalidation_0-logloss:0.43204\n",
      "[146]\tvalidation_0-logloss:0.43123\n",
      "[147]\tvalidation_0-logloss:0.43108\n",
      "[148]\tvalidation_0-logloss:0.43029\n",
      "[149]\tvalidation_0-logloss:0.42973\n",
      "[150]\tvalidation_0-logloss:0.42953\n",
      "[151]\tvalidation_0-logloss:0.42830\n",
      "[152]\tvalidation_0-logloss:0.42785\n",
      "[153]\tvalidation_0-logloss:0.42733\n",
      "[154]\tvalidation_0-logloss:0.42696\n",
      "[155]\tvalidation_0-logloss:0.42642\n",
      "[156]\tvalidation_0-logloss:0.42545\n",
      "[157]\tvalidation_0-logloss:0.42475\n",
      "[158]\tvalidation_0-logloss:0.42437\n",
      "[159]\tvalidation_0-logloss:0.42424\n",
      "[160]\tvalidation_0-logloss:0.42347\n",
      "[161]\tvalidation_0-logloss:0.42290\n",
      "[162]\tvalidation_0-logloss:0.42229\n",
      "[163]\tvalidation_0-logloss:0.42212\n",
      "[164]\tvalidation_0-logloss:0.42175\n",
      "[165]\tvalidation_0-logloss:0.42140\n",
      "[166]\tvalidation_0-logloss:0.42115\n",
      "[167]\tvalidation_0-logloss:0.42053\n",
      "[168]\tvalidation_0-logloss:0.42022\n",
      "[169]\tvalidation_0-logloss:0.41973\n",
      "[170]\tvalidation_0-logloss:0.41983\n",
      "[171]\tvalidation_0-logloss:0.41978\n",
      "[172]\tvalidation_0-logloss:0.41928\n",
      "[173]\tvalidation_0-logloss:0.41902\n",
      "[174]\tvalidation_0-logloss:0.41857\n",
      "[175]\tvalidation_0-logloss:0.41854\n",
      "[176]\tvalidation_0-logloss:0.41845\n",
      "[177]\tvalidation_0-logloss:0.41816\n",
      "[178]\tvalidation_0-logloss:0.41793\n",
      "[179]\tvalidation_0-logloss:0.41738\n",
      "[180]\tvalidation_0-logloss:0.41717\n",
      "[181]\tvalidation_0-logloss:0.41659\n",
      "[182]\tvalidation_0-logloss:0.41656\n",
      "[183]\tvalidation_0-logloss:0.41626\n",
      "[184]\tvalidation_0-logloss:0.41587\n",
      "[185]\tvalidation_0-logloss:0.41536\n",
      "[186]\tvalidation_0-logloss:0.41512\n",
      "[187]\tvalidation_0-logloss:0.41515\n",
      "[188]\tvalidation_0-logloss:0.41497\n",
      "[189]\tvalidation_0-logloss:0.41467\n",
      "[190]\tvalidation_0-logloss:0.41457\n",
      "[191]\tvalidation_0-logloss:0.41462\n",
      "[192]\tvalidation_0-logloss:0.41429\n",
      "[193]\tvalidation_0-logloss:0.41405\n",
      "[194]\tvalidation_0-logloss:0.41440\n",
      "[195]\tvalidation_0-logloss:0.41410\n",
      "[196]\tvalidation_0-logloss:0.41427\n",
      "[197]\tvalidation_0-logloss:0.41402\n",
      "[198]\tvalidation_0-logloss:0.41402\n",
      "[199]\tvalidation_0-logloss:0.41422\n",
      "[200]\tvalidation_0-logloss:0.41407\n",
      "[201]\tvalidation_0-logloss:0.41400\n",
      "[202]\tvalidation_0-logloss:0.41381\n",
      "[203]\tvalidation_0-logloss:0.41377\n",
      "[204]\tvalidation_0-logloss:0.41358\n",
      "[205]\tvalidation_0-logloss:0.41335\n",
      "[206]\tvalidation_0-logloss:0.41305\n",
      "[207]\tvalidation_0-logloss:0.41277\n",
      "[208]\tvalidation_0-logloss:0.41275\n",
      "[209]\tvalidation_0-logloss:0.41236\n",
      "[210]\tvalidation_0-logloss:0.41264\n",
      "[211]\tvalidation_0-logloss:0.41244\n",
      "[212]\tvalidation_0-logloss:0.41274\n",
      "[213]\tvalidation_0-logloss:0.41255\n",
      "[214]\tvalidation_0-logloss:0.41212\n",
      "[215]\tvalidation_0-logloss:0.41161\n",
      "[216]\tvalidation_0-logloss:0.41191\n",
      "[217]\tvalidation_0-logloss:0.41150\n",
      "[218]\tvalidation_0-logloss:0.41104\n",
      "[219]\tvalidation_0-logloss:0.41079\n",
      "[220]\tvalidation_0-logloss:0.41107\n",
      "[221]\tvalidation_0-logloss:0.41059\n",
      "[222]\tvalidation_0-logloss:0.41017\n",
      "[223]\tvalidation_0-logloss:0.41008\n",
      "[224]\tvalidation_0-logloss:0.40993\n",
      "[225]\tvalidation_0-logloss:0.40995\n",
      "[226]\tvalidation_0-logloss:0.41016\n",
      "[227]\tvalidation_0-logloss:0.40994\n",
      "[228]\tvalidation_0-logloss:0.41010\n",
      "[229]\tvalidation_0-logloss:0.40999\n",
      "[230]\tvalidation_0-logloss:0.40961\n",
      "[231]\tvalidation_0-logloss:0.40935\n",
      "[232]\tvalidation_0-logloss:0.40889\n",
      "[233]\tvalidation_0-logloss:0.40839\n",
      "[234]\tvalidation_0-logloss:0.40825\n",
      "[235]\tvalidation_0-logloss:0.40806\n",
      "[236]\tvalidation_0-logloss:0.40805\n",
      "[237]\tvalidation_0-logloss:0.40769\n",
      "[238]\tvalidation_0-logloss:0.40765\n",
      "[239]\tvalidation_0-logloss:0.40758\n",
      "[240]\tvalidation_0-logloss:0.40709\n",
      "[241]\tvalidation_0-logloss:0.40742\n",
      "[242]\tvalidation_0-logloss:0.40774\n",
      "[243]\tvalidation_0-logloss:0.40763\n",
      "[244]\tvalidation_0-logloss:0.40762\n",
      "[245]\tvalidation_0-logloss:0.40736\n",
      "[246]\tvalidation_0-logloss:0.40706\n",
      "[247]\tvalidation_0-logloss:0.40711\n",
      "[248]\tvalidation_0-logloss:0.40707\n",
      "[249]\tvalidation_0-logloss:0.40753\n",
      "[250]\tvalidation_0-logloss:0.40729\n",
      "[251]\tvalidation_0-logloss:0.40697\n",
      "[252]\tvalidation_0-logloss:0.40632\n",
      "[253]\tvalidation_0-logloss:0.40568\n",
      "[254]\tvalidation_0-logloss:0.40587\n",
      "[255]\tvalidation_0-logloss:0.40524\n",
      "[256]\tvalidation_0-logloss:0.40536\n",
      "[257]\tvalidation_0-logloss:0.40557\n",
      "[258]\tvalidation_0-logloss:0.40588\n",
      "[259]\tvalidation_0-logloss:0.40584\n",
      "[260]\tvalidation_0-logloss:0.40586\n",
      "[261]\tvalidation_0-logloss:0.40606\n",
      "[262]\tvalidation_0-logloss:0.40594\n",
      "[263]\tvalidation_0-logloss:0.40530\n",
      "[264]\tvalidation_0-logloss:0.40543\n",
      "[265]\tvalidation_0-logloss:0.40510\n",
      "[266]\tvalidation_0-logloss:0.40506\n",
      "[267]\tvalidation_0-logloss:0.40470\n",
      "[268]\tvalidation_0-logloss:0.40476\n",
      "[269]\tvalidation_0-logloss:0.40509\n",
      "[270]\tvalidation_0-logloss:0.40448\n",
      "[271]\tvalidation_0-logloss:0.40455\n",
      "[272]\tvalidation_0-logloss:0.40418\n",
      "[273]\tvalidation_0-logloss:0.40439\n",
      "[274]\tvalidation_0-logloss:0.40423\n",
      "[275]\tvalidation_0-logloss:0.40383\n",
      "[276]\tvalidation_0-logloss:0.40400\n",
      "[277]\tvalidation_0-logloss:0.40411\n",
      "[278]\tvalidation_0-logloss:0.40448\n",
      "[279]\tvalidation_0-logloss:0.40439\n",
      "[280]\tvalidation_0-logloss:0.40418\n",
      "[281]\tvalidation_0-logloss:0.40381\n",
      "[282]\tvalidation_0-logloss:0.40370\n",
      "[283]\tvalidation_0-logloss:0.40365\n",
      "[284]\tvalidation_0-logloss:0.40357\n",
      "[285]\tvalidation_0-logloss:0.40354\n",
      "[286]\tvalidation_0-logloss:0.40344\n",
      "[287]\tvalidation_0-logloss:0.40355\n",
      "[288]\tvalidation_0-logloss:0.40374\n",
      "[289]\tvalidation_0-logloss:0.40389\n",
      "[290]\tvalidation_0-logloss:0.40413\n",
      "[291]\tvalidation_0-logloss:0.40394\n",
      "[292]\tvalidation_0-logloss:0.40384\n",
      "[293]\tvalidation_0-logloss:0.40402\n",
      "[294]\tvalidation_0-logloss:0.40406\n",
      "[295]\tvalidation_0-logloss:0.40439\n",
      "[296]\tvalidation_0-logloss:0.40424\n",
      "[297]\tvalidation_0-logloss:0.40451\n",
      "[298]\tvalidation_0-logloss:0.40460\n",
      "[299]\tvalidation_0-logloss:0.40425\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7, device=None, early_stopping_rounds=50,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=8, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=1,\n",
       "              num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;XGBClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBClassifier\">?<span>Documentation for XGBClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7, device=None, early_stopping_rounds=50,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=8, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=1,\n",
       "              num_parallel_tree=None, ...)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7, device=None, early_stopping_rounds=50,\n",
       "              enable_categorical=False, eval_metric='logloss',\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=8, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=1,\n",
       "              num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the best parameters....UNCOMMENT THE LINE BELOW IF YOU DO HYPERPARAMETER TUNING\n",
    "# best_pretrain_params = grid_search.best_params_\n",
    "\n",
    "for param, value in best_pretrain_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Create and train fresh model\n",
    "xgb_pretrain = xgb.XGBClassifier(\n",
    "    **best_pretrain_params,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,   \n",
    "    early_stopping_rounds=50,\n",
    "    n_jobs=1             \n",
    ")\n",
    "\n",
    "xgb_pretrain.fit(\n",
    "    X_pretrain_train_res,\n",
    "    y_pretrain_train_res,\n",
    "    eval_set=[(X_pretrain_val, y_pretrain_val)],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0502320a-0a6e-4474-9a85-d5ba30124a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining Test Accuracy on Source domain: 85.48%\n",
      "Pretraining Test F1 on Source domain: 88.31%\n",
      "Pretraining Test Recall (Sensitivity) on Source domain: 91.89%\n",
      "Pretraining Test Specificity on Source domain: 76.00%\n"
     ]
    }
   ],
   "source": [
    "# PRETRAINED MODEL EVALUATION ON A-TEST\n",
    "Dataset_A_Pretrain = xgb_pretrain.predict(X_pretrain_test)\n",
    "\n",
    "#Calculate and print metrics on source domain's test set\n",
    "Dataset_A_Pretrain_acc = accuracy_score(y_pretrain_test, Dataset_A_Pretrain) * 100\n",
    "Dataset_A_Pretrain_f1  = f1_score(y_pretrain_test, Dataset_A_Pretrain) * 100\n",
    "\n",
    "cm_A = confusion_matrix(y_pretrain_test, Dataset_A_Pretrain)\n",
    "TN_A, FP_A, FN_A, TP_A = cm_A.ravel()\n",
    "\n",
    "\n",
    "recall_A = TP_A / (TP_A + FN_A) * 100\n",
    "specificity_A = TN_A / (TN_A + FP_A) * 100\n",
    "print(f\"Pretraining Test Accuracy on Source domain: {Dataset_A_Pretrain_acc:.2f}%\")\n",
    "print(f\"Pretraining Test F1 on Source domain: {Dataset_A_Pretrain_f1:.2f}%\")\n",
    "print(f\"Pretraining Test Recall (Sensitivity) on Source domain: {recall_A:.2f}%\")\n",
    "print(f\"Pretraining Test Specificity on Source domain: {specificity_A:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6f04989-11e6-4193-a3b7-c9861654f06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining Accuracy on target domain's test Set: 74.19%\n",
      "Pretraining F1 on target domain's test Set: 75.00%\n",
      "Pretraining Recall (Sensitivity) on target domain's test Set: 85.71%\n",
      "Pretraining Specificity on target domain's test Set: 64.71%\n"
     ]
    }
   ],
   "source": [
    "Dataset_B_Pretrain = xgb_pretrain.predict(X_finetune_test)\n",
    "\n",
    "#Calculate and print metrics on target domain's test set\n",
    "Dataset_B_Pretrain_acc = accuracy_score(y_finetune_test, Dataset_B_Pretrain) * 100\n",
    "Dataset_B_Pretrain_f1 = f1_score(y_finetune_test, Dataset_B_Pretrain) * 100\n",
    "cm_B = confusion_matrix(y_finetune_test, Dataset_B_Pretrain)\n",
    "TN_B, FP_B, FN_B, TP_B = cm_B.ravel()\n",
    "recall_B = TP_B / (TP_B + FN_B) * 100\n",
    "specificity_B = TN_B / (TN_B + FP_B) * 100\n",
    "\n",
    "print(f\"Pretraining Accuracy on target domain's test Set: {Dataset_B_Pretrain_acc:.2f}%\")\n",
    "print(f\"Pretraining F1 on target domain's test Set: {Dataset_B_Pretrain_f1:.2f}%\")\n",
    "print(f\"Pretraining Recall (Sensitivity) on target domain's test Set: {recall_B:.2f}%\")\n",
    "print(f\"Pretraining Specificity on target domain's test Set: {specificity_B:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cdaffd-5589-4825-a70c-53c2d7f5562f",
   "metadata": {},
   "source": [
    "### Finetuning on the target domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df4b914-0793-49bf-b37f-67c6d1bb0d17",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Hyperparameter tuning for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96e0a3b5-8fdb-4dfc-9d3f-fe59371e9293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2916 candidates, totalling 14580 fits\n"
     ]
    }
   ],
   "source": [
    "param_grid_finetune = {\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'learning_rate': [0.001, 0.01, 0.05],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [0.5, 1, 1.5],\n",
    "}\n",
    "\n",
    "xgb_finetune_best = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=50,\n",
    "    n_jobs=1,\n",
    "    scale_pos_weight=1\n",
    ")\n",
    "\n",
    "grid_search_finetune = GridSearchCV(\n",
    "    estimator=xgb_finetune_best,\n",
    "    param_grid=param_grid_finetune,\n",
    "    scoring='f1',\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "grid_search_finetune.fit(\n",
    "    X_finetune_train_res,\n",
    "    y_finetune_train_res,\n",
    "    eval_set=[(X_finetune_val, y_finetune_val)],\n",
    "    xgb_model=xgb_pretrain.get_booster(),\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_finetune_best = grid_search_finetune.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "311dca56-7e0e-48d6-8236-04c2869cebc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters for finetuning: {'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.5, 'subsample': 0.7}\n",
      "Best F1-score: 0.8216\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nBest parameters for finetuning: {grid_search_finetune.best_params_}\")\n",
    "print(f\"Best F1-score: {grid_search_finetune.best_score_:.4f}\")\n",
    "\n",
    "# Best parameters for finetuning: {'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_depth': 4, \n",
    "#                                  'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0.5, 'subsample': 0.7}\n",
    "# Best F1-score: 0.8231"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcd9fd0-9783-40e5-a383-59bb35b59f47",
   "metadata": {},
   "source": [
    "#### Finetuning on the target domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bddce10f-fb66-44ac-8ecf-754a03f87809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #DO NOT THE LINE CODE BELOW IF YOU DO HYPERPARAMETER TUNING\n",
    "best_finetune_params = {\n",
    "    'colsample_bytree': 0.7,\n",
    "    'learning_rate': 0.009,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 300,\n",
    "    'reg_alpha': 0.1, \n",
    "    'reg_lambda': 1.5, \n",
    "    'subsample': 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc73f4df-52f0-45e6-b742-9ada28c944aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  colsample_bytree: 0.7\n",
      "  learning_rate: 0.009\n",
      "  max_depth: 6\n",
      "  n_estimators: 300\n",
      "  reg_alpha: 0.1\n",
      "  reg_lambda: 1.5\n",
      "  subsample: 1.0\n",
      "[0]\tvalidation_0-logloss:0.31364\n",
      "[1]\tvalidation_0-logloss:0.31171\n",
      "[2]\tvalidation_0-logloss:0.31046\n",
      "[3]\tvalidation_0-logloss:0.30874\n",
      "[4]\tvalidation_0-logloss:0.30726\n",
      "[5]\tvalidation_0-logloss:0.30543\n",
      "[6]\tvalidation_0-logloss:0.30367\n",
      "[7]\tvalidation_0-logloss:0.30296\n",
      "[8]\tvalidation_0-logloss:0.30126\n",
      "[9]\tvalidation_0-logloss:0.30033\n",
      "[10]\tvalidation_0-logloss:0.29865\n",
      "[11]\tvalidation_0-logloss:0.29749\n",
      "[12]\tvalidation_0-logloss:0.29669\n",
      "[13]\tvalidation_0-logloss:0.29564\n",
      "[14]\tvalidation_0-logloss:0.29440\n",
      "[15]\tvalidation_0-logloss:0.29289\n",
      "[16]\tvalidation_0-logloss:0.29135\n",
      "[17]\tvalidation_0-logloss:0.29038\n",
      "[18]\tvalidation_0-logloss:0.28935\n",
      "[19]\tvalidation_0-logloss:0.28867\n",
      "[20]\tvalidation_0-logloss:0.28775\n",
      "[21]\tvalidation_0-logloss:0.28648\n",
      "[22]\tvalidation_0-logloss:0.28564\n",
      "[23]\tvalidation_0-logloss:0.28466\n",
      "[24]\tvalidation_0-logloss:0.28360\n",
      "[25]\tvalidation_0-logloss:0.28297\n",
      "[26]\tvalidation_0-logloss:0.28250\n",
      "[27]\tvalidation_0-logloss:0.28132\n",
      "[28]\tvalidation_0-logloss:0.28076\n",
      "[29]\tvalidation_0-logloss:0.28022\n",
      "[30]\tvalidation_0-logloss:0.27933\n",
      "[31]\tvalidation_0-logloss:0.27807\n",
      "[32]\tvalidation_0-logloss:0.27706\n",
      "[33]\tvalidation_0-logloss:0.27598\n",
      "[34]\tvalidation_0-logloss:0.27486\n",
      "[35]\tvalidation_0-logloss:0.27414\n",
      "[36]\tvalidation_0-logloss:0.27295\n",
      "[37]\tvalidation_0-logloss:0.27224\n",
      "[38]\tvalidation_0-logloss:0.27110\n",
      "[39]\tvalidation_0-logloss:0.27006\n",
      "[40]\tvalidation_0-logloss:0.26940\n",
      "[41]\tvalidation_0-logloss:0.26854\n",
      "[42]\tvalidation_0-logloss:0.26798\n",
      "[43]\tvalidation_0-logloss:0.26710\n",
      "[44]\tvalidation_0-logloss:0.26645\n",
      "[45]\tvalidation_0-logloss:0.26565\n",
      "[46]\tvalidation_0-logloss:0.26511\n",
      "[47]\tvalidation_0-logloss:0.26410\n",
      "[48]\tvalidation_0-logloss:0.26305\n",
      "[49]\tvalidation_0-logloss:0.26238\n",
      "[50]\tvalidation_0-logloss:0.26186\n",
      "[51]\tvalidation_0-logloss:0.26117\n",
      "[52]\tvalidation_0-logloss:0.26065\n",
      "[53]\tvalidation_0-logloss:0.25993\n",
      "[54]\tvalidation_0-logloss:0.25933\n",
      "[55]\tvalidation_0-logloss:0.25880\n",
      "[56]\tvalidation_0-logloss:0.25814\n",
      "[57]\tvalidation_0-logloss:0.25772\n",
      "[58]\tvalidation_0-logloss:0.25726\n",
      "[59]\tvalidation_0-logloss:0.25692\n",
      "[60]\tvalidation_0-logloss:0.25605\n",
      "[61]\tvalidation_0-logloss:0.25576\n",
      "[62]\tvalidation_0-logloss:0.25496\n",
      "[63]\tvalidation_0-logloss:0.25420\n",
      "[64]\tvalidation_0-logloss:0.25362\n",
      "[65]\tvalidation_0-logloss:0.25308\n",
      "[66]\tvalidation_0-logloss:0.25256\n",
      "[67]\tvalidation_0-logloss:0.25187\n",
      "[68]\tvalidation_0-logloss:0.25105\n",
      "[69]\tvalidation_0-logloss:0.25044\n",
      "[70]\tvalidation_0-logloss:0.24958\n",
      "[71]\tvalidation_0-logloss:0.24897\n",
      "[72]\tvalidation_0-logloss:0.24826\n",
      "[73]\tvalidation_0-logloss:0.24792\n",
      "[74]\tvalidation_0-logloss:0.24779\n",
      "[75]\tvalidation_0-logloss:0.24708\n",
      "[76]\tvalidation_0-logloss:0.24664\n",
      "[77]\tvalidation_0-logloss:0.24610\n",
      "[78]\tvalidation_0-logloss:0.24554\n",
      "[79]\tvalidation_0-logloss:0.24464\n",
      "[80]\tvalidation_0-logloss:0.24409\n",
      "[81]\tvalidation_0-logloss:0.24364\n",
      "[82]\tvalidation_0-logloss:0.24329\n",
      "[83]\tvalidation_0-logloss:0.24278\n",
      "[84]\tvalidation_0-logloss:0.24285\n",
      "[85]\tvalidation_0-logloss:0.24237\n",
      "[86]\tvalidation_0-logloss:0.24164\n",
      "[87]\tvalidation_0-logloss:0.24092\n",
      "[88]\tvalidation_0-logloss:0.24054\n",
      "[89]\tvalidation_0-logloss:0.24051\n",
      "[90]\tvalidation_0-logloss:0.23980\n",
      "[91]\tvalidation_0-logloss:0.23948\n",
      "[92]\tvalidation_0-logloss:0.23886\n",
      "[93]\tvalidation_0-logloss:0.23882\n",
      "[94]\tvalidation_0-logloss:0.23864\n",
      "[95]\tvalidation_0-logloss:0.23789\n",
      "[96]\tvalidation_0-logloss:0.23754\n",
      "[97]\tvalidation_0-logloss:0.23684\n",
      "[98]\tvalidation_0-logloss:0.23682\n",
      "[99]\tvalidation_0-logloss:0.23636\n",
      "[100]\tvalidation_0-logloss:0.23638\n",
      "[101]\tvalidation_0-logloss:0.23606\n",
      "[102]\tvalidation_0-logloss:0.23558\n",
      "[103]\tvalidation_0-logloss:0.23523\n",
      "[104]\tvalidation_0-logloss:0.23523\n",
      "[105]\tvalidation_0-logloss:0.23487\n",
      "[106]\tvalidation_0-logloss:0.23456\n",
      "[107]\tvalidation_0-logloss:0.23390\n",
      "[108]\tvalidation_0-logloss:0.23397\n",
      "[109]\tvalidation_0-logloss:0.23370\n",
      "[110]\tvalidation_0-logloss:0.23380\n",
      "[111]\tvalidation_0-logloss:0.23388\n",
      "[112]\tvalidation_0-logloss:0.23381\n",
      "[113]\tvalidation_0-logloss:0.23366\n",
      "[114]\tvalidation_0-logloss:0.23321\n",
      "[115]\tvalidation_0-logloss:0.23309\n",
      "[116]\tvalidation_0-logloss:0.23255\n",
      "[117]\tvalidation_0-logloss:0.23237\n",
      "[118]\tvalidation_0-logloss:0.23229\n",
      "[119]\tvalidation_0-logloss:0.23163\n",
      "[120]\tvalidation_0-logloss:0.23119\n",
      "[121]\tvalidation_0-logloss:0.23092\n",
      "[122]\tvalidation_0-logloss:0.23036\n",
      "[123]\tvalidation_0-logloss:0.23034\n",
      "[124]\tvalidation_0-logloss:0.23023\n",
      "[125]\tvalidation_0-logloss:0.22991\n",
      "[126]\tvalidation_0-logloss:0.22966\n",
      "[127]\tvalidation_0-logloss:0.22928\n",
      "[128]\tvalidation_0-logloss:0.22923\n",
      "[129]\tvalidation_0-logloss:0.22861\n",
      "[130]\tvalidation_0-logloss:0.22850\n",
      "[131]\tvalidation_0-logloss:0.22812\n",
      "[132]\tvalidation_0-logloss:0.22788\n",
      "[133]\tvalidation_0-logloss:0.22745\n",
      "[134]\tvalidation_0-logloss:0.22748\n",
      "[135]\tvalidation_0-logloss:0.22718\n",
      "[136]\tvalidation_0-logloss:0.22663\n",
      "[137]\tvalidation_0-logloss:0.22634\n",
      "[138]\tvalidation_0-logloss:0.22605\n",
      "[139]\tvalidation_0-logloss:0.22586\n",
      "[140]\tvalidation_0-logloss:0.22536\n",
      "[141]\tvalidation_0-logloss:0.22514\n",
      "[142]\tvalidation_0-logloss:0.22466\n",
      "[143]\tvalidation_0-logloss:0.22482\n",
      "[144]\tvalidation_0-logloss:0.22456\n",
      "[145]\tvalidation_0-logloss:0.22440\n",
      "[146]\tvalidation_0-logloss:0.22420\n",
      "[147]\tvalidation_0-logloss:0.22371\n",
      "[148]\tvalidation_0-logloss:0.22392\n",
      "[149]\tvalidation_0-logloss:0.22392\n",
      "[150]\tvalidation_0-logloss:0.22379\n",
      "[151]\tvalidation_0-logloss:0.22382\n",
      "[152]\tvalidation_0-logloss:0.22372\n",
      "[153]\tvalidation_0-logloss:0.22317\n",
      "[154]\tvalidation_0-logloss:0.22289\n",
      "[155]\tvalidation_0-logloss:0.22254\n",
      "[156]\tvalidation_0-logloss:0.22236\n",
      "[157]\tvalidation_0-logloss:0.22217\n",
      "[158]\tvalidation_0-logloss:0.22174\n",
      "[159]\tvalidation_0-logloss:0.22152\n",
      "[160]\tvalidation_0-logloss:0.22120\n",
      "[161]\tvalidation_0-logloss:0.22117\n",
      "[162]\tvalidation_0-logloss:0.22081\n",
      "[163]\tvalidation_0-logloss:0.22062\n",
      "[164]\tvalidation_0-logloss:0.22085\n",
      "[165]\tvalidation_0-logloss:0.22066\n",
      "[166]\tvalidation_0-logloss:0.22030\n",
      "[167]\tvalidation_0-logloss:0.21982\n",
      "[168]\tvalidation_0-logloss:0.21955\n",
      "[169]\tvalidation_0-logloss:0.21929\n",
      "[170]\tvalidation_0-logloss:0.21879\n",
      "[171]\tvalidation_0-logloss:0.21858\n",
      "[172]\tvalidation_0-logloss:0.21790\n",
      "[173]\tvalidation_0-logloss:0.21771\n",
      "[174]\tvalidation_0-logloss:0.21738\n",
      "[175]\tvalidation_0-logloss:0.21704\n",
      "[176]\tvalidation_0-logloss:0.21690\n",
      "[177]\tvalidation_0-logloss:0.21651\n",
      "[178]\tvalidation_0-logloss:0.21620\n",
      "[179]\tvalidation_0-logloss:0.21596\n",
      "[180]\tvalidation_0-logloss:0.21568\n",
      "[181]\tvalidation_0-logloss:0.21552\n",
      "[182]\tvalidation_0-logloss:0.21543\n",
      "[183]\tvalidation_0-logloss:0.21516\n",
      "[184]\tvalidation_0-logloss:0.21465\n",
      "[185]\tvalidation_0-logloss:0.21463\n",
      "[186]\tvalidation_0-logloss:0.21455\n",
      "[187]\tvalidation_0-logloss:0.21429\n",
      "[188]\tvalidation_0-logloss:0.21403\n",
      "[189]\tvalidation_0-logloss:0.21404\n",
      "[190]\tvalidation_0-logloss:0.21405\n",
      "[191]\tvalidation_0-logloss:0.21379\n",
      "[192]\tvalidation_0-logloss:0.21378\n",
      "[193]\tvalidation_0-logloss:0.21347\n",
      "[194]\tvalidation_0-logloss:0.21347\n",
      "[195]\tvalidation_0-logloss:0.21331\n",
      "[196]\tvalidation_0-logloss:0.21283\n",
      "[197]\tvalidation_0-logloss:0.21255\n",
      "[198]\tvalidation_0-logloss:0.21232\n",
      "[199]\tvalidation_0-logloss:0.21226\n",
      "[200]\tvalidation_0-logloss:0.21211\n",
      "[201]\tvalidation_0-logloss:0.21222\n",
      "[202]\tvalidation_0-logloss:0.21214\n",
      "[203]\tvalidation_0-logloss:0.21213\n",
      "[204]\tvalidation_0-logloss:0.21193\n",
      "[205]\tvalidation_0-logloss:0.21188\n",
      "[206]\tvalidation_0-logloss:0.21163\n",
      "[207]\tvalidation_0-logloss:0.21140\n",
      "[208]\tvalidation_0-logloss:0.21116\n",
      "[209]\tvalidation_0-logloss:0.21090\n",
      "[210]\tvalidation_0-logloss:0.21074\n",
      "[211]\tvalidation_0-logloss:0.21039\n",
      "[212]\tvalidation_0-logloss:0.21037\n",
      "[213]\tvalidation_0-logloss:0.21008\n",
      "[214]\tvalidation_0-logloss:0.20988\n",
      "[215]\tvalidation_0-logloss:0.21012\n",
      "[216]\tvalidation_0-logloss:0.20973\n",
      "[217]\tvalidation_0-logloss:0.20972\n",
      "[218]\tvalidation_0-logloss:0.20964\n",
      "[219]\tvalidation_0-logloss:0.20954\n",
      "[220]\tvalidation_0-logloss:0.20930\n",
      "[221]\tvalidation_0-logloss:0.20915\n",
      "[222]\tvalidation_0-logloss:0.20874\n",
      "[223]\tvalidation_0-logloss:0.20870\n",
      "[224]\tvalidation_0-logloss:0.20860\n",
      "[225]\tvalidation_0-logloss:0.20801\n",
      "[226]\tvalidation_0-logloss:0.20786\n",
      "[227]\tvalidation_0-logloss:0.20779\n",
      "[228]\tvalidation_0-logloss:0.20753\n",
      "[229]\tvalidation_0-logloss:0.20740\n",
      "[230]\tvalidation_0-logloss:0.20690\n",
      "[231]\tvalidation_0-logloss:0.20681\n",
      "[232]\tvalidation_0-logloss:0.20680\n",
      "[233]\tvalidation_0-logloss:0.20656\n",
      "[234]\tvalidation_0-logloss:0.20648\n",
      "[235]\tvalidation_0-logloss:0.20632\n",
      "[236]\tvalidation_0-logloss:0.20623\n",
      "[237]\tvalidation_0-logloss:0.20621\n",
      "[238]\tvalidation_0-logloss:0.20621\n",
      "[239]\tvalidation_0-logloss:0.20643\n",
      "[240]\tvalidation_0-logloss:0.20649\n",
      "[241]\tvalidation_0-logloss:0.20638\n",
      "[242]\tvalidation_0-logloss:0.20627\n",
      "[243]\tvalidation_0-logloss:0.20576\n",
      "[244]\tvalidation_0-logloss:0.20560\n",
      "[245]\tvalidation_0-logloss:0.20543\n",
      "[246]\tvalidation_0-logloss:0.20516\n",
      "[247]\tvalidation_0-logloss:0.20464\n",
      "[248]\tvalidation_0-logloss:0.20428\n",
      "[249]\tvalidation_0-logloss:0.20416\n",
      "[250]\tvalidation_0-logloss:0.20407\n",
      "[251]\tvalidation_0-logloss:0.20425\n",
      "[252]\tvalidation_0-logloss:0.20380\n",
      "[253]\tvalidation_0-logloss:0.20361\n",
      "[254]\tvalidation_0-logloss:0.20340\n",
      "[255]\tvalidation_0-logloss:0.20296\n",
      "[256]\tvalidation_0-logloss:0.20246\n",
      "[257]\tvalidation_0-logloss:0.20268\n",
      "[258]\tvalidation_0-logloss:0.20254\n",
      "[259]\tvalidation_0-logloss:0.20238\n",
      "[260]\tvalidation_0-logloss:0.20223\n",
      "[261]\tvalidation_0-logloss:0.20180\n",
      "[262]\tvalidation_0-logloss:0.20122\n",
      "[263]\tvalidation_0-logloss:0.20086\n",
      "[264]\tvalidation_0-logloss:0.20072\n",
      "[265]\tvalidation_0-logloss:0.20052\n",
      "[266]\tvalidation_0-logloss:0.20003\n",
      "[267]\tvalidation_0-logloss:0.20013\n",
      "[268]\tvalidation_0-logloss:0.20030\n",
      "[269]\tvalidation_0-logloss:0.20022\n",
      "[270]\tvalidation_0-logloss:0.20031\n",
      "[271]\tvalidation_0-logloss:0.20008\n",
      "[272]\tvalidation_0-logloss:0.20000\n",
      "[273]\tvalidation_0-logloss:0.20022\n",
      "[274]\tvalidation_0-logloss:0.20007\n",
      "[275]\tvalidation_0-logloss:0.19983\n",
      "[276]\tvalidation_0-logloss:0.19975\n",
      "[277]\tvalidation_0-logloss:0.19946\n",
      "[278]\tvalidation_0-logloss:0.19953\n",
      "[279]\tvalidation_0-logloss:0.19963\n",
      "[280]\tvalidation_0-logloss:0.19950\n",
      "[281]\tvalidation_0-logloss:0.19939\n",
      "[282]\tvalidation_0-logloss:0.19944\n",
      "[283]\tvalidation_0-logloss:0.19929\n",
      "[284]\tvalidation_0-logloss:0.19929\n",
      "[285]\tvalidation_0-logloss:0.19941\n",
      "[286]\tvalidation_0-logloss:0.19934\n",
      "[287]\tvalidation_0-logloss:0.19925\n",
      "[288]\tvalidation_0-logloss:0.19922\n",
      "[289]\tvalidation_0-logloss:0.19865\n",
      "[290]\tvalidation_0-logloss:0.19848\n",
      "[291]\tvalidation_0-logloss:0.19821\n",
      "[292]\tvalidation_0-logloss:0.19848\n",
      "[293]\tvalidation_0-logloss:0.19836\n",
      "[294]\tvalidation_0-logloss:0.19821\n",
      "[295]\tvalidation_0-logloss:0.19819\n",
      "[296]\tvalidation_0-logloss:0.19808\n",
      "[297]\tvalidation_0-logloss:0.19794\n",
      "[298]\tvalidation_0-logloss:0.19797\n",
      "[299]\tvalidation_0-logloss:0.19763\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7, device=None, early_stopping_rounds=50,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.009, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=1,\n",
       "              num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;XGBClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBClassifier\">?<span>Documentation for XGBClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7, device=None, early_stopping_rounds=50,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.009, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=1,\n",
       "              num_parallel_tree=None, ...)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7, device=None, early_stopping_rounds=50,\n",
       "              enable_categorical=False, eval_metric='logloss',\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.009, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=1,\n",
       "              num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the best parameters from fine-tuning grid search......UNCOMMENT THE LINE BELOW IF YOU DO HYPERPARAMETER TUNING\n",
    "# best_finetune_params = grid_search_finetune.best_params_\n",
    "\n",
    "for param, value in best_finetune_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "xgb_finetune = xgb.XGBClassifier(\n",
    "    **best_finetune_params,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=50,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "xgb_finetune.fit(\n",
    "    X_finetune_train_res,\n",
    "    y_finetune_train_res, \n",
    "    eval_set=[(X_finetune_val, y_finetune_val)], \n",
    "    xgb_model=xgb_pretrain.get_booster(),\n",
    "    verbose=1 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b386de1-f1cb-4ba2-ad3c-dd10c4138c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Test Accuracy on Source domain: 80.65%\n",
      "Fine-Tuning Test F1 on Source domain: 84.62%\n",
      "Fine-Tuning Test Recall (Sensitivity) on Source domain: 89.19%\n",
      "Fine-Tuning Test Specificity on Source domain: 68.00%\n"
     ]
    }
   ],
   "source": [
    "# FINE-TUNED MODEL EVALUATION ON A-TEST (Checking for Catastrophic Forgetting)\n",
    "Dataset_A_Finetune = xgb_finetune.predict(X_pretrain_test) # Using the fine-tuned model on A's test set\n",
    "\n",
    "# Calculate and print metrics on source domain's test set\n",
    "acc_A_Finetune = accuracy_score(y_pretrain_test, Dataset_A_Finetune) * 100\n",
    "f1_A_Finetune = f1_score(y_pretrain_test, Dataset_A_Finetune) * 100\n",
    "\n",
    "cm_A_Finetune = confusion_matrix(y_pretrain_test, Dataset_A_Finetune)\n",
    "TN_A_F, FP_A_F, FN_A_F, TP_A_F = cm_A_Finetune.ravel()\n",
    "\n",
    "recall_A_Finetune = TP_A_F / (TP_A_F + FN_A_F) * 100\n",
    "specificity_A_Finetune = TN_A_F / (TN_A_F + FP_A_F) * 100\n",
    "\n",
    "print(f\"Fine-Tuning Test Accuracy on Source domain: {acc_A_Finetune:.2f}%\")\n",
    "print(f\"Fine-Tuning Test F1 on Source domain: {f1_A_Finetune:.2f}%\")\n",
    "print(f\"Fine-Tuning Test Recall (Sensitivity) on Source domain: {recall_A_Finetune:.2f}%\")\n",
    "print(f\"Fine-Tuning Test Specificity on Source domain: {specificity_A_Finetune:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afac6e80-0424-49c3-abc1-27ce75b57a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Test ROC-AUC on Target domain (B): 85.29%\n",
      "Fine-Tuning Test Accuracy on Target domain (B): 83.87%\n",
      "Fine-Tuning Test F1 on Target domain (B): 84.85%\n",
      "Fine-Tuning Test Recall (Sensitivity) on Target domain (B): 100.00%\n",
      "Fine-Tuning Test Specificity on Target domain (B): 70.59%\n"
     ]
    }
   ],
   "source": [
    "# FINE-TUNED MODEL EVALUATION ON B-TEST (Final Transfer Learning Performance)\n",
    "Dataset_B_Finetune = xgb_finetune.predict(X_finetune_test) # Using the fine-tuned model on B's test set\n",
    "\n",
    "# Calculate and print metrics on target domain's test set\n",
    "acc_B_Finetune = accuracy_score(y_finetune_test, Dataset_B_Finetune) * 100\n",
    "f1_B_Finetune = f1_score(y_finetune_test, Dataset_B_Finetune) * 100\n",
    "\n",
    "cm_B_Finetune = confusion_matrix(y_finetune_test, Dataset_B_Finetune)\n",
    "TN_B_F, FP_B_F, FN_B_F, TP_B_F = cm_B_Finetune.ravel()\n",
    "\n",
    "recall_B_Finetune = TP_B_F / (TP_B_F + FN_B_F) * 100\n",
    "specificity_B_Finetune = TN_B_F / (TN_B_F + FP_B_F) * 100\n",
    "roc_auc_B_Finetune = roc_auc_score(y_finetune_test, Dataset_B_Finetune) * 100\n",
    "\n",
    "print(f\"Fine-Tuning Test ROC-AUC on Target domain (B): {roc_auc_B_Finetune:.2f}%\")\n",
    "print(f\"Fine-Tuning Test Accuracy on Target domain (B): {acc_B_Finetune:.2f}%\")\n",
    "print(f\"Fine-Tuning Test F1 on Target domain (B): {f1_B_Finetune:.2f}%\")\n",
    "print(f\"Fine-Tuning Test Recall (Sensitivity) on Target domain (B): {recall_B_Finetune:.2f}%\")\n",
    "print(f\"Fine-Tuning Test Specificity on Target domain (B): {specificity_B_Finetune:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aba10ea4-ab9b-4d57-914a-c4f843778d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global font to Times New Roman\n",
    "# plt.rcParams.update({'font.family': 'Times New Roman'})\n",
    "plt.rcParams.update({'font.family': 'Times New Roman', 'font.weight': 'bold', 'font.size' : '16'})\n",
    "\n",
    "# Assuming y_finetune_test and Dataset_B_Finetune are already defined\n",
    "cm = confusion_matrix(y_finetune_test, Dataset_B_Finetune)\n",
    "\n",
    "# Unravel the confusion matrix to get TN, FP, FN, TP\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "im = plt.imshow(cm, cmap='Blues', interpolation='nearest') \n",
    "\n",
    "vmax = cm.max()\n",
    "colorbar_ticks = np.arange(0, vmax + 10, 10)\n",
    "plt.colorbar(im, ticks=colorbar_ticks)\n",
    "\n",
    "# plt.title('Confusion Matrix of the fine-tuned model')\n",
    "\n",
    "ax = plt.gca()\n",
    "# ax.invert_yaxis()  # You can uncomment this if you want to invert the Y-axis\n",
    "\n",
    "# Set X-axis labels (Predicted: Disease then No Disease)\n",
    "plt.xticks([0, 1], ['No Disease', 'Disease']) \n",
    "plt.yticks([0, 1], ['No Disease', 'Disease']) \n",
    "\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "\n",
    "# Dynamic text color for contrast\n",
    "norm = mcolors.Normalize(vmin=cm.min(), vmax=cm.max())\n",
    "cmap = plt.colormaps['Blues']\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        cell_value = cm[i, j]\n",
    "        cell_color = cmap(norm(cell_value))\n",
    "        luminance = 0.299 * cell_color[0] + 0.587 * cell_color[1] + 0.114 * cell_color[2]\n",
    "        text_color = 'white' if luminance < 0.5 else 'black'\n",
    "        \n",
    "        # Set the text in Times New Roman\n",
    "        plt.text(j, i, f'{cell_value}', ha='center', va='center', color=text_color, fontsize=24, fontweight='bold')\n",
    "\n",
    "# Save the confusion matrix image\n",
    "plt.savefig('confusion_matrix_xgboost.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c790849-940f-44e0-b416-6e65f4e90ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 82.35%\n"
     ]
    }
   ],
   "source": [
    "plt.rcParams.update({'font.family': 'Times New Roman', 'font.weight': 'bold', 'font.size' : '16'})\n",
    "\n",
    "# Calculate Precision, Recall, F1-Score\n",
    "precision = precision_score(y_finetune_test, Dataset_B_Finetune) * 100\n",
    "recall = recall_score(y_finetune_test, Dataset_B_Finetune) * 100\n",
    "f1 = f1_score(y_finetune_test, Dataset_B_Finetune) * 100\n",
    "\n",
    "# Calculate Specificity: TN / (TN + FP)\n",
    "specificity = (tn / (tn + fp)) * 100\n",
    "\n",
    "# Calculate ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_finetune_test, Dataset_B_Finetune) * 100\n",
    "\n",
    "print(f\"ROC-AUC: {roc_auc:.2f}%\")\n",
    "\n",
    "# Plot ROC curve with blue tones (IEEE standard)\n",
    "fpr, tpr, _ = roc_curve(y_finetune_test, Dataset_B_Finetune)\n",
    "roc_auc_value = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "\n",
    "# Plot the ROC curve: Use 'orange' color and update the label\n",
    "plt.plot(fpr, tpr, color='orange', lw=2, label=f'ROC curve (AUC = {roc_auc_value:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', lw=2, linestyle='--')\n",
    "\n",
    "# Set limits and labels as in the original image\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate') # Changed back to match the image axis label\n",
    "\n",
    "# Adjust legend location to match the original image\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Save the ROC curve image (you can keep your original filename or update it)\n",
    "plt.savefig('roc_curve_xgboost.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136a481e-9936-409b-972e-3021ab1952b7",
   "metadata": {},
   "source": [
    "### XGBoost with no Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44636e68-a03d-4dca-ae99-4163c450919b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.68921\n",
      "[1]\tvalidation_0-logloss:0.68507\n",
      "[2]\tvalidation_0-logloss:0.68126\n",
      "[3]\tvalidation_0-logloss:0.67732\n",
      "[4]\tvalidation_0-logloss:0.67260\n",
      "[5]\tvalidation_0-logloss:0.66873\n",
      "[6]\tvalidation_0-logloss:0.66405\n",
      "[7]\tvalidation_0-logloss:0.65966\n",
      "[8]\tvalidation_0-logloss:0.65562\n",
      "[9]\tvalidation_0-logloss:0.65204\n",
      "[10]\tvalidation_0-logloss:0.64815\n",
      "[11]\tvalidation_0-logloss:0.64398\n",
      "[12]\tvalidation_0-logloss:0.64027\n",
      "[13]\tvalidation_0-logloss:0.63645\n",
      "[14]\tvalidation_0-logloss:0.63237\n",
      "[15]\tvalidation_0-logloss:0.62943\n",
      "[16]\tvalidation_0-logloss:0.62584\n",
      "[17]\tvalidation_0-logloss:0.62310\n",
      "[18]\tvalidation_0-logloss:0.61913\n",
      "[19]\tvalidation_0-logloss:0.61560\n",
      "[20]\tvalidation_0-logloss:0.61218\n",
      "[21]\tvalidation_0-logloss:0.60934\n",
      "[22]\tvalidation_0-logloss:0.60605\n",
      "[23]\tvalidation_0-logloss:0.60343\n",
      "[24]\tvalidation_0-logloss:0.60043\n",
      "[25]\tvalidation_0-logloss:0.59687\n",
      "[26]\tvalidation_0-logloss:0.59372\n",
      "[27]\tvalidation_0-logloss:0.59036\n",
      "[28]\tvalidation_0-logloss:0.58756\n",
      "[29]\tvalidation_0-logloss:0.58452\n",
      "[30]\tvalidation_0-logloss:0.58149\n",
      "[31]\tvalidation_0-logloss:0.57854\n",
      "[32]\tvalidation_0-logloss:0.57535\n",
      "[33]\tvalidation_0-logloss:0.57228\n",
      "[34]\tvalidation_0-logloss:0.56890\n",
      "[35]\tvalidation_0-logloss:0.56590\n",
      "[36]\tvalidation_0-logloss:0.56372\n",
      "[37]\tvalidation_0-logloss:0.56057\n",
      "[38]\tvalidation_0-logloss:0.55727\n",
      "[39]\tvalidation_0-logloss:0.55458\n",
      "[40]\tvalidation_0-logloss:0.55232\n",
      "[41]\tvalidation_0-logloss:0.54947\n",
      "[42]\tvalidation_0-logloss:0.54766\n",
      "[43]\tvalidation_0-logloss:0.54499\n",
      "[44]\tvalidation_0-logloss:0.54195\n",
      "[45]\tvalidation_0-logloss:0.53931\n",
      "[46]\tvalidation_0-logloss:0.53610\n",
      "[47]\tvalidation_0-logloss:0.53368\n",
      "[48]\tvalidation_0-logloss:0.53113\n",
      "[49]\tvalidation_0-logloss:0.52881\n",
      "[50]\tvalidation_0-logloss:0.52644\n",
      "[51]\tvalidation_0-logloss:0.52385\n",
      "[52]\tvalidation_0-logloss:0.52155\n",
      "[53]\tvalidation_0-logloss:0.51925\n",
      "[54]\tvalidation_0-logloss:0.51676\n",
      "[55]\tvalidation_0-logloss:0.51455\n",
      "[56]\tvalidation_0-logloss:0.51233\n",
      "[57]\tvalidation_0-logloss:0.51004\n",
      "[58]\tvalidation_0-logloss:0.50824\n",
      "[59]\tvalidation_0-logloss:0.50640\n",
      "[60]\tvalidation_0-logloss:0.50481\n",
      "[61]\tvalidation_0-logloss:0.50236\n",
      "[62]\tvalidation_0-logloss:0.49988\n",
      "[63]\tvalidation_0-logloss:0.49870\n",
      "[64]\tvalidation_0-logloss:0.49643\n",
      "[65]\tvalidation_0-logloss:0.49447\n",
      "[66]\tvalidation_0-logloss:0.49284\n",
      "[67]\tvalidation_0-logloss:0.49029\n",
      "[68]\tvalidation_0-logloss:0.48833\n",
      "[69]\tvalidation_0-logloss:0.48670\n",
      "[70]\tvalidation_0-logloss:0.48475\n",
      "[71]\tvalidation_0-logloss:0.48286\n",
      "[72]\tvalidation_0-logloss:0.48104\n",
      "[73]\tvalidation_0-logloss:0.47897\n",
      "[74]\tvalidation_0-logloss:0.47701\n",
      "[75]\tvalidation_0-logloss:0.47504\n",
      "[76]\tvalidation_0-logloss:0.47401\n",
      "[77]\tvalidation_0-logloss:0.47232\n",
      "[78]\tvalidation_0-logloss:0.47108\n",
      "[79]\tvalidation_0-logloss:0.46899\n",
      "[80]\tvalidation_0-logloss:0.46678\n",
      "[81]\tvalidation_0-logloss:0.46456\n",
      "[82]\tvalidation_0-logloss:0.46259\n",
      "[83]\tvalidation_0-logloss:0.46062\n",
      "[84]\tvalidation_0-logloss:0.45914\n",
      "[85]\tvalidation_0-logloss:0.45756\n",
      "[86]\tvalidation_0-logloss:0.45648\n",
      "[87]\tvalidation_0-logloss:0.45477\n",
      "[88]\tvalidation_0-logloss:0.45305\n",
      "[89]\tvalidation_0-logloss:0.45136\n",
      "[90]\tvalidation_0-logloss:0.44969\n",
      "[91]\tvalidation_0-logloss:0.44772\n",
      "[92]\tvalidation_0-logloss:0.44635\n",
      "[93]\tvalidation_0-logloss:0.44501\n",
      "[94]\tvalidation_0-logloss:0.44305\n",
      "[95]\tvalidation_0-logloss:0.44120\n",
      "[96]\tvalidation_0-logloss:0.43942\n",
      "[97]\tvalidation_0-logloss:0.43809\n",
      "[98]\tvalidation_0-logloss:0.43659\n",
      "[99]\tvalidation_0-logloss:0.43490\n",
      "[100]\tvalidation_0-logloss:0.43320\n",
      "[101]\tvalidation_0-logloss:0.43184\n",
      "[102]\tvalidation_0-logloss:0.42999\n",
      "[103]\tvalidation_0-logloss:0.42836\n",
      "[104]\tvalidation_0-logloss:0.42704\n",
      "[105]\tvalidation_0-logloss:0.42535\n",
      "[106]\tvalidation_0-logloss:0.42391\n",
      "[107]\tvalidation_0-logloss:0.42226\n",
      "[108]\tvalidation_0-logloss:0.42123\n",
      "[109]\tvalidation_0-logloss:0.41971\n",
      "[110]\tvalidation_0-logloss:0.41809\n",
      "[111]\tvalidation_0-logloss:0.41657\n",
      "[112]\tvalidation_0-logloss:0.41485\n",
      "[113]\tvalidation_0-logloss:0.41368\n",
      "[114]\tvalidation_0-logloss:0.41185\n",
      "[115]\tvalidation_0-logloss:0.41031\n",
      "[116]\tvalidation_0-logloss:0.40852\n",
      "[117]\tvalidation_0-logloss:0.40665\n",
      "[118]\tvalidation_0-logloss:0.40499\n",
      "[119]\tvalidation_0-logloss:0.40347\n",
      "[120]\tvalidation_0-logloss:0.40207\n",
      "[121]\tvalidation_0-logloss:0.40097\n",
      "[122]\tvalidation_0-logloss:0.39931\n",
      "[123]\tvalidation_0-logloss:0.39789\n",
      "[124]\tvalidation_0-logloss:0.39698\n",
      "[125]\tvalidation_0-logloss:0.39555\n",
      "[126]\tvalidation_0-logloss:0.39394\n",
      "[127]\tvalidation_0-logloss:0.39255\n",
      "[128]\tvalidation_0-logloss:0.39121\n",
      "[129]\tvalidation_0-logloss:0.39002\n",
      "[130]\tvalidation_0-logloss:0.38862\n",
      "[131]\tvalidation_0-logloss:0.38746\n",
      "[132]\tvalidation_0-logloss:0.38641\n",
      "[133]\tvalidation_0-logloss:0.38570\n",
      "[134]\tvalidation_0-logloss:0.38413\n",
      "[135]\tvalidation_0-logloss:0.38278\n",
      "[136]\tvalidation_0-logloss:0.38211\n",
      "[137]\tvalidation_0-logloss:0.38087\n",
      "[138]\tvalidation_0-logloss:0.37964\n",
      "[139]\tvalidation_0-logloss:0.37837\n",
      "[140]\tvalidation_0-logloss:0.37709\n",
      "[141]\tvalidation_0-logloss:0.37581\n",
      "[142]\tvalidation_0-logloss:0.37485\n",
      "[143]\tvalidation_0-logloss:0.37360\n",
      "[144]\tvalidation_0-logloss:0.37281\n",
      "[145]\tvalidation_0-logloss:0.37164\n",
      "[146]\tvalidation_0-logloss:0.37025\n",
      "[147]\tvalidation_0-logloss:0.36906\n",
      "[148]\tvalidation_0-logloss:0.36769\n",
      "[149]\tvalidation_0-logloss:0.36660\n",
      "[150]\tvalidation_0-logloss:0.36547\n",
      "[151]\tvalidation_0-logloss:0.36404\n",
      "[152]\tvalidation_0-logloss:0.36302\n",
      "[153]\tvalidation_0-logloss:0.36178\n",
      "[154]\tvalidation_0-logloss:0.36071\n",
      "[155]\tvalidation_0-logloss:0.35929\n",
      "[156]\tvalidation_0-logloss:0.35829\n",
      "[157]\tvalidation_0-logloss:0.35713\n",
      "[158]\tvalidation_0-logloss:0.35584\n",
      "[159]\tvalidation_0-logloss:0.35445\n",
      "[160]\tvalidation_0-logloss:0.35316\n",
      "[161]\tvalidation_0-logloss:0.35202\n",
      "[162]\tvalidation_0-logloss:0.35086\n",
      "[163]\tvalidation_0-logloss:0.34989\n",
      "[164]\tvalidation_0-logloss:0.34862\n",
      "[165]\tvalidation_0-logloss:0.34738\n",
      "[166]\tvalidation_0-logloss:0.34686\n",
      "[167]\tvalidation_0-logloss:0.34619\n",
      "[168]\tvalidation_0-logloss:0.34506\n",
      "[169]\tvalidation_0-logloss:0.34416\n",
      "[170]\tvalidation_0-logloss:0.34360\n",
      "[171]\tvalidation_0-logloss:0.34298\n",
      "[172]\tvalidation_0-logloss:0.34204\n",
      "[173]\tvalidation_0-logloss:0.34143\n",
      "[174]\tvalidation_0-logloss:0.34033\n",
      "[175]\tvalidation_0-logloss:0.33972\n",
      "[176]\tvalidation_0-logloss:0.33861\n",
      "[177]\tvalidation_0-logloss:0.33751\n",
      "[178]\tvalidation_0-logloss:0.33670\n",
      "[179]\tvalidation_0-logloss:0.33561\n",
      "[180]\tvalidation_0-logloss:0.33456\n",
      "[181]\tvalidation_0-logloss:0.33330\n",
      "[182]\tvalidation_0-logloss:0.33273\n",
      "[183]\tvalidation_0-logloss:0.33199\n",
      "[184]\tvalidation_0-logloss:0.33099\n",
      "[185]\tvalidation_0-logloss:0.33021\n",
      "[186]\tvalidation_0-logloss:0.32922\n",
      "[187]\tvalidation_0-logloss:0.32832\n",
      "[188]\tvalidation_0-logloss:0.32756\n",
      "[189]\tvalidation_0-logloss:0.32662\n",
      "[190]\tvalidation_0-logloss:0.32553\n",
      "[191]\tvalidation_0-logloss:0.32474\n",
      "[192]\tvalidation_0-logloss:0.32380\n",
      "[193]\tvalidation_0-logloss:0.32312\n",
      "[194]\tvalidation_0-logloss:0.32264\n",
      "[195]\tvalidation_0-logloss:0.32195\n",
      "[196]\tvalidation_0-logloss:0.32128\n",
      "[197]\tvalidation_0-logloss:0.32077\n",
      "[198]\tvalidation_0-logloss:0.32028\n",
      "[199]\tvalidation_0-logloss:0.31924\n",
      "[200]\tvalidation_0-logloss:0.31891\n",
      "[201]\tvalidation_0-logloss:0.31810\n",
      "[202]\tvalidation_0-logloss:0.31731\n",
      "[203]\tvalidation_0-logloss:0.31636\n",
      "[204]\tvalidation_0-logloss:0.31573\n",
      "[205]\tvalidation_0-logloss:0.31495\n",
      "[206]\tvalidation_0-logloss:0.31429\n",
      "[207]\tvalidation_0-logloss:0.31356\n",
      "[208]\tvalidation_0-logloss:0.31281\n",
      "[209]\tvalidation_0-logloss:0.31216\n",
      "[210]\tvalidation_0-logloss:0.31158\n",
      "[211]\tvalidation_0-logloss:0.31066\n",
      "[212]\tvalidation_0-logloss:0.31030\n",
      "[213]\tvalidation_0-logloss:0.30984\n",
      "[214]\tvalidation_0-logloss:0.30923\n",
      "[215]\tvalidation_0-logloss:0.30871\n",
      "[216]\tvalidation_0-logloss:0.30855\n",
      "[217]\tvalidation_0-logloss:0.30813\n",
      "[218]\tvalidation_0-logloss:0.30753\n",
      "[219]\tvalidation_0-logloss:0.30680\n",
      "[220]\tvalidation_0-logloss:0.30615\n",
      "[221]\tvalidation_0-logloss:0.30544\n",
      "[222]\tvalidation_0-logloss:0.30453\n",
      "[223]\tvalidation_0-logloss:0.30377\n",
      "[224]\tvalidation_0-logloss:0.30320\n",
      "[225]\tvalidation_0-logloss:0.30239\n",
      "[226]\tvalidation_0-logloss:0.30192\n",
      "[227]\tvalidation_0-logloss:0.30140\n",
      "[228]\tvalidation_0-logloss:0.30110\n",
      "[229]\tvalidation_0-logloss:0.30041\n",
      "[230]\tvalidation_0-logloss:0.29980\n",
      "[231]\tvalidation_0-logloss:0.29916\n",
      "[232]\tvalidation_0-logloss:0.29867\n",
      "[233]\tvalidation_0-logloss:0.29809\n",
      "[234]\tvalidation_0-logloss:0.29741\n",
      "[235]\tvalidation_0-logloss:0.29670\n",
      "[236]\tvalidation_0-logloss:0.29581\n",
      "[237]\tvalidation_0-logloss:0.29524\n",
      "[238]\tvalidation_0-logloss:0.29487\n",
      "[239]\tvalidation_0-logloss:0.29434\n",
      "[240]\tvalidation_0-logloss:0.29352\n",
      "[241]\tvalidation_0-logloss:0.29297\n",
      "[242]\tvalidation_0-logloss:0.29242\n",
      "[243]\tvalidation_0-logloss:0.29239\n",
      "[244]\tvalidation_0-logloss:0.29207\n",
      "[245]\tvalidation_0-logloss:0.29156\n",
      "[246]\tvalidation_0-logloss:0.29080\n",
      "[247]\tvalidation_0-logloss:0.29034\n",
      "[248]\tvalidation_0-logloss:0.28981\n",
      "[249]\tvalidation_0-logloss:0.28947\n",
      "[250]\tvalidation_0-logloss:0.28889\n",
      "[251]\tvalidation_0-logloss:0.28813\n",
      "[252]\tvalidation_0-logloss:0.28764\n",
      "[253]\tvalidation_0-logloss:0.28705\n",
      "[254]\tvalidation_0-logloss:0.28673\n",
      "[255]\tvalidation_0-logloss:0.28625\n",
      "[256]\tvalidation_0-logloss:0.28583\n",
      "[257]\tvalidation_0-logloss:0.28544\n",
      "[258]\tvalidation_0-logloss:0.28500\n",
      "[259]\tvalidation_0-logloss:0.28470\n",
      "[260]\tvalidation_0-logloss:0.28426\n",
      "[261]\tvalidation_0-logloss:0.28400\n",
      "[262]\tvalidation_0-logloss:0.28347\n",
      "[263]\tvalidation_0-logloss:0.28290\n",
      "[264]\tvalidation_0-logloss:0.28242\n",
      "[265]\tvalidation_0-logloss:0.28189\n",
      "[266]\tvalidation_0-logloss:0.28128\n",
      "[267]\tvalidation_0-logloss:0.28074\n",
      "[268]\tvalidation_0-logloss:0.28034\n",
      "[269]\tvalidation_0-logloss:0.27980\n",
      "[270]\tvalidation_0-logloss:0.27966\n",
      "[271]\tvalidation_0-logloss:0.27915\n",
      "[272]\tvalidation_0-logloss:0.27860\n",
      "[273]\tvalidation_0-logloss:0.27813\n",
      "[274]\tvalidation_0-logloss:0.27764\n",
      "[275]\tvalidation_0-logloss:0.27694\n",
      "[276]\tvalidation_0-logloss:0.27662\n",
      "[277]\tvalidation_0-logloss:0.27622\n",
      "[278]\tvalidation_0-logloss:0.27580\n",
      "[279]\tvalidation_0-logloss:0.27542\n",
      "[280]\tvalidation_0-logloss:0.27496\n",
      "[281]\tvalidation_0-logloss:0.27448\n",
      "[282]\tvalidation_0-logloss:0.27421\n",
      "[283]\tvalidation_0-logloss:0.27383\n",
      "[284]\tvalidation_0-logloss:0.27351\n",
      "[285]\tvalidation_0-logloss:0.27305\n",
      "[286]\tvalidation_0-logloss:0.27272\n",
      "[287]\tvalidation_0-logloss:0.27234\n",
      "[288]\tvalidation_0-logloss:0.27193\n",
      "[289]\tvalidation_0-logloss:0.27141\n",
      "[290]\tvalidation_0-logloss:0.27100\n",
      "[291]\tvalidation_0-logloss:0.27061\n",
      "[292]\tvalidation_0-logloss:0.27017\n",
      "[293]\tvalidation_0-logloss:0.26990\n",
      "[294]\tvalidation_0-logloss:0.26969\n",
      "[295]\tvalidation_0-logloss:0.26932\n",
      "[296]\tvalidation_0-logloss:0.26909\n",
      "[297]\tvalidation_0-logloss:0.26857\n",
      "[298]\tvalidation_0-logloss:0.26818\n",
      "[299]\tvalidation_0-logloss:0.26777\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7, device=None, early_stopping_rounds=50,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.009, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=1,\n",
       "              num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;XGBClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBClassifier\">?<span>Documentation for XGBClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7, device=None, early_stopping_rounds=50,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.009, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=1,\n",
       "              num_parallel_tree=None, ...)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7, device=None, early_stopping_rounds=50,\n",
       "              enable_categorical=False, eval_metric='logloss',\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.009, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=1,\n",
       "              num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model = XGBClassifier(\n",
    "    n_estimators=300,            \n",
    "    max_depth=6,\n",
    "    learning_rate=0.009,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=0.7,\n",
    "    eval_metric='logloss',\n",
    "    reg_lambda = 1.5,\n",
    "    reg_alpha = 0.1,\n",
    "    early_stopping_rounds=50,\n",
    "    random_state=42,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "baseline_model.fit(\n",
    "    X_finetune_train_res,\n",
    "    y_finetune_train_res,\n",
    "    eval_set=[(X_finetune_val, y_finetune_val)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bab295b0-00fb-44d5-be89-b0a3367bbf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy (Dataset B): 83.87%\n",
      "Baseline F1-score (Dataset B): 83.87%\n",
      "Baseline Recall (Sensitivity): 92.86%\n",
      "Baseline Specificity: 76.47%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on Dataset B test set\n",
    "y_pred_baseline = baseline_model.predict(X_finetune_test)\n",
    "\n",
    "baseline_acc = accuracy_score(y_finetune_test, y_pred_baseline) * 100\n",
    "baseline_f1  = f1_score(y_finetune_test, y_pred_baseline) * 100\n",
    "\n",
    "cm = confusion_matrix(y_finetune_test, y_pred_baseline)\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "recall_score_calc = TP / (TP + FN) * 100\n",
    "specificity_score_calc = TN / (TN + FP) * 100\n",
    "\n",
    "\n",
    "print(f\"Baseline Accuracy (Dataset B): {baseline_acc:.2f}%\")\n",
    "print(f\"Baseline F1-score (Dataset B): {baseline_f1:.2f}%\")\n",
    "print(f\"Baseline Recall (Sensitivity): {recall_score_calc:.2f}%\")\n",
    "print(f\"Baseline Specificity: {specificity_score_calc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69ada6c-57c4-4083-b15b-4a9f32ce3e7d",
   "metadata": {},
   "source": [
    "## 6. Tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88547a09-5369-4f68-9e57-6dbef8030156",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2baf0ab6-9b0b-462b-9157-66d45fdc5e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training, validation, and test datasets for the source domain\n",
    "X_A_train = X_pretrain_train_res.values  # Features for training on the source domain (pre-training phase)\n",
    "y_A_train = y_pretrain_train_res.values  # Target labels for training on the source domain\n",
    "\n",
    "X_A_val   = X_pretrain_val.values  # Features for validation on the source domain\n",
    "y_A_val   = y_pretrain_val.values  # Target labels for validation on the source domain\n",
    "\n",
    "X_A_test  = X_pretrain_test.values  # Features for testing on the source domain\n",
    "y_A_test  = y_pretrain_test.values  # Target labels for testing on the source domain\n",
    "\n",
    "# Defining the training, validation, and test datasets for the target domain\n",
    "X_B_train = X_finetune_train_res.values  # Features for training on the target domain (fine-tuning phase)\n",
    "y_B_train = y_finetune_train_res.values  # Target labels for training on the target domain\n",
    "\n",
    "X_B_val   = X_finetune_val.values  # Features for validation on the target domain\n",
    "y_B_val   = y_finetune_val.values  # Target labels for validation on the target domain\n",
    "\n",
    "X_B_test  = X_finetune_test.values  # Features for testing on the target domain\n",
    "y_B_test  = y_finetune_test.values  # Target labels for testing on the target domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71aa0290-a98b-4f3c-916e-427ebc557794",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_A_train = scaler.fit_transform(X_A_train).astype(np.float32)  # Fit scaler on source train set, transform it\n",
    "X_A_val = scaler.transform(X_A_val).astype(np.float32)          # Transform source validation set\n",
    "X_A_test = scaler.transform(X_A_test).astype(np.float32)        # Transform source test set\n",
    "\n",
    "y_A_train = y_A_train.astype(np.int64)  # Convert source train labels to int64\n",
    "y_A_val = y_A_val.astype(np.int64)      # Convert source validation labels\n",
    "y_A_test = y_A_test.astype(np.int64)    # Convert source test labels\n",
    "\n",
    "X_B_train = scaler.transform(X_B_train).astype(np.float32)      # Apply source-domain scaling to target train set\n",
    "X_B_val = scaler.transform(X_B_val).astype(np.float32)          # Apply scaling to target validation set\n",
    "X_B_test = scaler.transform(X_B_test).astype(np.float32)        # Apply scaling to target test set\n",
    "\n",
    "y_B_train = y_B_train.astype(np.int64)  # Convert target train labels to int64\n",
    "y_B_val = y_B_val.astype(np.int64)      # Convert target validation labels\n",
    "y_B_test = y_B_test.astype(np.int64)    # Convert target test labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbc3966-a643-4813-bbcb-b8b2e7db60ab",
   "metadata": {},
   "source": [
    "### Pretraining on the source domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d40a9a9-1054-4cb1-aec6-bd8211b2d002",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning for Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "feb57e27-1f6c-437d-82c0-853b7eb56ff0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-29 22:50:19,621] A new study created in memory with name: no-name-bbda8a31-e4aa-4bbd-8bbc-d30a736d7e4c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_val_balanced_accuracy = 0.81588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 22:50:45,741] Trial 0 finished with value: 0.8235294117647058 and parameters: {'n_d_a': 8, 'n_steps': 10, 'gamma': 1.5213303366613837, 'lambda_sparse': 5.410389733001134e-06, 'lr': 0.01342596619210637, 'momentum': 0.24000000000000002, 'batch_size': 64, 'virtual_batch_size': 32, 'step_size': 80, 'scheduler_gamma': 0.818499861081206}. Best is trial 0 with value: 0.8235294117647058.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 results: Accuracy = 0.8033, F1-Score = 0.8235\n",
      "\n",
      "Early stopping occurred at epoch 59 with best_epoch = 39 and best_val_balanced_accuracy = 0.52083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 22:51:06,406] Trial 1 finished with value: 0.7628865979381443 and parameters: {'n_d_a': 16, 'n_steps': 6, 'gamma': 1.2169440085479426, 'lambda_sparse': 0.00010244960546271797, 'lr': 1.7637096255912015e-05, 'momentum': 0.17, 'batch_size': 128, 'virtual_batch_size': 16, 'step_size': 60, 'scheduler_gamma': 0.8143267286746101}. Best is trial 0 with value: 0.8235294117647058.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 results: Accuracy = 0.6230, F1-Score = 0.7629\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 2 and best_val_balanced_accuracy = 0.52196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 22:51:35,691] Trial 2 finished with value: 0.7391304347826086 and parameters: {'n_d_a': 16, 'n_steps': 9, 'gamma': 1.3772838542029473, 'lambda_sparse': 0.0012147182643342878, 'lr': 4.4391477650325e-05, 'momentum': 0.14, 'batch_size': 32, 'virtual_batch_size': 16, 'step_size': 20, 'scheduler_gamma': 0.972381929489005}. Best is trial 0 with value: 0.8235294117647058.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 results: Accuracy = 0.6066, F1-Score = 0.7391\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_val_balanced_accuracy = 0.82095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 22:51:55,161] Trial 3 finished with value: 0.868421052631579 and parameters: {'n_d_a': 8, 'n_steps': 8, 'gamma': 1.9220570074194403, 'lambda_sparse': 0.00018960152929251446, 'lr': 0.0018429014386945133, 'momentum': 0.31, 'batch_size': 64, 'virtual_batch_size': 32, 'step_size': 20, 'scheduler_gamma': 0.8042307215723511}. Best is trial 3 with value: 0.868421052631579.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 results: Accuracy = 0.8361, F1-Score = 0.8684\n",
      "\n",
      "Early stopping occurred at epoch 40 with best_epoch = 20 and best_val_balanced_accuracy = 0.64077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 22:52:19,574] Trial 4 finished with value: 0.7710843373493976 and parameters: {'n_d_a': 16, 'n_steps': 8, 'gamma': 1.9171355348195878, 'lambda_sparse': 0.0004203641470651379, 'lr': 0.0001039912561129326, 'momentum': 0.13, 'batch_size': 64, 'virtual_batch_size': 32, 'step_size': 40, 'scheduler_gamma': 0.9406610746396492}. Best is trial 3 with value: 0.868421052631579.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 results: Accuracy = 0.6885, F1-Score = 0.7711\n",
      "\n",
      "Early stopping occurred at epoch 50 with best_epoch = 30 and best_val_balanced_accuracy = 0.57714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 22:52:36,389] Trial 5 finished with value: 0.7692307692307693 and parameters: {'n_d_a': 16, 'n_steps': 8, 'gamma': 1.0656518045688657, 'lambda_sparse': 7.166146405054393e-05, 'lr': 2.6190820019582343e-05, 'momentum': 0.11, 'batch_size': 128, 'virtual_batch_size': 32, 'step_size': 80, 'scheduler_gamma': 0.8984430057811911}. Best is trial 3 with value: 0.868421052631579.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 results: Accuracy = 0.6557, F1-Score = 0.7692\n",
      "\n",
      "Early stopping occurred at epoch 49 with best_epoch = 29 and best_val_balanced_accuracy = 0.64302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 22:53:10,358] Trial 6 finished with value: 0.7123287671232876 and parameters: {'n_d_a': 24, 'n_steps': 4, 'gamma': 1.2625119200618897, 'lambda_sparse': 8.104964390129223e-05, 'lr': 6.227907072548271e-05, 'momentum': 0.11, 'batch_size': 32, 'virtual_batch_size': 16, 'step_size': 70, 'scheduler_gamma': 0.9745848890014654}. Best is trial 3 with value: 0.868421052631579.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 results: Accuracy = 0.6557, F1-Score = 0.7123\n",
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 3 and best_val_balanced_accuracy = 0.84291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 22:53:20,156] Trial 7 finished with value: 0.8571428571428571 and parameters: {'n_d_a': 16, 'n_steps': 9, 'gamma': 1.713032600489249, 'lambda_sparse': 2.1129961427347326e-06, 'lr': 0.026017830155790147, 'momentum': 0.3, 'batch_size': 128, 'virtual_batch_size': 32, 'step_size': 20, 'scheduler_gamma': 0.8748846634237243}. Best is trial 3 with value: 0.868421052631579.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 results: Accuracy = 0.8361, F1-Score = 0.8571\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_balanced_accuracy = 0.49493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 22:53:32,031] Trial 8 finished with value: 0.7111111111111111 and parameters: {'n_d_a': 8, 'n_steps': 10, 'gamma': 1.2370498443341091, 'lambda_sparse': 4.031704677136463e-05, 'lr': 0.00012576354450521905, 'momentum': 0.03, 'batch_size': 128, 'virtual_batch_size': 16, 'step_size': 60, 'scheduler_gamma': 0.8413542259560896}. Best is trial 3 with value: 0.868421052631579.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 results: Accuracy = 0.5738, F1-Score = 0.7111\n",
      "\n",
      "Early stopping occurred at epoch 49 with best_epoch = 29 and best_val_balanced_accuracy = 0.86261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 22:54:30,260] Trial 9 finished with value: 0.8918918918918919 and parameters: {'n_d_a': 24, 'n_steps': 8, 'gamma': 1.6885600549362636, 'lambda_sparse': 2.8716801056848872e-05, 'lr': 0.010630745765207688, 'momentum': 0.03, 'batch_size': 32, 'virtual_batch_size': 16, 'step_size': 80, 'scheduler_gamma': 0.9710378005624937}. Best is trial 9 with value: 0.8918918918918919.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 results: Accuracy = 0.8689, F1-Score = 0.8919\n",
      "\n",
      "Early stopping occurred at epoch 53 with best_epoch = 33 and best_val_balanced_accuracy = 0.85529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 22:55:19,458] Trial 10 finished with value: 0.8947368421052632 and parameters: {'n_d_a': 32, 'n_steps': 6, 'gamma': 1.653956992579569, 'lambda_sparse': 0.0071352940992739445, 'lr': 0.08738079416760637, 'momentum': 0.01, 'batch_size': 32, 'virtual_batch_size': 16, 'step_size': 40, 'scheduler_gamma': 0.9199664752544251}. Best is trial 10 with value: 0.8947368421052632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 results: Accuracy = 0.8689, F1-Score = 0.8947\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 5 and best_val_balanced_accuracy = 0.82827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 22:55:44,880] Trial 11 finished with value: 0.8648648648648649 and parameters: {'n_d_a': 32, 'n_steps': 6, 'gamma': 1.642315121942996, 'lambda_sparse': 0.009822421841518512, 'lr': 0.09345138729220054, 'momentum': 0.01, 'batch_size': 32, 'virtual_batch_size': 16, 'step_size': 40, 'scheduler_gamma': 0.9286832550462242}. Best is trial 10 with value: 0.8947368421052632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 11 results: Accuracy = 0.8361, F1-Score = 0.8649\n",
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 14 and best_val_balanced_accuracy = 0.82939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 22:56:17,029] Trial 12 finished with value: 0.8405797101449275 and parameters: {'n_d_a': 32, 'n_steps': 6, 'gamma': 1.7264472543313818, 'lambda_sparse': 0.009815277977733744, 'lr': 0.004162291150038078, 'momentum': 0.05, 'batch_size': 32, 'virtual_batch_size': 16, 'step_size': 40, 'scheduler_gamma': 0.9330066911301361}. Best is trial 10 with value: 0.8947368421052632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 12 results: Accuracy = 0.8197, F1-Score = 0.8406\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_balanced_accuracy = 0.8491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 22:56:45,648] Trial 13 finished with value: 0.8767123287671232 and parameters: {'n_d_a': 24, 'n_steps': 4, 'gamma': 1.5658215988402213, 'lambda_sparse': 1.533337572138197e-05, 'lr': 0.0811135327427699, 'momentum': 0.06999999999999999, 'batch_size': 32, 'virtual_batch_size': 16, 'step_size': 50, 'scheduler_gamma': 0.9012108693048789}. Best is trial 10 with value: 0.8947368421052632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13 results: Accuracy = 0.8525, F1-Score = 0.8767\n",
      "\n",
      "Early stopping occurred at epoch 52 with best_epoch = 32 and best_val_balanced_accuracy = 0.86261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 22:58:46,094] Trial 14 finished with value: 0.8918918918918919 and parameters: {'n_d_a': 32, 'n_steps': 7, 'gamma': 1.8029000566405253, 'lambda_sparse': 0.001431577558782822, 'lr': 0.011476610065719284, 'momentum': 0.21000000000000002, 'batch_size': 32, 'virtual_batch_size': 16, 'step_size': 50, 'scheduler_gamma': 0.9528847015976063}. Best is trial 10 with value: 0.8947368421052632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 14 results: Accuracy = 0.8689, F1-Score = 0.8919\n",
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 7 and best_val_balanced_accuracy = 0.86261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 22:59:41,553] Trial 15 finished with value: 0.8918918918918919 and parameters: {'n_d_a': 24, 'n_steps': 5, 'gamma': 1.4285509288879212, 'lambda_sparse': 1.3373549361967474e-05, 'lr': 0.030643125950367577, 'momentum': 0.38, 'batch_size': 32, 'virtual_batch_size': 16, 'step_size': 30, 'scheduler_gamma': 0.8697609975218107}. Best is trial 10 with value: 0.8947368421052632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 results: Accuracy = 0.8689, F1-Score = 0.8919\n",
      "\n",
      "Early stopping occurred at epoch 50 with best_epoch = 30 and best_val_balanced_accuracy = 0.82207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 23:01:52,989] Trial 16 finished with value: 0.8450704225352113 and parameters: {'n_d_a': 32, 'n_steps': 7, 'gamma': 1.9952946042149973, 'lambda_sparse': 0.001894821100149528, 'lr': 0.0006807733619712546, 'momentum': 0.060000000000000005, 'batch_size': 32, 'virtual_batch_size': 16, 'step_size': 70, 'scheduler_gamma': 0.9125637029256918}. Best is trial 10 with value: 0.8947368421052632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 16 results: Accuracy = 0.8197, F1-Score = 0.8451\n",
      "\n",
      "Early stopping occurred at epoch 63 with best_epoch = 43 and best_val_balanced_accuracy = 0.85642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 23:03:55,227] Trial 17 finished with value: 0.8732394366197183 and parameters: {'n_d_a': 24, 'n_steps': 5, 'gamma': 1.8080781254654097, 'lambda_sparse': 2.0153132622979908e-05, 'lr': 0.003651351555928336, 'momentum': 0.01, 'batch_size': 32, 'virtual_batch_size': 16, 'step_size': 60, 'scheduler_gamma': 0.9564458604455907}. Best is trial 10 with value: 0.8947368421052632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 17 results: Accuracy = 0.8525, F1-Score = 0.8732\n",
      "\n",
      "Early stopping occurred at epoch 45 with best_epoch = 25 and best_val_balanced_accuracy = 0.82207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 23:05:48,475] Trial 18 finished with value: 0.8450704225352113 and parameters: {'n_d_a': 32, 'n_steps': 7, 'gamma': 1.5800332973195446, 'lambda_sparse': 1.2948214118603603e-06, 'lr': 0.00044186008478279214, 'momentum': 0.08, 'batch_size': 32, 'virtual_batch_size': 16, 'step_size': 30, 'scheduler_gamma': 0.9170202357728412}. Best is trial 10 with value: 0.8947368421052632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 18 results: Accuracy = 0.8197, F1-Score = 0.8451\n",
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 10 and best_val_balanced_accuracy = 0.82207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 23:06:52,262] Trial 19 finished with value: 0.8450704225352113 and parameters: {'n_d_a': 24, 'n_steps': 9, 'gamma': 1.3893349416284, 'lambda_sparse': 0.00048682957895504356, 'lr': 0.029248975918370217, 'momentum': 0.17, 'batch_size': 64, 'virtual_batch_size': 16, 'step_size': 70, 'scheduler_gamma': 0.8790260223663894}. Best is trial 10 with value: 0.8947368421052632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 19 results: Accuracy = 0.8197, F1-Score = 0.8451\n",
      "\n",
      "Early stopping occurred at epoch 37 with best_epoch = 17 and best_val_balanced_accuracy = 0.82939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 23:08:03,178] Trial 20 finished with value: 0.8405797101449275 and parameters: {'n_d_a': 32, 'n_steps': 5, 'gamma': 1.6579464069909144, 'lambda_sparse': 4.2174500472724605e-06, 'lr': 0.007369260206584333, 'momentum': 0.24000000000000002, 'batch_size': 32, 'virtual_batch_size': 16, 'step_size': 30, 'scheduler_gamma': 0.8494739835105347}. Best is trial 10 with value: 0.8947368421052632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 results: Accuracy = 0.8197, F1-Score = 0.8406\n",
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 6 and best_val_balanced_accuracy = 0.86261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 23:09:08,986] Trial 21 finished with value: 0.8918918918918919 and parameters: {'n_d_a': 32, 'n_steps': 7, 'gamma': 1.8187273929128114, 'lambda_sparse': 0.003342339323325052, 'lr': 0.012374050860024573, 'momentum': 0.22, 'batch_size': 32, 'virtual_batch_size': 16, 'step_size': 50, 'scheduler_gamma': 0.9524080844627302}. Best is trial 10 with value: 0.8947368421052632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 21 results: Accuracy = 0.8689, F1-Score = 0.8919\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 15 and best_val_balanced_accuracy = 0.8491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 23:10:57,880] Trial 22 finished with value: 0.8767123287671232 and parameters: {'n_d_a': 32, 'n_steps': 7, 'gamma': 1.7741755406804633, 'lambda_sparse': 0.003675212293114865, 'lr': 0.050294795347091754, 'momentum': 0.28, 'batch_size': 32, 'virtual_batch_size': 16, 'step_size': 50, 'scheduler_gamma': 0.9582341994481751}. Best is trial 10 with value: 0.8947368421052632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 22 results: Accuracy = 0.8525, F1-Score = 0.8767\n",
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 13 and best_val_balanced_accuracy = 0.85642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 23:12:14,753] Trial 23 finished with value: 0.8732394366197183 and parameters: {'n_d_a': 24, 'n_steps': 6, 'gamma': 1.8636296560320442, 'lambda_sparse': 0.0005133516655906477, 'lr': 0.0017297901427154295, 'momentum': 0.19, 'batch_size': 32, 'virtual_batch_size': 16, 'step_size': 50, 'scheduler_gamma': 0.9494454223807756}. Best is trial 10 with value: 0.8947368421052632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 23 results: Accuracy = 0.8525, F1-Score = 0.8732\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_val_balanced_accuracy = 0.82207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 23:13:36,213] Trial 24 finished with value: 0.8450704225352113 and parameters: {'n_d_a': 32, 'n_steps': 8, 'gamma': 1.6419859742228073, 'lambda_sparse': 0.00433714178241383, 'lr': 0.0074901428387888404, 'momentum': 0.36000000000000004, 'batch_size': 32, 'virtual_batch_size': 16, 'step_size': 40, 'scheduler_gamma': 0.9778694854051286}. Best is trial 10 with value: 0.8947368421052632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 24 results: Accuracy = 0.8197, F1-Score = 0.8451\n",
      "\n",
      "Early stopping occurred at epoch 40 with best_epoch = 20 and best_val_balanced_accuracy = 0.8491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 23:57:16,828] Trial 25 finished with value: 0.8767123287671232 and parameters: {'n_d_a': 24, 'n_steps': 6, 'gamma': 1.7373038564603938, 'lambda_sparse': 0.0014312062156990168, 'lr': 0.0127026042834243, 'momentum': 0.09, 'batch_size': 32, 'virtual_batch_size': 16, 'step_size': 60, 'scheduler_gamma': 0.9207373774849952}. Best is trial 10 with value: 0.8947368421052632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 25 results: Accuracy = 0.8525, F1-Score = 0.8767\n",
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 16 and best_val_balanced_accuracy = 0.80743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 23:57:54,457] Trial 26 finished with value: 0.8533333333333334 and parameters: {'n_d_a': 32, 'n_steps': 7, 'gamma': 1.5938083620815644, 'lambda_sparse': 0.00022432731604777229, 'lr': 0.00027639575871573425, 'momentum': 0.15000000000000002, 'batch_size': 32, 'virtual_batch_size': 32, 'step_size': 40, 'scheduler_gamma': 0.940452181097548}. Best is trial 10 with value: 0.8947368421052632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 26 results: Accuracy = 0.8197, F1-Score = 0.8533\n",
      "\n",
      "Early stopping occurred at epoch 50 with best_epoch = 30 and best_val_balanced_accuracy = 0.83559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 23:58:55,173] Trial 27 finished with value: 0.8611111111111112 and parameters: {'n_d_a': 24, 'n_steps': 8, 'gamma': 1.4907155075230714, 'lambda_sparse': 0.0008937513535196862, 'lr': 0.0020600050629223368, 'momentum': 0.05, 'batch_size': 32, 'virtual_batch_size': 16, 'step_size': 50, 'scheduler_gamma': 0.9637049821325204}. Best is trial 10 with value: 0.8947368421052632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 27 results: Accuracy = 0.8361, F1-Score = 0.8611\n",
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 21 and best_val_balanced_accuracy = 0.87613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 23:59:20,799] Trial 28 finished with value: 0.9066666666666666 and parameters: {'n_d_a': 32, 'n_steps': 7, 'gamma': 1.8698558098046372, 'lambda_sparse': 0.005393173126575441, 'lr': 0.04179737727422789, 'momentum': 0.27, 'batch_size': 64, 'virtual_batch_size': 16, 'step_size': 30, 'scheduler_gamma': 0.9421521438008308}. Best is trial 28 with value: 0.9066666666666666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 28 results: Accuracy = 0.8852, F1-Score = 0.9067\n",
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 11 and best_val_balanced_accuracy = 0.82207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-29 23:59:43,820] Trial 29 finished with value: 0.8450704225352113 and parameters: {'n_d_a': 32, 'n_steps': 10, 'gamma': 1.4776803433276946, 'lambda_sparse': 7.632793437647826e-06, 'lr': 0.05179155526814573, 'momentum': 0.26, 'batch_size': 64, 'virtual_batch_size': 32, 'step_size': 80, 'scheduler_gamma': 0.9052827819153031}. Best is trial 28 with value: 0.9066666666666666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 29 results: Accuracy = 0.8197, F1-Score = 0.8451\n"
     ]
    }
   ],
   "source": [
    "# Objective function to optimize\n",
    "def objective(trial):\n",
    "# n_d and n_a (often kept equal for simplicity)\n",
    "    n_d_a = trial.suggest_int('n_d_a', 8, 32, step=8) \n",
    "    n_steps = trial.suggest_int('n_steps', 4, 10) \n",
    "    gamma = trial.suggest_float('gamma', 1.0, 2.0)\n",
    "    lambda_sparse = trial.suggest_float('lambda_sparse', 1e-6, 1e-2, log=True)\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    momentum = trial.suggest_float('momentum', 0.01, 0.4, step=0.01)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    virtual_batch_size = trial.suggest_categorical('virtual_batch_size', [16, 32])\n",
    "    step_size = trial.suggest_int('step_size', 20, 80, step=10)\n",
    "    scheduler_gamma = trial.suggest_float('scheduler_gamma', 0.8, 0.98)\n",
    "\n",
    "    tabnet_model = TabNetClassifier(\n",
    "        n_d=n_d_a,\n",
    "        n_a=n_d_a,\n",
    "        n_steps=n_steps,\n",
    "        gamma=gamma,\n",
    "        lambda_sparse=lambda_sparse,\n",
    "        momentum=momentum,\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params={'lr': lr},\n",
    "        mask_type='sparsemax',\n",
    "        scheduler_params={'step_size': step_size, 'gamma': scheduler_gamma},\n",
    "        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "        verbose=0, # Crucial to silence output during 50+ trials\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    tabnet_model.fit(\n",
    "        X_train=X_A_train, \n",
    "        y_train=y_A_train, \n",
    "        eval_set=[(X_A_val, y_A_val)], \n",
    "        eval_name=[\"val\"], \n",
    "        # Use a supported metric for early stopping\n",
    "        eval_metric=[\"balanced_accuracy\"], \n",
    "        max_epochs=100, \n",
    "        patience=20, # Reduced patience for faster tuning\n",
    "        batch_size=batch_size, \n",
    "        virtual_batch_size=virtual_batch_size\n",
    "    )\n",
    "    \n",
    "    # Predict and calculate the final metric to maximize\n",
    "    preds = tabnet_model.predict(X_A_val)\n",
    "    f1 = f1_score(y_A_val, preds)\n",
    "\n",
    "    accuracy = accuracy_score(y_A_val, preds)\n",
    "    \n",
    "    # Print the results of the current trial for monitoring\n",
    "    print(f\"Trial {trial.number} results: Accuracy = {accuracy:.4f}, F1-Score = {f1:.4f}\")\n",
    "        \n",
    "    return f1 \n",
    "\n",
    "# --- Run the Optimization ---\n",
    "study_scratch = optuna.create_study(direction='maximize')\n",
    "study_scratch.optimize(objective, n_trials=30) # Recommend a higher trial count for full tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "db6e5139-db7e-4c28-8d68-f0005c79e622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best pretrain params:\n",
      "  Value (F1-score): 0.9067\n",
      "  Params:\n",
      "    n_d_a: 32\n",
      "    n_steps: 7\n",
      "    gamma: 1.8698558098046372\n",
      "    lambda_sparse: 0.005393173126575441\n",
      "    lr: 0.04179737727422789\n",
      "    momentum: 0.27\n",
      "    batch_size: 64\n",
      "    virtual_batch_size: 16\n",
      "    step_size: 30\n",
      "    scheduler_gamma: 0.9421521438008308\n"
     ]
    }
   ],
   "source": [
    "# Get best trial and parameters\n",
    "best_trial = study_scratch.best_trial\n",
    "best_pretrain_params = best_trial.params\n",
    "\n",
    "print(\"Best pretrain params:\")\n",
    "print(f\"  Value (F1-score): {best_trial.value:.4f}\")\n",
    "print(\"  Params:\")\n",
    "\n",
    "for key, value in best_pretrain_params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e578e93b-d85d-451f-9949-1eb687a6bc80",
   "metadata": {},
   "source": [
    "#### Pre-training on the source domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "79f43b5b-4246-48f2-bf12-642d16222412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_d_a: 16\n",
      "n_steps: 9\n",
      "gamma: 1.7899467948557366\n",
      "lambda_sparse: 4.387608964907318e-05\n",
      "lr: 0.003074702431245894\n",
      "momentum: 0.22\n",
      "batch_size: 32\n",
      "virtual_batch_size: 32\n",
      "step_size: 40\n",
      "scheduler_gamma: 0.9464940708357479\n"
     ]
    }
   ],
   "source": [
    "# Define the best parameters found during the Grid Search for pretraining on Dataset A......DO NOT RUN THIS CODE IF YOU DO HYPERPARAMETER TUNING\n",
    "best_pretrain_params = {\n",
    "    \"n_d_a\": 16,\n",
    "    \"n_steps\": 9,\n",
    "    \"gamma\": 1.7899467948557366,\n",
    "    \"lambda_sparse\": 4.387608964907318e-05,\n",
    "    \"lr\": 0.003074702431245894,\n",
    "    \"momentum\": 0.22,\n",
    "    \"batch_size\": 32,\n",
    "    \"virtual_batch_size\": 32,\n",
    "    \"step_size\": 40,\n",
    "    \"scheduler_gamma\": 0.9464940708357479,\n",
    "}\n",
    "for key, value in best_pretrain_params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4e6cf23c-94a0-4d00-b5b7-94b8f78973a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.21364 | val_logloss: 1.03659 |  0:00:01s\n",
      "epoch 1  | loss: 0.79312 | val_logloss: 0.87236 |  0:00:03s\n",
      "epoch 2  | loss: 0.65424 | val_logloss: 0.55415 |  0:00:04s\n",
      "epoch 3  | loss: 0.6064  | val_logloss: 0.69821 |  0:00:06s\n",
      "epoch 4  | loss: 0.53383 | val_logloss: 0.55025 |  0:00:08s\n",
      "epoch 5  | loss: 0.4903  | val_logloss: 0.49206 |  0:00:09s\n",
      "epoch 6  | loss: 0.50623 | val_logloss: 0.46262 |  0:00:11s\n",
      "epoch 7  | loss: 0.4663  | val_logloss: 0.53205 |  0:00:13s\n",
      "epoch 8  | loss: 0.46921 | val_logloss: 0.58841 |  0:00:15s\n",
      "epoch 9  | loss: 0.47955 | val_logloss: 0.59093 |  0:00:16s\n",
      "epoch 10 | loss: 0.46567 | val_logloss: 0.53411 |  0:00:18s\n",
      "epoch 11 | loss: 0.42929 | val_logloss: 0.44199 |  0:00:19s\n",
      "epoch 12 | loss: 0.43843 | val_logloss: 0.53864 |  0:00:21s\n",
      "epoch 13 | loss: 0.43684 | val_logloss: 0.48473 |  0:00:22s\n",
      "epoch 14 | loss: 0.40049 | val_logloss: 0.46563 |  0:00:24s\n",
      "epoch 15 | loss: 0.41974 | val_logloss: 0.55322 |  0:00:25s\n",
      "epoch 16 | loss: 0.40758 | val_logloss: 0.42258 |  0:00:27s\n",
      "epoch 17 | loss: 0.37546 | val_logloss: 0.53323 |  0:00:29s\n",
      "epoch 18 | loss: 0.41949 | val_logloss: 0.5033  |  0:00:30s\n",
      "epoch 19 | loss: 0.40517 | val_logloss: 0.53719 |  0:00:32s\n",
      "epoch 20 | loss: 0.38633 | val_logloss: 0.48165 |  0:00:33s\n",
      "epoch 21 | loss: 0.36847 | val_logloss: 0.49956 |  0:00:35s\n",
      "epoch 22 | loss: 0.40491 | val_logloss: 0.45653 |  0:00:37s\n",
      "epoch 23 | loss: 0.40175 | val_logloss: 0.53628 |  0:00:38s\n",
      "epoch 24 | loss: 0.40812 | val_logloss: 0.52744 |  0:00:40s\n",
      "epoch 25 | loss: 0.37205 | val_logloss: 0.49454 |  0:00:41s\n",
      "epoch 26 | loss: 0.42724 | val_logloss: 0.45027 |  0:00:43s\n",
      "epoch 27 | loss: 0.3534  | val_logloss: 0.51327 |  0:00:45s\n",
      "epoch 28 | loss: 0.38214 | val_logloss: 0.60029 |  0:00:46s\n",
      "epoch 29 | loss: 0.43604 | val_logloss: 0.44041 |  0:00:48s\n",
      "epoch 30 | loss: 0.37854 | val_logloss: 0.40392 |  0:00:49s\n",
      "epoch 31 | loss: 0.36674 | val_logloss: 0.42045 |  0:00:51s\n",
      "epoch 32 | loss: 0.41555 | val_logloss: 0.38889 |  0:00:53s\n",
      "epoch 33 | loss: 0.36124 | val_logloss: 0.39741 |  0:00:54s\n",
      "epoch 34 | loss: 0.37411 | val_logloss: 0.48098 |  0:00:56s\n",
      "epoch 35 | loss: 0.38373 | val_logloss: 0.42283 |  0:00:57s\n",
      "epoch 36 | loss: 0.37533 | val_logloss: 0.3955  |  0:00:59s\n",
      "epoch 37 | loss: 0.36249 | val_logloss: 0.64829 |  0:01:01s\n",
      "epoch 38 | loss: 0.37468 | val_logloss: 0.44046 |  0:01:03s\n",
      "epoch 39 | loss: 0.37329 | val_logloss: 0.61148 |  0:01:04s\n",
      "epoch 40 | loss: 0.44057 | val_logloss: 0.46411 |  0:01:06s\n",
      "epoch 41 | loss: 0.34593 | val_logloss: 0.48457 |  0:01:08s\n",
      "epoch 42 | loss: 0.37121 | val_logloss: 0.5053  |  0:01:10s\n",
      "epoch 43 | loss: 0.35215 | val_logloss: 0.46895 |  0:01:12s\n",
      "epoch 44 | loss: 0.34898 | val_logloss: 0.4676  |  0:01:14s\n",
      "epoch 45 | loss: 0.35125 | val_logloss: 0.49337 |  0:01:16s\n",
      "epoch 46 | loss: 0.34718 | val_logloss: 0.44733 |  0:01:17s\n",
      "epoch 47 | loss: 0.33936 | val_logloss: 0.46599 |  0:01:19s\n",
      "epoch 48 | loss: 0.35585 | val_logloss: 0.44051 |  0:01:21s\n",
      "epoch 49 | loss: 0.33832 | val_logloss: 0.45882 |  0:01:23s\n",
      "epoch 50 | loss: 0.37378 | val_logloss: 0.47202 |  0:01:25s\n",
      "epoch 51 | loss: 0.34563 | val_logloss: 0.52779 |  0:01:27s\n",
      "epoch 52 | loss: 0.36071 | val_logloss: 0.47257 |  0:01:28s\n",
      "epoch 53 | loss: 0.35965 | val_logloss: 0.45399 |  0:01:30s\n",
      "epoch 54 | loss: 0.35026 | val_logloss: 0.40842 |  0:01:31s\n",
      "epoch 55 | loss: 0.36428 | val_logloss: 0.52241 |  0:01:33s\n",
      "epoch 56 | loss: 0.36017 | val_logloss: 0.47733 |  0:01:34s\n",
      "epoch 57 | loss: 0.31402 | val_logloss: 0.46098 |  0:01:36s\n",
      "epoch 58 | loss: 0.3391  | val_logloss: 0.40029 |  0:01:37s\n",
      "epoch 59 | loss: 0.34277 | val_logloss: 0.38818 |  0:01:39s\n",
      "epoch 60 | loss: 0.33725 | val_logloss: 0.39439 |  0:01:40s\n",
      "epoch 61 | loss: 0.34463 | val_logloss: 0.43517 |  0:01:42s\n",
      "epoch 62 | loss: 0.375   | val_logloss: 0.45344 |  0:01:43s\n",
      "epoch 63 | loss: 0.34717 | val_logloss: 0.41998 |  0:01:45s\n",
      "epoch 64 | loss: 0.30625 | val_logloss: 0.57424 |  0:01:46s\n",
      "epoch 65 | loss: 0.3322  | val_logloss: 0.45803 |  0:01:48s\n",
      "epoch 66 | loss: 0.3345  | val_logloss: 0.40454 |  0:01:49s\n",
      "epoch 67 | loss: 0.34376 | val_logloss: 0.36115 |  0:01:51s\n",
      "epoch 68 | loss: 0.3169  | val_logloss: 0.4424  |  0:01:52s\n",
      "epoch 69 | loss: 0.3182  | val_logloss: 0.4569  |  0:01:54s\n",
      "epoch 70 | loss: 0.30706 | val_logloss: 0.44153 |  0:01:55s\n",
      "epoch 71 | loss: 0.30564 | val_logloss: 0.4098  |  0:01:57s\n",
      "epoch 72 | loss: 0.29289 | val_logloss: 0.43236 |  0:01:58s\n",
      "epoch 73 | loss: 0.32352 | val_logloss: 0.43008 |  0:02:00s\n",
      "epoch 74 | loss: 0.31261 | val_logloss: 0.41132 |  0:02:02s\n",
      "epoch 75 | loss: 0.31033 | val_logloss: 0.4173  |  0:02:03s\n",
      "epoch 76 | loss: 0.3139  | val_logloss: 0.37414 |  0:02:04s\n",
      "epoch 77 | loss: 0.32519 | val_logloss: 0.39135 |  0:02:06s\n",
      "epoch 78 | loss: 0.29784 | val_logloss: 0.44724 |  0:02:08s\n",
      "epoch 79 | loss: 0.29168 | val_logloss: 0.45871 |  0:02:09s\n",
      "epoch 80 | loss: 0.28412 | val_logloss: 0.42653 |  0:02:11s\n",
      "epoch 81 | loss: 0.2971  | val_logloss: 0.4173  |  0:02:12s\n",
      "epoch 82 | loss: 0.27447 | val_logloss: 0.36715 |  0:02:14s\n",
      "epoch 83 | loss: 0.28735 | val_logloss: 0.3616  |  0:02:15s\n",
      "epoch 84 | loss: 0.31415 | val_logloss: 0.4022  |  0:02:17s\n",
      "epoch 85 | loss: 0.30245 | val_logloss: 0.40624 |  0:02:18s\n",
      "epoch 86 | loss: 0.27899 | val_logloss: 0.43431 |  0:02:19s\n",
      "epoch 87 | loss: 0.31808 | val_logloss: 0.36999 |  0:02:21s\n",
      "epoch 88 | loss: 0.29381 | val_logloss: 0.36118 |  0:02:23s\n",
      "epoch 89 | loss: 0.26261 | val_logloss: 0.37114 |  0:02:24s\n",
      "epoch 90 | loss: 0.26055 | val_logloss: 0.3858  |  0:02:26s\n",
      "epoch 91 | loss: 0.26257 | val_logloss: 0.41009 |  0:02:27s\n",
      "epoch 92 | loss: 0.26964 | val_logloss: 0.37279 |  0:02:29s\n",
      "epoch 93 | loss: 0.28125 | val_logloss: 0.32616 |  0:02:30s\n",
      "epoch 94 | loss: 0.31629 | val_logloss: 0.39561 |  0:02:32s\n",
      "epoch 95 | loss: 0.27685 | val_logloss: 0.40175 |  0:02:33s\n",
      "epoch 96 | loss: 0.26251 | val_logloss: 0.44152 |  0:02:35s\n",
      "epoch 97 | loss: 0.28802 | val_logloss: 0.32493 |  0:02:36s\n",
      "epoch 98 | loss: 0.29367 | val_logloss: 0.32724 |  0:02:38s\n",
      "epoch 99 | loss: 0.27225 | val_logloss: 0.33861 |  0:02:39s\n",
      "epoch 100| loss: 0.27879 | val_logloss: 0.36244 |  0:02:41s\n",
      "epoch 101| loss: 0.272   | val_logloss: 0.40353 |  0:02:42s\n",
      "epoch 102| loss: 0.28283 | val_logloss: 0.39964 |  0:02:44s\n",
      "epoch 103| loss: 0.30847 | val_logloss: 0.36811 |  0:02:45s\n",
      "epoch 104| loss: 0.29549 | val_logloss: 0.36767 |  0:02:47s\n",
      "epoch 105| loss: 0.30798 | val_logloss: 0.44554 |  0:02:48s\n",
      "epoch 106| loss: 0.29234 | val_logloss: 0.38522 |  0:02:49s\n",
      "epoch 107| loss: 0.27869 | val_logloss: 0.37435 |  0:02:51s\n",
      "epoch 108| loss: 0.27733 | val_logloss: 0.35729 |  0:02:52s\n",
      "epoch 109| loss: 0.2938  | val_logloss: 0.36866 |  0:02:54s\n",
      "epoch 110| loss: 0.25986 | val_logloss: 0.41006 |  0:02:55s\n",
      "epoch 111| loss: 0.32115 | val_logloss: 0.47359 |  0:02:57s\n",
      "epoch 112| loss: 0.2633  | val_logloss: 0.41832 |  0:02:58s\n",
      "epoch 113| loss: 0.25932 | val_logloss: 0.43033 |  0:03:00s\n",
      "epoch 114| loss: 0.26161 | val_logloss: 0.49206 |  0:03:02s\n",
      "epoch 115| loss: 0.2657  | val_logloss: 0.45841 |  0:03:03s\n",
      "epoch 116| loss: 0.29692 | val_logloss: 0.54386 |  0:03:05s\n",
      "epoch 117| loss: 0.26819 | val_logloss: 0.5414  |  0:03:06s\n",
      "epoch 118| loss: 0.24652 | val_logloss: 0.49986 |  0:03:08s\n",
      "epoch 119| loss: 0.29358 | val_logloss: 0.49227 |  0:03:09s\n",
      "epoch 120| loss: 0.2385  | val_logloss: 0.49272 |  0:03:11s\n",
      "epoch 121| loss: 0.2634  | val_logloss: 0.46037 |  0:03:12s\n",
      "epoch 122| loss: 0.28116 | val_logloss: 0.46909 |  0:03:14s\n",
      "epoch 123| loss: 0.28007 | val_logloss: 0.47436 |  0:03:15s\n",
      "epoch 124| loss: 0.26708 | val_logloss: 0.41538 |  0:03:17s\n",
      "epoch 125| loss: 0.27909 | val_logloss: 0.35476 |  0:03:18s\n",
      "epoch 126| loss: 0.26677 | val_logloss: 0.4252  |  0:03:19s\n",
      "epoch 127| loss: 0.25523 | val_logloss: 0.38179 |  0:03:21s\n",
      "\n",
      "Early stopping occurred at epoch 127 with best_epoch = 97 and best_val_logloss = 0.32493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at tabnet_pretrain_model.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tabnet_pretrain_model.zip'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .....UNCOMMENT THIS LINE IF YOU DO HYPERPARAMETER TUNING\n",
    "# best_pretrain_params = study_scratch.best_params\n",
    "\n",
    "tabnet_pretrain = TabNetClassifier(\n",
    "    n_d=best_pretrain_params['n_d_a'],           # split n_d_a into n_d\n",
    "    n_a=best_pretrain_params['n_d_a'],           # and n_a\n",
    "    n_steps=best_pretrain_params['n_steps'],\n",
    "    gamma=best_pretrain_params['gamma'],\n",
    "    lambda_sparse=best_pretrain_params['lambda_sparse'],\n",
    "    momentum=best_pretrain_params['momentum'],\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params={'lr': best_pretrain_params['lr']},\n",
    "    mask_type='sparsemax',\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    scheduler_params={'step_size': best_pretrain_params['step_size'], 'gamma': best_pretrain_params['scheduler_gamma']},\n",
    "    verbose=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "\n",
    "# Train the final model\n",
    "tabnet_pretrain.fit(\n",
    "    X_train=X_A_train,\n",
    "    y_train=y_A_train,\n",
    "    eval_set=[(X_A_val, y_A_val)],\n",
    "    eval_name=[\"val\"],\n",
    "    eval_metric=[\"logloss\"],\n",
    "    max_epochs=150,\n",
    "    patience=30,\n",
    "    batch_size=best_pretrain_params['batch_size'],\n",
    "    virtual_batch_size=best_pretrain_params['virtual_batch_size'],\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "tabnet_pretrain.save_model(\"tabnet_pretrain_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c8244220-7390-4797-a19a-4d7e158111ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training, test on Dataset A, Test Accuracy: 83.87%\n",
      "Pre-training, test on Dataset A, Test F1-score: 86.49%\n",
      "Pre-training, test on Dataset A, Test Recall (Sensitivity): 86.49%\n",
      "Pre-training, test on Dataset A, Test Specificity: 80.00%\n"
     ]
    }
   ],
   "source": [
    "Dataset_A_Pretrain_TNet = tabnet_pretrain.predict(X_A_test)\n",
    "\n",
    "cm_A_TNet = confusion_matrix(y_A_test, Dataset_A_Pretrain_TNet)\n",
    "TN_A, FP_A, FN_A, TP_A = cm_A_TNet.ravel()\n",
    "\n",
    "# Calculate Metrics for TabNet on Dataset A\n",
    "acc_A_TNet = accuracy_score(y_A_test, Dataset_A_Pretrain_TNet) * 100\n",
    "f1_A_TNet = f1_score(y_A_test, Dataset_A_Pretrain_TNet) * 100\n",
    "recall_A_TNet = TP_A / (TP_A + FN_A) * 100\n",
    "specificity_A_TNet = TN_A / (TN_A + FP_A) * 100\n",
    "\n",
    "\n",
    "print(f\"Pre-training, test on Dataset A, Test Accuracy: {acc_A_TNet:.2f}%\")\n",
    "print(f\"Pre-training, test on Dataset A, Test F1-score: {f1_A_TNet:.2f}%\")\n",
    "print(f\"Pre-training, test on Dataset A, Test Recall (Sensitivity): {recall_A_TNet:.2f}%\")\n",
    "print(f\"Pre-training, test on Dataset A, Test Specificity: {specificity_A_TNet:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4c40aac0-e457-447b-8a49-bca02c5b9a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training, test on Dataset B, Accuracy: 74.19%\n",
      "Pre-training, test on Dataset B, F1-score: 73.33%\n",
      "Pre-training, test on Dataset B, Recall (Sensitivity): 78.57%\n",
      "Pre-training, test on Dataset B, Specificity: 70.59%\n"
     ]
    }
   ],
   "source": [
    "Dataset_B_Pretrain_TNet = tabnet_pretrain.predict(X_B_test)\n",
    "\n",
    "# Calculate Confusion Matrix for TabNet on Dataset B\n",
    "# Structure: [[TN, FP], [FN, TP]]\n",
    "cm_B_TNet = confusion_matrix(y_B_test, Dataset_B_Pretrain_TNet)\n",
    "TN_B, FP_B, FN_B, TP_B = cm_B_TNet.ravel()\n",
    "\n",
    "# Calculate Core Metrics\n",
    "acc_B_TNet = accuracy_score(y_B_test, Dataset_B_Pretrain_TNet) * 100\n",
    "f1_B_TNet = f1_score(y_B_test, Dataset_B_Pretrain_TNet) * 100\n",
    "\n",
    "# Calculate Recall (Sensitivity) and Specificity\n",
    "recall_B_TNet = TP_B / (TP_B + FN_B) * 100\n",
    "specificity_B_TNet = TN_B / (TN_B + FP_B) * 100\n",
    "\n",
    "print(f\"Pre-training, test on Dataset B, Accuracy: {acc_B_TNet:.2f}%\")\n",
    "print(f\"Pre-training, test on Dataset B, F1-score: {f1_B_TNet:.2f}%\")\n",
    "print(f\"Pre-training, test on Dataset B, Recall (Sensitivity): {recall_B_TNet:.2f}%\")\n",
    "print(f\"Pre-training, test on Dataset B, Specificity: {specificity_B_TNet:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed41fb9f-b867-490f-92a5-7a68fa4b134b",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5429c335-a2f1-4d1c-96a8-4f2ca38b8b2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-30 00:08:46,950] A new study created in memory with name: no-name-bab354e7-25a7-47e6-b959-3fb7edff0247\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:09:05,180] Trial 0 finished with value: 0.9333333333333333 and parameters: {'gamma': 1.2422921413473458, 'lambda_sparse': 4.4674843238322395e-05, 'lr': 0.0005454191359802393, 'momentum': 0.06999999999999999, 'weight_decay': 0.00021888834807352566, 'batch_size': 128, 'virtual_batch_size': 16, 'step_size': 30, 'scheduler_gamma': 0.8283388192784523}. Best is trial 0 with value: 0.9333333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 76 with best_epoch = 36 and best_val_logloss = 0.17214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:09:18,424] Trial 1 finished with value: 0.9032258064516129 and parameters: {'gamma': 1.1143049288356874, 'lambda_sparse': 2.4633424661838425e-05, 'lr': 1.4170275794756587e-05, 'momentum': 0.24000000000000002, 'weight_decay': 0.0008719987291601801, 'batch_size': 128, 'virtual_batch_size': 32, 'step_size': 50, 'scheduler_gamma': 0.9445796584268462}. Best is trial 0 with value: 0.9333333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 71 with best_epoch = 31 and best_val_logloss = 0.26891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_val_logloss = 0.23802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:10:08,530] Trial 2 finished with value: 0.9333333333333333 and parameters: {'gamma': 1.7010768485304697, 'lambda_sparse': 0.004209994443332075, 'lr': 7.19430394095179e-05, 'momentum': 0.01, 'weight_decay': 0.00014615584048433632, 'batch_size': 32, 'virtual_batch_size': 32, 'step_size': 30, 'scheduler_gamma': 0.8988790736053499}. Best is trial 0 with value: 0.9333333333333333.\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 46 with best_epoch = 6 and best_val_logloss = 0.16703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:10:32,248] Trial 3 finished with value: 0.9333333333333333 and parameters: {'gamma': 1.6859638435272462, 'lambda_sparse': 0.007710358970953229, 'lr': 0.0011670357457049811, 'momentum': 0.37, 'weight_decay': 1.2197843684303826e-05, 'batch_size': 32, 'virtual_batch_size': 16, 'step_size': 40, 'scheduler_gamma': 0.8659247247952376}. Best is trial 0 with value: 0.9333333333333333.\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 100 with best_epoch = 95 and best_val_logloss = 0.19454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:11:27,469] Trial 4 finished with value: 0.9333333333333333 and parameters: {'gamma': 1.2055073954950917, 'lambda_sparse': 1.3570165178999344e-06, 'lr': 0.00010255317285506084, 'momentum': 0.05, 'weight_decay': 0.00010480136300812869, 'batch_size': 32, 'virtual_batch_size': 16, 'step_size': 30, 'scheduler_gamma': 0.8352495239962844}. Best is trial 0 with value: 0.9333333333333333.\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 57 with best_epoch = 17 and best_val_logloss = 0.29894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:11:50,220] Trial 5 finished with value: 0.875 and parameters: {'gamma': 1.5364086938612993, 'lambda_sparse': 0.0001646856904190091, 'lr': 1.3623264669154953e-05, 'momentum': 0.23, 'weight_decay': 0.00085863401351914, 'batch_size': 64, 'virtual_batch_size': 16, 'step_size': 40, 'scheduler_gamma': 0.9598941196769243}. Best is trial 0 with value: 0.9333333333333333.\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:12:06,210] Trial 6 finished with value: 0.9333333333333333 and parameters: {'gamma': 1.5202562811749707, 'lambda_sparse': 0.001492946611442833, 'lr': 0.0015173683281350027, 'momentum': 0.12, 'weight_decay': 0.00038741137118607585, 'batch_size': 128, 'virtual_batch_size': 16, 'step_size': 40, 'scheduler_gamma': 0.8256660444317862}. Best is trial 0 with value: 0.9333333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 63 with best_epoch = 23 and best_val_logloss = 0.17338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:12:25,902] Trial 7 finished with value: 0.875 and parameters: {'gamma': 1.2238634803903032, 'lambda_sparse': 0.005932084907582451, 'lr': 5.970895836225864e-05, 'momentum': 0.06999999999999999, 'weight_decay': 1.4341988553927436e-05, 'batch_size': 128, 'virtual_batch_size': 32, 'step_size': 20, 'scheduler_gamma': 0.9161687142715591}. Best is trial 0 with value: 0.9333333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 93 with best_epoch = 53 and best_val_logloss = 0.26856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 26 and best_val_logloss = 0.16256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:12:49,491] Trial 8 finished with value: 0.9655172413793104 and parameters: {'gamma': 1.0093336159088, 'lambda_sparse': 0.000287623142768745, 'lr': 0.0005151997387371908, 'momentum': 0.29000000000000004, 'weight_decay': 0.00035940837944500203, 'batch_size': 64, 'virtual_batch_size': 16, 'step_size': 70, 'scheduler_gamma': 0.8291178754457263}. Best is trial 8 with value: 0.9655172413793104.\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:13:08,649] Trial 9 finished with value: 0.875 and parameters: {'gamma': 1.9123842149319294, 'lambda_sparse': 1.2856603319977834e-05, 'lr': 2.2238398861131135e-05, 'momentum': 0.18000000000000002, 'weight_decay': 0.0005982640845713534, 'batch_size': 128, 'virtual_batch_size': 32, 'step_size': 80, 'scheduler_gamma': 0.8972708789479265}. Best is trial 8 with value: 0.9655172413793104.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 100 with best_epoch = 81 and best_val_logloss = 0.35572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 1 and best_val_logloss = 0.12619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:13:25,200] Trial 10 finished with value: 0.9285714285714286 and parameters: {'gamma': 1.056491089792874, 'lambda_sparse': 0.0003355712601491638, 'lr': 0.009163076554093286, 'momentum': 0.36000000000000004, 'weight_decay': 4.2900515449855195e-05, 'batch_size': 64, 'virtual_batch_size': 16, 'step_size': 80, 'scheduler_gamma': 0.8055107152024413}. Best is trial 8 with value: 0.9655172413793104.\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:13:56,033] Trial 11 finished with value: 0.9655172413793104 and parameters: {'gamma': 1.3244735471416709, 'lambda_sparse': 4.3676256950140066e-05, 'lr': 0.00046880596004914905, 'momentum': 0.29000000000000004, 'weight_decay': 0.00023369340665332084, 'batch_size': 64, 'virtual_batch_size': 16, 'step_size': 60, 'scheduler_gamma': 0.8578587526087255}. Best is trial 8 with value: 0.9655172413793104.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 69 with best_epoch = 29 and best_val_logloss = 0.14469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:14:19,472] Trial 12 finished with value: 0.9655172413793104 and parameters: {'gamma': 1.367507400579059, 'lambda_sparse': 0.0004251618915937133, 'lr': 0.0002913586505450098, 'momentum': 0.31, 'weight_decay': 0.00028979687663479756, 'batch_size': 64, 'virtual_batch_size': 16, 'step_size': 70, 'scheduler_gamma': 0.8659228799097902}. Best is trial 8 with value: 0.9655172413793104.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 69 with best_epoch = 29 and best_val_logloss = 0.14635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:14:34,111] Trial 13 finished with value: 0.9333333333333333 and parameters: {'gamma': 1.3687334972351624, 'lambda_sparse': 2.8857319575208558e-06, 'lr': 0.0038778033501324204, 'momentum': 0.29000000000000004, 'weight_decay': 5.1987993662030704e-05, 'batch_size': 64, 'virtual_batch_size': 16, 'step_size': 60, 'scheduler_gamma': 0.8496538330136844}. Best is trial 8 with value: 0.9655172413793104.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 2 and best_val_logloss = 0.1422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:14:56,852] Trial 14 finished with value: 0.9655172413793104 and parameters: {'gamma': 1.0441750291969256, 'lambda_sparse': 8.403206735528225e-06, 'lr': 0.00028370846539118943, 'momentum': 0.29000000000000004, 'weight_decay': 0.00039282828129995513, 'batch_size': 64, 'virtual_batch_size': 16, 'step_size': 60, 'scheduler_gamma': 0.8704054512320905}. Best is trial 8 with value: 0.9655172413793104.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 69 with best_epoch = 29 and best_val_logloss = 0.17077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:15:15,867] Trial 15 finished with value: 0.9333333333333333 and parameters: {'gamma': 1.367157955639628, 'lambda_sparse': 8.975856154353565e-05, 'lr': 0.0007532660778740462, 'momentum': 0.17, 'weight_decay': 5.62424157757405e-05, 'batch_size': 64, 'virtual_batch_size': 16, 'step_size': 70, 'scheduler_gamma': 0.8041387914316378}. Best is trial 8 with value: 0.9655172413793104.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 57 with best_epoch = 17 and best_val_logloss = 0.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:15:30,175] Trial 16 finished with value: 0.9655172413793104 and parameters: {'gamma': 1.0144225699927032, 'lambda_sparse': 0.0010281254654526094, 'lr': 0.002761025671404811, 'momentum': 0.4, 'weight_decay': 0.00019052461370645097, 'batch_size': 64, 'virtual_batch_size': 16, 'step_size': 60, 'scheduler_gamma': 0.8481697239896773}. Best is trial 8 with value: 0.9655172413793104.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 4 and best_val_logloss = 0.13228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 79 with best_epoch = 39 and best_val_logloss = 0.163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:16:03,015] Trial 17 finished with value: 0.9655172413793104 and parameters: {'gamma': 1.1640956787854742, 'lambda_sparse': 7.875496948974441e-05, 'lr': 0.00017714633793248366, 'momentum': 0.32, 'weight_decay': 9.425180373703271e-05, 'batch_size': 64, 'virtual_batch_size': 16, 'step_size': 70, 'scheduler_gamma': 0.9233799647128421}. Best is trial 8 with value: 0.9655172413793104.\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:16:25,599] Trial 18 finished with value: 0.9333333333333333 and parameters: {'gamma': 1.6463492872169527, 'lambda_sparse': 0.000276905937036994, 'lr': 0.0005424595396278098, 'momentum': 0.26, 'weight_decay': 0.0005120038395547117, 'batch_size': 64, 'virtual_batch_size': 32, 'step_size': 50, 'scheduler_gamma': 0.8785190569628886}. Best is trial 8 with value: 0.9655172413793104.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 73 with best_epoch = 33 and best_val_logloss = 0.18969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:16:56,648] Trial 19 finished with value: 0.9333333333333333 and parameters: {'gamma': 1.8953141104688926, 'lambda_sparse': 0.001141507117085539, 'lr': 0.00015188140705682494, 'momentum': 0.19, 'weight_decay': 2.3474766977528945e-05, 'batch_size': 64, 'virtual_batch_size': 16, 'step_size': 50, 'scheduler_gamma': 0.8474892853065131}. Best is trial 8 with value: 0.9655172413793104.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 100 with best_epoch = 86 and best_val_logloss = 0.19736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 100 with best_epoch = 91 and best_val_logloss = 0.20341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:17:46,938] Trial 20 finished with value: 0.9655172413793104 and parameters: {'gamma': 1.3027921810114023, 'lambda_sparse': 6.439677736864882e-06, 'lr': 3.676301976046555e-05, 'momentum': 0.34, 'weight_decay': 0.00011798399091112759, 'batch_size': 32, 'virtual_batch_size': 16, 'step_size': 80, 'scheduler_gamma': 0.8148534992691371}. Best is trial 8 with value: 0.9655172413793104.\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:18:09,235] Trial 21 finished with value: 0.9655172413793104 and parameters: {'gamma': 1.4450803677623445, 'lambda_sparse': 0.000525220952174356, 'lr': 0.0003498965749168937, 'momentum': 0.29000000000000004, 'weight_decay': 0.00026975227309178785, 'batch_size': 64, 'virtual_batch_size': 16, 'step_size': 70, 'scheduler_gamma': 0.8584268259771287}. Best is trial 8 with value: 0.9655172413793104.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 69 with best_epoch = 29 and best_val_logloss = 0.14347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:18:31,351] Trial 22 finished with value: 0.9655172413793104 and parameters: {'gamma': 1.4279136888033503, 'lambda_sparse': 3.111392333427441e-05, 'lr': 0.0002553392100364577, 'momentum': 0.32, 'weight_decay': 0.0003048851522889992, 'batch_size': 64, 'virtual_batch_size': 16, 'step_size': 70, 'scheduler_gamma': 0.8847416628354472}. Best is trial 8 with value: 0.9655172413793104.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 69 with best_epoch = 29 and best_val_logloss = 0.16422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:18:53,180] Trial 23 finished with value: 0.9655172413793104 and parameters: {'gamma': 1.6073537882702764, 'lambda_sparse': 0.000141705515446348, 'lr': 0.0010741830020459238, 'momentum': 0.26, 'weight_decay': 0.0001786146614554196, 'batch_size': 64, 'virtual_batch_size': 16, 'step_size': 60, 'scheduler_gamma': 0.8364462889344734}. Best is trial 8 with value: 0.9655172413793104.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 69 with best_epoch = 29 and best_val_logloss = 0.17957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:19:15,135] Trial 24 finished with value: 0.9655172413793104 and parameters: {'gamma': 1.3291972342796488, 'lambda_sparse': 0.0024096083969248447, 'lr': 0.00044209270883691456, 'momentum': 0.22, 'weight_decay': 0.000574566245674024, 'batch_size': 64, 'virtual_batch_size': 16, 'step_size': 70, 'scheduler_gamma': 0.8579010259989587}. Best is trial 8 with value: 0.9655172413793104.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 69 with best_epoch = 29 and best_val_logloss = 0.15997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:19:29,248] Trial 25 finished with value: 0.9333333333333333 and parameters: {'gamma': 1.7813299025266465, 'lambda_sparse': 0.0004516781153057403, 'lr': 0.002021764788064135, 'momentum': 0.32, 'weight_decay': 0.00031467034169290956, 'batch_size': 64, 'virtual_batch_size': 16, 'step_size': 60, 'scheduler_gamma': 0.8754011740190726}. Best is trial 8 with value: 0.9655172413793104.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 43 with best_epoch = 3 and best_val_logloss = 0.17753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:19:56,903] Trial 26 finished with value: 0.9655172413793104 and parameters: {'gamma': 1.1117857717386121, 'lambda_sparse': 0.00017894829946397023, 'lr': 0.00015548252739601523, 'momentum': 0.4, 'weight_decay': 7.415960220252758e-05, 'batch_size': 64, 'virtual_batch_size': 32, 'step_size': 80, 'scheduler_gamma': 0.8212116594809854}. Best is trial 8 with value: 0.9655172413793104.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 100 with best_epoch = 86 and best_val_logloss = 0.16757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:20:18,550] Trial 27 finished with value: 0.9655172413793104 and parameters: {'gamma': 1.2769280168017008, 'lambda_sparse': 0.0007260379723648869, 'lr': 0.0007643322743133475, 'momentum': 0.15000000000000002, 'weight_decay': 0.00022938367426647064, 'batch_size': 64, 'virtual_batch_size': 16, 'step_size': 70, 'scheduler_gamma': 0.9052485020473331}. Best is trial 8 with value: 0.9655172413793104.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 63 with best_epoch = 23 and best_val_logloss = 0.16858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:20:33,298] Trial 28 finished with value: 0.9333333333333333 and parameters: {'gamma': 1.4529768377094345, 'lambda_sparse': 5.64854915071735e-05, 'lr': 0.004418840121748493, 'momentum': 0.27, 'weight_decay': 0.0004359578007004053, 'batch_size': 64, 'virtual_batch_size': 16, 'step_size': 50, 'scheduler_gamma': 0.9773694610251796}. Best is trial 8 with value: 0.9655172413793104.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 2 and best_val_logloss = 0.16867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: mask_type changed from entmax to sparsemax\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 55 with best_epoch = 15 and best_val_logloss = 0.16513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-30 00:21:01,868] Trial 29 finished with value: 0.9655172413793104 and parameters: {'gamma': 1.1505639134565493, 'lambda_sparse': 2.424671090397197e-05, 'lr': 0.0007412784697649567, 'momentum': 0.36000000000000004, 'weight_decay': 0.00014598653267065606, 'batch_size': 32, 'virtual_batch_size': 16, 'step_size': 60, 'scheduler_gamma': 0.8363723847926725}. Best is trial 8 with value: 0.9655172413793104.\n"
     ]
    }
   ],
   "source": [
    "def objective_finetune(trial):\n",
    "\n",
    "    # --- Hyperparameters to tune ---\n",
    "    gamma = trial.suggest_float('gamma', 1.0, 2.0)\n",
    "    lambda_sparse = trial.suggest_float('lambda_sparse', 1e-6, 1e-2, log=True)\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "    momentum = trial.suggest_float('momentum', 0.01, 0.4, step=0.01)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True)\n",
    "\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    virtual_batch_size = trial.suggest_categorical('virtual_batch_size', [16, 32])\n",
    "\n",
    "    step_size = trial.suggest_int('step_size', 20, 80, step=10)\n",
    "    scheduler_gamma = trial.suggest_float('scheduler_gamma', 0.8, 0.98)\n",
    "\n",
    "    # --- Model definition ---\n",
    "    tabnet_model_finetune = TabNetClassifier(\n",
    "        n_d=best_pretrain_params['n_d_a'],\n",
    "        n_a=best_pretrain_params['n_d_a'],\n",
    "        n_steps=best_pretrain_params['n_steps'],\n",
    "        gamma=gamma,\n",
    "        lambda_sparse=lambda_sparse,\n",
    "        momentum=momentum,\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params={\n",
    "            'lr': lr,\n",
    "        },\n",
    "        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "        scheduler_params={\n",
    "            'step_size': step_size,\n",
    "            'gamma': scheduler_gamma\n",
    "        },\n",
    "        mask_type='entmax',\n",
    "        verbose=0,\n",
    "        seed=42,\n",
    "        device_name='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "\n",
    "    # --- Fine-tuning ---\n",
    "    tabnet_model_finetune.fit(\n",
    "        X_train=X_B_train,\n",
    "        y_train=y_B_train,\n",
    "        eval_set=[(X_B_val, y_B_val)],\n",
    "        eval_name=[\"val\"],\n",
    "        eval_metric=[\"logloss\"],\n",
    "        max_epochs=100,\n",
    "        patience=40,  # FIXED (not optimized)\n",
    "        batch_size=batch_size,\n",
    "        virtual_batch_size=virtual_batch_size,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        from_unsupervised=tabnet_pretrain\n",
    "    )\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    preds = tabnet_model_finetune.predict(X_B_val)\n",
    "    f1 = f1_score(y_B_val, preds)\n",
    "    acc = accuracy_score(y_B_val, preds)\n",
    "\n",
    "    print(\n",
    "        f\"Trial {trial.number} | \"\n",
    "        f\"F1: {f1:.4f} | Acc: {acc:.4f} | \"\n",
    "        f\"LR: {lr:.1e} | Mom: {momentum:.2f}\"\n",
    "    )\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "# --- Run the Optimization ---\n",
    "study_finetune = optuna.create_study(direction='maximize')  # We want to maximize F1-Score\n",
    "study_finetune.optimize(objective_finetune, n_trials=30)  # Number of trials to run for optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "46cfcb11-06a9-4d81-8aa6-9e34972d611d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best finetune params:\n",
      "  Value (F1-score): 0.9655\n",
      "  Params:\n",
      "    gamma: 1.0093336159088\n",
      "    lambda_sparse: 0.000287623142768745\n",
      "    lr: 0.0005151997387371908\n",
      "    momentum: 0.29000000000000004\n",
      "    weight_decay: 0.00035940837944500203\n",
      "    batch_size: 64\n",
      "    virtual_batch_size: 16\n",
      "    step_size: 70\n",
      "    scheduler_gamma: 0.8291178754457263\n"
     ]
    }
   ],
   "source": [
    "# Get best trial and parameters\n",
    "best_trial_finetune = study_finetune.best_trial\n",
    "best_finetune_params = best_trial_finetune.params\n",
    "\n",
    "print(\"Best finetune params:\")\n",
    "print(f\"  Value (F1-score): {best_trial_finetune.value:.4f}\")\n",
    "print(\"  Params:\")\n",
    "\n",
    "for key, value in best_finetune_params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42615b97-2b2e-436f-ba9c-900939d4e405",
   "metadata": {},
   "source": [
    "#### Finetunine on the target domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "62318eaf-23a4-4d5f-ad96-4c2a8fc38558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 1.87\n",
      "lambda_sparse: 0.00176\n",
      "lr: 0.00236\n",
      "momentum: 0.92\n",
      "batch_size: 32\n",
      "virtual_batch_size: 32\n",
      "step_size: 60\n",
      "scheduler_gamma: 0.946\n"
     ]
    }
   ],
   "source": [
    "best_finetune_params = {\n",
    "    \"gamma\": 1.87,  \n",
    "    \"lambda_sparse\": 0.00176,  \n",
    "    \"lr\": 0.00236,  \n",
    "    \"momentum\": 0.92, \n",
    "    \"batch_size\": 32,  \n",
    "    \"virtual_batch_size\": 32,  \n",
    "    \"step_size\": 60,  \n",
    "    \"scheduler_gamma\": 0.946, \n",
    "}\n",
    "\n",
    "# Print the updated values\n",
    "for key, value in best_finetune_params.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "60a1ed99-b4e0-49c3-b7f3-3e48f2446be2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_a changed from 8 to 16\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_d changed from 8 to 16\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: n_steps changed from 3 to 9\n",
      "  warnings.warn(wrn_msg)\n",
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.65235 | val_logloss: 0.24879 |  0:00:00s\n",
      "epoch 1  | loss: 0.59593 | val_logloss: 0.2366  |  0:00:01s\n",
      "epoch 2  | loss: 0.58144 | val_logloss: 0.22615 |  0:00:02s\n",
      "epoch 3  | loss: 0.485   | val_logloss: 0.30044 |  0:00:03s\n",
      "epoch 4  | loss: 0.60511 | val_logloss: 0.3088  |  0:00:04s\n",
      "epoch 5  | loss: 0.48286 | val_logloss: 0.23589 |  0:00:04s\n",
      "epoch 6  | loss: 0.45735 | val_logloss: 0.16189 |  0:00:05s\n",
      "epoch 7  | loss: 0.49516 | val_logloss: 0.16318 |  0:00:06s\n",
      "epoch 8  | loss: 0.48312 | val_logloss: 0.1911  |  0:00:07s\n",
      "epoch 9  | loss: 0.51852 | val_logloss: 0.19563 |  0:00:08s\n",
      "epoch 10 | loss: 0.41846 | val_logloss: 0.20051 |  0:00:08s\n",
      "epoch 11 | loss: 0.45739 | val_logloss: 0.18843 |  0:00:09s\n",
      "epoch 12 | loss: 0.45904 | val_logloss: 0.25945 |  0:00:10s\n",
      "epoch 13 | loss: 0.43297 | val_logloss: 0.17831 |  0:00:11s\n",
      "epoch 14 | loss: 0.43837 | val_logloss: 0.28718 |  0:00:11s\n",
      "epoch 15 | loss: 0.44192 | val_logloss: 0.29492 |  0:00:12s\n",
      "epoch 16 | loss: 0.46761 | val_logloss: 0.27825 |  0:00:13s\n",
      "epoch 17 | loss: 0.41965 | val_logloss: 0.27933 |  0:00:14s\n",
      "epoch 18 | loss: 0.43714 | val_logloss: 0.30427 |  0:00:15s\n",
      "epoch 19 | loss: 0.39024 | val_logloss: 0.28889 |  0:00:15s\n",
      "epoch 20 | loss: 0.45468 | val_logloss: 0.24873 |  0:00:16s\n",
      "epoch 21 | loss: 0.38451 | val_logloss: 0.26281 |  0:00:17s\n",
      "epoch 22 | loss: 0.42085 | val_logloss: 0.2804  |  0:00:18s\n",
      "epoch 23 | loss: 0.41017 | val_logloss: 0.23485 |  0:00:19s\n",
      "epoch 24 | loss: 0.41168 | val_logloss: 0.23441 |  0:00:20s\n",
      "epoch 25 | loss: 0.42947 | val_logloss: 0.2916  |  0:00:21s\n",
      "epoch 26 | loss: 0.40664 | val_logloss: 0.3218  |  0:00:21s\n",
      "epoch 27 | loss: 0.38821 | val_logloss: 0.33632 |  0:00:22s\n",
      "epoch 28 | loss: 0.43451 | val_logloss: 0.2991  |  0:00:23s\n",
      "epoch 29 | loss: 0.41786 | val_logloss: 0.2609  |  0:00:24s\n",
      "epoch 30 | loss: 0.41462 | val_logloss: 0.28093 |  0:00:24s\n",
      "epoch 31 | loss: 0.40889 | val_logloss: 0.23643 |  0:00:25s\n",
      "epoch 32 | loss: 0.41186 | val_logloss: 0.20836 |  0:00:26s\n",
      "epoch 33 | loss: 0.38377 | val_logloss: 0.25244 |  0:00:27s\n",
      "epoch 34 | loss: 0.43937 | val_logloss: 0.28511 |  0:00:27s\n",
      "epoch 35 | loss: 0.42113 | val_logloss: 0.25574 |  0:00:28s\n",
      "epoch 36 | loss: 0.42674 | val_logloss: 0.2591  |  0:00:29s\n",
      "epoch 37 | loss: 0.38818 | val_logloss: 0.26547 |  0:00:29s\n",
      "epoch 38 | loss: 0.39275 | val_logloss: 0.25639 |  0:00:30s\n",
      "epoch 39 | loss: 0.36681 | val_logloss: 0.28337 |  0:00:31s\n",
      "epoch 40 | loss: 0.41538 | val_logloss: 0.28305 |  0:00:32s\n",
      "epoch 41 | loss: 0.39936 | val_logloss: 0.23091 |  0:00:32s\n",
      "epoch 42 | loss: 0.39722 | val_logloss: 0.24954 |  0:00:33s\n",
      "epoch 43 | loss: 0.39084 | val_logloss: 0.26122 |  0:00:34s\n",
      "epoch 44 | loss: 0.39068 | val_logloss: 0.24932 |  0:00:34s\n",
      "epoch 45 | loss: 0.39989 | val_logloss: 0.25626 |  0:00:35s\n",
      "epoch 46 | loss: 0.4177  | val_logloss: 0.2531  |  0:00:36s\n",
      "\n",
      "Early stopping occurred at epoch 46 with best_epoch = 6 and best_val_logloss = 0.16189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the line below if you do hyperparameter tuning\n",
    "# best_finetune_params = study_finetune.best_params\n",
    "\n",
    "# Create a new model for fine-tuning\n",
    "tabnet_finetuned = TabNetClassifier(\n",
    "    n_d=8,\n",
    "    n_a=8,\n",
    "    n_steps=3,\n",
    "    gamma=best_finetune_params['gamma'],\n",
    "    lambda_sparse=best_finetune_params['lambda_sparse'],\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params={\n",
    "        'lr': best_finetune_params['lr']\n",
    "    },\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    scheduler_params={'step_size': best_finetune_params['step_size'], 'gamma': best_finetune_params['scheduler_gamma']},\n",
    "    seed=42,\n",
    "    verbose=1,\n",
    "    device_name='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "tabnet_finetuned.fit(\n",
    "    X_train=X_B_train,\n",
    "    y_train=y_B_train,           \n",
    "    eval_set=[(X_B_val, y_B_val)],\n",
    "    eval_name=[\"val\"],\n",
    "    eval_metric=['logloss'],\n",
    "    max_epochs=100,\n",
    "    patience=40,\n",
    "    batch_size=best_finetune_params['batch_size'],\n",
    "    virtual_batch_size=best_finetune_params['virtual_batch_size'],\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    "    # Initialize with pre-trained weights\n",
    "    from_unsupervised=tabnet_pretrain  # Transfer learning!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fcb0603e-8399-4861-9f00-b943aeff7fd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned, test on Dataset A, Test Accuracy: 82.26%\n",
      "Fine-tuned, test on Dataset A, Test F1-score: 84.51%\n",
      "Fine-tuned, test on Dataset A, Test Recall (Sensitivity): 81.08%\n",
      "Fine-tuned, test on Dataset A, Test Specificity: 84.00%\n"
     ]
    }
   ],
   "source": [
    "Dataset_A_Finetune_TNet = tabnet_finetuned.predict(X_A_test)\n",
    "\n",
    "# Calculate Confusion Matrix for Fine-Tuned TabNet on Dataset A\n",
    "# Structure: [[TN, FP], [FN, TP]]\n",
    "cm_A_Finetune_TNet = confusion_matrix(y_A_test, Dataset_A_Finetune_TNet)\n",
    "TN_A_F, FP_A_F, FN_A_F, TP_A_F = cm_A_Finetune_TNet.ravel()\n",
    "\n",
    "# Calculate Core Metrics\n",
    "acc_A_Finetune_TNet = accuracy_score(y_A_test, Dataset_A_Finetune_TNet) * 100\n",
    "f1_A_Finetune_TNet = f1_score(y_A_test, Dataset_A_Finetune_TNet) * 100\n",
    "\n",
    "# Calculate Recall (Sensitivity) and Specificity\n",
    "recall_A_Finetune_TNet = TP_A_F / (TP_A_F + FN_A_F) * 100 if (TP_A_F + FN_A_F) > 0 else 0\n",
    "specificity_A_Finetune_TNet = TN_A_F / (TN_A_F + FP_A_F) * 100 if (TN_A_F + FP_A_F) > 0 else 0\n",
    "\n",
    "print(f\"Fine-tuned, test on Dataset A, Test Accuracy: {acc_A_Finetune_TNet:.2f}%\")\n",
    "print(f\"Fine-tuned, test on Dataset A, Test F1-score: {f1_A_Finetune_TNet:.2f}%\")\n",
    "print(f\"Fine-tuned, test on Dataset A, Test Recall (Sensitivity): {recall_A_Finetune_TNet:.2f}%\")\n",
    "print(f\"Fine-tuned, test on Dataset A, Test Specificity: {specificity_A_Finetune_TNet:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "357c6039-9f26-4de1-a9da-1f10dcda17b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned, test on Dataset B, Accuracy: 70.97%\n",
      "Fine-tuned, test on Dataset B, F1-score: 70.97%\n",
      "Fine-tuned, test on Dataset B, Recall (Sensitivity): 78.57%\n",
      "Fine-tuned, test on Dataset B, Specificity: 64.71%\n"
     ]
    }
   ],
   "source": [
    "Dataset_B_Finetune_TNet = tabnet_finetuned.predict(X_B_test)\n",
    "\n",
    "# Calculate Confusion Matrix for Fine-Tuned TabNet on Dataset B\n",
    "# Structure: [[TN, FP], [FN, TP]]\n",
    "cm_B_Finetune_TNet = confusion_matrix(y_B_test, Dataset_B_Finetune_TNet)\n",
    "TN_F, FP_F, FN_F, TP_F = cm_B_Finetune_TNet.ravel()\n",
    "\n",
    "# Calculate Core Metrics\n",
    "acc_B_Finetune_TNet = accuracy_score(y_B_test, Dataset_B_Finetune_TNet) * 100\n",
    "f1_B_Finetune_TNet = f1_score(y_B_test, Dataset_B_Finetune_TNet) * 100\n",
    "\n",
    "# Calculate Recall (Sensitivity) and Specificity\n",
    "recall_B_Finetune_TNet = TP_F / (TP_F + FN_F) * 100\n",
    "specificity_B_Finetune_TNet = TN_F / (TN_F + FP_F) * 100\n",
    "\n",
    "print(f\"Fine-tuned, test on Dataset B, Accuracy: {acc_B_Finetune_TNet:.2f}%\")\n",
    "print(f\"Fine-tuned, test on Dataset B, F1-score: {f1_B_Finetune_TNet:.2f}%\")\n",
    "print(f\"Fine-tuned, test on Dataset B, Recall (Sensitivity): {recall_B_Finetune_TNet:.2f}%\")\n",
    "print(f\"Fine-tuned, test on Dataset B, Specificity: {specificity_B_Finetune_TNet:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81509e62-8502-4cb6-a7e1-d6c997fb4e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global font to Times New Roman\n",
    "plt.rcParams.update({'font.family': 'Times New Roman', 'font.weight': 'bold', 'font.size' : '16'})\n",
    "\n",
    "cm = confusion_matrix(y_finetune_test, Dataset_B_Finetune)\n",
    "\n",
    "# Unravel the confusion matrix to get TN, FP, FN, TP\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Create the plot with a specific size\n",
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "# Display confusion matrix as an image with the 'Blues' color map\n",
    "im = plt.imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "\n",
    "# Set color bar ticks based on the max value of the matrix\n",
    "vmax = cm.max()\n",
    "colorbar_ticks = np.arange(0, vmax + 10, 10)\n",
    "plt.colorbar(im, ticks=colorbar_ticks)\n",
    "\n",
    "# Remove title as per request (You can uncomment the next line if you want to add a title)\n",
    "# plt.title('Confusion Matrix of the fine-tuned model')\n",
    "\n",
    "# Set X-axis labels (Predicted: Disease then No Disease)\n",
    "plt.xticks([0, 1], ['No Disease', 'Disease']) \n",
    "plt.yticks([0, 1], ['No Disease', 'Disease']) \n",
    "\n",
    "# Label the axes\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "\n",
    "# Normalize the color range based on the confusion matrix values\n",
    "norm = mcolors.Normalize(vmin=cm.min(), vmax=cm.max())\n",
    "cmap = plt.colormaps['Blues']\n",
    "\n",
    "# Iterate through the matrix to add text inside each cell\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        cell_value = cm[i, j]\n",
    "        cell_color = cmap(norm(cell_value))  # Get the color of the cell based on the value\n",
    "        luminance = 0.299 * cell_color[0] + 0.587 * cell_color[1] + 0.114 * cell_color[2]  # Calculate luminance\n",
    "        text_color = 'white' if luminance < 0.5 else 'black'  # Choose white or black text based on luminance\n",
    "        \n",
    "        # Add the text inside the cell with bold and Times New Roman font\n",
    "        plt.text(j, i, f'{cell_value}', ha='center', va='center', color=text_color, fontsize=24, fontweight='bold')\n",
    "\n",
    "# Save the confusion matrix image with high resolution\n",
    "plt.savefig('confusion_matrix_tabnet.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b26f9b1-2251-4300-90ca-eca153ba1349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Times New Roman font is used in all plots\n",
    "plt.rcParams.update({'font.family': 'Times New Roman', 'font.weight': 'bold', 'font.size' : '20'})\n",
    "\n",
    "# Get predictions for TabNet model\n",
    "y_pred = tabnet_finetuned.predict(X_B_test)  # Get the class predictions for target domain\n",
    "y_pred_prob = tabnet_finetuned.predict_proba(X_B_test)[:, 1]  # Probabilities for the positive class (ROC)\n",
    "\n",
    "# Calculate Precision, Recall, F1-Score\n",
    "precision = precision_score(y_B_test, y_pred) * 100\n",
    "recall = recall_score(y_B_test, y_pred) * 100\n",
    "f1 = f1_score(y_B_test, y_pred) * 100\n",
    "\n",
    "# Display the metrics\n",
    "print(f\"Precision: {precision:.2f}%\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"F1-Score: {f1:.2f}%\")\n",
    "\n",
    "# Calculate Specificity: TN / (TN + FP)\n",
    "tn, fp, fn, tp = confusion_matrix(y_B_test, y_pred).ravel()\n",
    "specificity = (tn / (tn + fp)) * 100\n",
    "print(f\"Specificity: {specificity:.2f}%\")\n",
    "\n",
    "# Calculate ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_B_test, y_pred_prob) * 100\n",
    "print(f\"ROC-AUC: {roc_auc:.2f}%\")\n",
    "\n",
    "# Plot ROC curve with blue tones (IEEE standard)\n",
    "fpr, tpr, _ = roc_curve(y_B_test, y_pred_prob)\n",
    "roc_auc_value = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "\n",
    "# Plot the ROC curve: Use 'orange' color and update the label\n",
    "plt.plot(fpr, tpr, color='orange', lw=2, label=f'ROC curve (AUC = {roc_auc_value:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', lw=2, linestyle='--')\n",
    "\n",
    "# Set limits and labels as in the original image\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=20)\n",
    "plt.ylabel('True Positive Rate', fontsize=20)\n",
    "\n",
    "# Adjust legend location to match the original image\n",
    "plt.legend(loc='lower right', fontsize=18)\n",
    "\n",
    "# Save the ROC curve image (you can keep your original filename or update it)\n",
    "plt.savefig('roc_curve_tabnet.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()  # Display the plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d28fe7-74ba-41e7-8fef-2b8d2b0f3303",
   "metadata": {},
   "source": [
    "## 7. MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40e077c-9bfc-4381-bb15-265bf2789828",
   "metadata": {},
   "source": [
    "### Pretraining on the source domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63318ef1-8c86-4760-8cb1-aad3d9a6f266",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning for pretraining stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c7664098-5023-43ce-947f-7e8ca0bdbdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_for_tuning(hp):\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    hp_l2_reg = hp.Choice('l2_reg', values=[0.01, 0.001, 0.0001])\n",
    "    hp_dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)\n",
    "    hp_neurons_l1 = hp.Int('neurons_l1', min_value=32, max_value=128, step=32)\n",
    "\n",
    "    # input_dim = X_pretrain_train_res.shape[1]\n",
    "    input_dim = X_A_train.shape[1]\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "\n",
    "    # Layer 1 (Tuned Neurons, Tuned Dropout, Tuned L2)\n",
    "    x = Dense(\n",
    "        hp_neurons_l1, \n",
    "        activation='relu', \n",
    "        name='feature_layer_1',\n",
    "        kernel_regularizer=keras.regularizers.l2(hp_l2_reg)\n",
    "    )(inputs)\n",
    "    x = Dropout(hp_dropout_rate)(x)\n",
    "\n",
    "    # Layer 2 (Fixed at half of L1, or can be tuned as well)\n",
    "    x = Dense(\n",
    "        int(hp_neurons_l1 / 2), # e.g., 64 -> 32\n",
    "        activation='relu', \n",
    "        name='feature_layer_2',\n",
    "        kernel_regularizer=keras.regularizers.l2(hp_l2_reg)\n",
    "    )(x)\n",
    "    x = Dropout(hp_dropout_rate)(x)\n",
    "    \n",
    "    # Layer 3 (Fixed at quarter of L1, or can be tuned)\n",
    "    x = Dense(\n",
    "        int(hp_neurons_l1 / 4), # e.g., 64 -> 16\n",
    "        activation='relu', \n",
    "        name='feature_layer_3',\n",
    "        kernel_regularizer=keras.regularizers.l2(hp_l2_reg)\n",
    "    )(x)\n",
    "    x = Dropout(hp_dropout_rate)(x)\n",
    "    \n",
    "    # Output Layer\n",
    "    outputs = Dense(1, activation='sigmoid', name='output_layer')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', AUC(name='auc')] \n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4c473533-9faa-4745-9ee2-1db8d6643f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61, 10)\n",
      "(590, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_pretrain_val.shape)\n",
    "print(X_pretrain_train_res.shape)  # should match input layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c0a24bac-0d47-4c74-b389-245d1b1bf42b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 4\n",
      "learning_rate (Choice)\n",
      "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n",
      "l2_reg (Choice)\n",
      "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n",
      "dropout_rate (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.5, 'step': 0.1, 'sampling': 'linear'}\n",
      "neurons_l1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "0.001             |0.001             |learning_rate\n",
      "0.001             |0.001             |l2_reg\n",
      "0.1               |0.1               |dropout_rate\n",
      "96                |96                |neurons_l1\n",
      "\n",
      "Epoch 1/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7508 - auc: 0.8465 - loss: 0.6890 - val_accuracy: 0.7541 - val_auc: 0.8198 - val_loss: 0.6453\n",
      "Epoch 2/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8441 - auc: 0.9101 - loss: 0.5092 - val_accuracy: 0.7541 - val_auc: 0.8452 - val_loss: 0.6083\n",
      "Epoch 3/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8644 - auc: 0.9193 - loss: 0.4703 - val_accuracy: 0.7705 - val_auc: 0.8666 - val_loss: 0.5745\n",
      "Epoch 4/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8712 - auc: 0.9364 - loss: 0.4307 - val_accuracy: 0.8033 - val_auc: 0.8863 - val_loss: 0.5316\n",
      "Epoch 5/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8746 - auc: 0.9469 - loss: 0.4016 - val_accuracy: 0.8033 - val_auc: 0.8936 - val_loss: 0.5274\n",
      "Epoch 6/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8847 - auc: 0.9510 - loss: 0.3891 - val_accuracy: 0.8197 - val_auc: 0.9037 - val_loss: 0.4959\n",
      "Epoch 7/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8898 - auc: 0.9539 - loss: 0.3768 - val_accuracy: 0.8033 - val_auc: 0.9071 - val_loss: 0.4985\n",
      "Epoch 8/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8932 - auc: 0.9539 - loss: 0.3735 - val_accuracy: 0.8033 - val_auc: 0.9043 - val_loss: 0.4754\n",
      "Epoch 9/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9085 - auc: 0.9608 - loss: 0.3510 - val_accuracy: 0.7869 - val_auc: 0.9015 - val_loss: 0.4755\n",
      "Epoch 10/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9119 - auc: 0.9593 - loss: 0.3502 - val_accuracy: 0.7869 - val_auc: 0.9020 - val_loss: 0.4690\n",
      "Epoch 11/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9017 - auc: 0.9632 - loss: 0.3445 - val_accuracy: 0.7869 - val_auc: 0.9043 - val_loss: 0.4634\n",
      "Epoch 12/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9068 - auc: 0.9613 - loss: 0.3419 - val_accuracy: 0.7869 - val_auc: 0.9009 - val_loss: 0.4839\n",
      "Epoch 13/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9102 - auc: 0.9659 - loss: 0.3290 - val_accuracy: 0.7705 - val_auc: 0.8958 - val_loss: 0.4696\n",
      "Epoch 14/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9169 - auc: 0.9686 - loss: 0.3169 - val_accuracy: 0.7705 - val_auc: 0.9015 - val_loss: 0.4689\n",
      "Epoch 15/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9136 - auc: 0.9695 - loss: 0.3101 - val_accuracy: 0.7705 - val_auc: 0.8981 - val_loss: 0.4708\n",
      "Epoch 16/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9237 - auc: 0.9695 - loss: 0.3049 - val_accuracy: 0.7705 - val_auc: 0.8992 - val_loss: 0.4686\n",
      "Epoch 17/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9169 - auc: 0.9704 - loss: 0.3029 - val_accuracy: 0.8033 - val_auc: 0.9009 - val_loss: 0.4656\n",
      "Epoch 18/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9254 - auc: 0.9708 - loss: 0.3024 - val_accuracy: 0.7705 - val_auc: 0.9003 - val_loss: 0.4850\n",
      "Epoch 19/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9068 - auc: 0.9697 - loss: 0.3013 - val_accuracy: 0.7869 - val_auc: 0.8986 - val_loss: 0.4717\n",
      "Epoch 20/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9186 - auc: 0.9712 - loss: 0.2916 - val_accuracy: 0.7705 - val_auc: 0.9026 - val_loss: 0.4658\n",
      "Epoch 21/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9136 - auc: 0.9709 - loss: 0.2918 - val_accuracy: 0.7869 - val_auc: 0.9015 - val_loss: 0.4779\n",
      "Epoch 1/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7525 - auc: 0.8510 - loss: 0.6657 - val_accuracy: 0.7705 - val_auc: 0.8592 - val_loss: 0.6219\n",
      "Epoch 2/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8424 - auc: 0.9232 - loss: 0.5013 - val_accuracy: 0.7869 - val_auc: 0.8744 - val_loss: 0.5614\n",
      "Epoch 3/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8458 - auc: 0.9290 - loss: 0.4504 - val_accuracy: 0.8033 - val_auc: 0.8834 - val_loss: 0.5452\n",
      "Epoch 4/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8695 - auc: 0.9397 - loss: 0.4206 - val_accuracy: 0.8033 - val_auc: 0.8919 - val_loss: 0.5113\n",
      "Epoch 5/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8780 - auc: 0.9485 - loss: 0.3969 - val_accuracy: 0.8361 - val_auc: 0.9020 - val_loss: 0.4931\n",
      "Epoch 6/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8864 - auc: 0.9538 - loss: 0.3765 - val_accuracy: 0.8197 - val_auc: 0.9003 - val_loss: 0.4889\n",
      "Epoch 7/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8949 - auc: 0.9572 - loss: 0.3655 - val_accuracy: 0.8361 - val_auc: 0.9054 - val_loss: 0.4677\n",
      "Epoch 8/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9017 - auc: 0.9598 - loss: 0.3559 - val_accuracy: 0.8197 - val_auc: 0.9065 - val_loss: 0.4662\n",
      "Epoch 9/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8932 - auc: 0.9581 - loss: 0.3585 - val_accuracy: 0.8197 - val_auc: 0.9065 - val_loss: 0.4587\n",
      "Epoch 10/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9017 - auc: 0.9626 - loss: 0.3376 - val_accuracy: 0.8197 - val_auc: 0.9071 - val_loss: 0.4605\n",
      "Epoch 11/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9102 - auc: 0.9620 - loss: 0.3397 - val_accuracy: 0.8033 - val_auc: 0.9093 - val_loss: 0.4553\n",
      "Epoch 12/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9034 - auc: 0.9648 - loss: 0.3320 - val_accuracy: 0.8361 - val_auc: 0.9032 - val_loss: 0.4608\n",
      "Epoch 13/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9051 - auc: 0.9671 - loss: 0.3204 - val_accuracy: 0.8033 - val_auc: 0.9048 - val_loss: 0.4597\n",
      "Epoch 14/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9034 - auc: 0.9674 - loss: 0.3226 - val_accuracy: 0.8033 - val_auc: 0.9071 - val_loss: 0.4557\n",
      "Epoch 15/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9119 - auc: 0.9686 - loss: 0.3123 - val_accuracy: 0.8197 - val_auc: 0.9122 - val_loss: 0.4463\n",
      "Epoch 16/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9085 - auc: 0.9673 - loss: 0.3141 - val_accuracy: 0.8361 - val_auc: 0.9093 - val_loss: 0.4613\n",
      "Epoch 17/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9102 - auc: 0.9707 - loss: 0.3073 - val_accuracy: 0.8197 - val_auc: 0.9105 - val_loss: 0.4552\n",
      "Epoch 18/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9034 - auc: 0.9720 - loss: 0.2989 - val_accuracy: 0.7869 - val_auc: 0.9060 - val_loss: 0.4464\n",
      "Epoch 19/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9102 - auc: 0.9742 - loss: 0.2892 - val_accuracy: 0.8033 - val_auc: 0.9032 - val_loss: 0.4587\n",
      "Epoch 20/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9085 - auc: 0.9713 - loss: 0.3000 - val_accuracy: 0.7869 - val_auc: 0.9009 - val_loss: 0.4643\n",
      "Epoch 21/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9136 - auc: 0.9707 - loss: 0.3018 - val_accuracy: 0.7869 - val_auc: 0.9032 - val_loss: 0.4623\n",
      "Epoch 22/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9220 - auc: 0.9752 - loss: 0.2831 - val_accuracy: 0.8033 - val_auc: 0.8986 - val_loss: 0.4579\n",
      "Epoch 23/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9305 - auc: 0.9758 - loss: 0.2739 - val_accuracy: 0.8033 - val_auc: 0.9032 - val_loss: 0.4652\n",
      "Epoch 24/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9119 - auc: 0.9727 - loss: 0.2883 - val_accuracy: 0.7869 - val_auc: 0.9032 - val_loss: 0.4687\n",
      "Epoch 25/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9136 - auc: 0.9739 - loss: 0.2799 - val_accuracy: 0.7869 - val_auc: 0.9043 - val_loss: 0.4662\n",
      "Epoch 1/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7051 - auc: 0.7684 - loss: 0.7423 - val_accuracy: 0.7377 - val_auc: 0.8288 - val_loss: 0.6830\n",
      "Epoch 2/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8424 - auc: 0.9076 - loss: 0.5736 - val_accuracy: 0.7541 - val_auc: 0.8485 - val_loss: 0.5994\n",
      "Epoch 3/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8542 - auc: 0.9245 - loss: 0.4692 - val_accuracy: 0.7705 - val_auc: 0.8643 - val_loss: 0.5780\n",
      "Epoch 4/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8627 - auc: 0.9336 - loss: 0.4347 - val_accuracy: 0.7869 - val_auc: 0.8806 - val_loss: 0.5383\n",
      "Epoch 5/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8814 - auc: 0.9447 - loss: 0.4050 - val_accuracy: 0.8033 - val_auc: 0.8857 - val_loss: 0.5280\n",
      "Epoch 6/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8915 - auc: 0.9502 - loss: 0.3857 - val_accuracy: 0.8033 - val_auc: 0.8919 - val_loss: 0.5166\n",
      "Epoch 7/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8966 - auc: 0.9592 - loss: 0.3622 - val_accuracy: 0.8033 - val_auc: 0.8953 - val_loss: 0.5119\n",
      "Epoch 8/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8915 - auc: 0.9574 - loss: 0.3572 - val_accuracy: 0.8033 - val_auc: 0.8930 - val_loss: 0.5044\n",
      "Epoch 9/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9051 - auc: 0.9581 - loss: 0.3524 - val_accuracy: 0.7869 - val_auc: 0.8936 - val_loss: 0.5083\n",
      "Epoch 10/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9085 - auc: 0.9611 - loss: 0.3438 - val_accuracy: 0.8033 - val_auc: 0.8925 - val_loss: 0.4941\n",
      "Epoch 11/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9102 - auc: 0.9648 - loss: 0.3312 - val_accuracy: 0.7705 - val_auc: 0.8947 - val_loss: 0.4951\n",
      "Epoch 12/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9068 - auc: 0.9639 - loss: 0.3328 - val_accuracy: 0.7869 - val_auc: 0.8913 - val_loss: 0.4975\n",
      "Epoch 13/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9136 - auc: 0.9627 - loss: 0.3313 - val_accuracy: 0.7705 - val_auc: 0.8891 - val_loss: 0.4899\n",
      "Epoch 14/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9102 - auc: 0.9667 - loss: 0.3186 - val_accuracy: 0.7541 - val_auc: 0.8880 - val_loss: 0.5090\n",
      "Epoch 15/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9186 - auc: 0.9707 - loss: 0.3039 - val_accuracy: 0.7705 - val_auc: 0.8964 - val_loss: 0.4949\n",
      "Epoch 16/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9220 - auc: 0.9666 - loss: 0.3130 - val_accuracy: 0.7541 - val_auc: 0.8975 - val_loss: 0.4767\n",
      "Epoch 17/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9186 - auc: 0.9723 - loss: 0.3022 - val_accuracy: 0.7541 - val_auc: 0.8981 - val_loss: 0.4932\n",
      "Epoch 18/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9237 - auc: 0.9724 - loss: 0.2912 - val_accuracy: 0.7541 - val_auc: 0.8941 - val_loss: 0.5079\n",
      "Epoch 19/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9085 - auc: 0.9735 - loss: 0.2926 - val_accuracy: 0.7705 - val_auc: 0.8936 - val_loss: 0.4729\n",
      "Epoch 20/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9203 - auc: 0.9730 - loss: 0.2874 - val_accuracy: 0.7541 - val_auc: 0.8964 - val_loss: 0.5039\n",
      "Epoch 21/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9237 - auc: 0.9714 - loss: 0.2959 - val_accuracy: 0.7541 - val_auc: 0.8941 - val_loss: 0.4847\n",
      "Epoch 22/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9305 - auc: 0.9757 - loss: 0.2793 - val_accuracy: 0.7541 - val_auc: 0.8936 - val_loss: 0.4860\n",
      "Epoch 23/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9220 - auc: 0.9762 - loss: 0.2734 - val_accuracy: 0.7705 - val_auc: 0.8975 - val_loss: 0.4690\n",
      "Epoch 24/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9322 - auc: 0.9753 - loss: 0.2723 - val_accuracy: 0.7541 - val_auc: 0.8975 - val_loss: 0.4686\n",
      "Epoch 25/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9339 - auc: 0.9772 - loss: 0.2699 - val_accuracy: 0.7705 - val_auc: 0.8964 - val_loss: 0.4699\n",
      "Epoch 26/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9390 - auc: 0.9788 - loss: 0.2595 - val_accuracy: 0.7705 - val_auc: 0.8964 - val_loss: 0.4890\n",
      "Epoch 27/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9271 - auc: 0.9805 - loss: 0.2533 - val_accuracy: 0.8033 - val_auc: 0.8947 - val_loss: 0.4907\n",
      "Epoch 28/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9271 - auc: 0.9791 - loss: 0.2564 - val_accuracy: 0.7869 - val_auc: 0.8986 - val_loss: 0.4907\n",
      "Epoch 29/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9305 - auc: 0.9794 - loss: 0.2539 - val_accuracy: 0.7705 - val_auc: 0.9003 - val_loss: 0.4783\n",
      "Epoch 30/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9356 - auc: 0.9759 - loss: 0.2628 - val_accuracy: 0.7705 - val_auc: 0.9015 - val_loss: 0.4730\n",
      "Epoch 31/80\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9271 - auc: 0.9789 - loss: 0.2547 - val_accuracy: 0.7869 - val_auc: 0.8958 - val_loss: 0.4863\n",
      "Epoch 32/80\n",
      "\u001b[1m 1/19\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9062 - auc: 0.9824 - loss: 0.2585"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19640\\451259872.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Optional: Print the search space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtuner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch_space_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Run the search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m tuner.search(\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;31m# X_pretrain_train_res,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# y_pretrain_train_res,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mX_A_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    230\u001b[0m                 \u001b[1;31m# Oracle is calculating, resend request.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_run_and_update_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_and_update_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrial_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOMPLETED\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFatalError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mconfig_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEBUG\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_and_update_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m         if self.oracle.get_trial(trial.trial_id).metrics.exists(\n\u001b[0;32m    241\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuner_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTunerCallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[1;31m# Only checkpoint the best epoch across all executions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m             \u001b[0mcopied_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"callbacks\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m             \u001b[0mobj_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m             \u001b[0mhistories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhistories\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mfit\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         \"\"\"\n\u001b[0;32m    231\u001b[0m         \u001b[0mhp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[1;31m# Save the build config for model loading later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[1;33m`\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m`\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[0mIf\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m \u001b[1;33m`\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m`\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \"\"\"\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    419\u001b[0m                         \u001b[0msteps_per_execution\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps_per_execution\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m                         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m                     \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m                 val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m    424\u001b[0m                     \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m                     \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m                     \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_evaluating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 507\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbegin_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbegin_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\trainer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m             \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\compile_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m                 \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_weighted_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m                 \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\compile_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\metrics\\confusion_metrics.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1572\u001b[0m                 \u001b[0mvariable_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_thresholds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1574\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrue_positives\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1575\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfalse_positives\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1576\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrue_negatives\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1577\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfalse_negatives\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\numpy.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(shape, dtype)\u001b[0m\n\u001b[0;32m   7333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7334\u001b[0m     \u001b[0mReturns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7335\u001b[0m         \u001b[0mTensor\u001b[0m \u001b[0mof\u001b[0m \u001b[0mzeros\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7336\u001b[0m     \"\"\"\n\u001b[1;32m-> 7337\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(shape, dtype)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1265\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1266\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2590\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2591\u001b[1;33m     \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2592\u001b[0m     \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_zeros_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2593\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(shape, dtype, name, layout)\u001b[0m\n\u001b[0;32m   2651\u001b[0m         \u001b[1;31m# Happens when shape is a list with tensor elements\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2652\u001b[0m         \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2653\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2654\u001b[0m       \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Ensure it's a vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2655\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzero\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2656\u001b[0m   \u001b[1;32massert\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2657\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1265\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1266\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(dims, value, name, layout)\u001b[0m\n\u001b[0;32m    242\u001b[0m   \u001b[0mnumber\u001b[0m \u001b[0margument\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m`\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m`\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mvalid\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m`\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m`\u001b[0m \u001b[1;32mfor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m   \u001b[0mspecifying\u001b[0m \u001b[0ma\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mD\u001b[0m \u001b[0mshaped\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mTensorFlow\u001b[0m \u001b[0mdoes\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msupport\u001b[0m \u001b[0mthis\u001b[0m \u001b[0msyntax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mend_compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m   \"\"\"\n\u001b[1;32m--> 246\u001b[1;33m   result = d_api.call_with_layout(\n\u001b[0m\u001b[0;32m    247\u001b[0m       \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m   \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m   \u001b[0mshape_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\dtensor\\python\\api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(fn, layout, *args, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_dtensor_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_layout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mrelayout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(dims, value, name)\u001b[0m\n\u001b[0;32m   3621\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Fill\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3622\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3623\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3624\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3625\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3626\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3627\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3628\u001b[0m       return fill_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model_for_tuning,\n",
    "    objective='auc',  # Metric to MAXIMIZE during tuning\n",
    "    max_trials=30,        # Number of different models/hyperparameter combos to test\n",
    "    executions_per_trial=3, # Number of times to train each model to reduce randomness\n",
    "    directory='mlp_tuning', \n",
    "    project_name='heart_disease_pretrain'\n",
    ")\n",
    "\n",
    "# Optional: Print the search space\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Run the search\n",
    "tuner.search(\n",
    "    # X_pretrain_train_res, \n",
    "    # y_pretrain_train_res,\n",
    "    X_A_train, \n",
    "    y_A_train,\n",
    "    epochs=80, \n",
    "    validation_data=(X_A_val, y_A_val),\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"\\n--- Optimal Hyperparameters ---\")\n",
    "print(f\"Learning Rate: {best_hps.get('learning_rate')}\")\n",
    "print(f\"L2 Reg: {best_hps.get('l2_reg')}\")\n",
    "print(f\"Dropout Rate: {best_hps.get('dropout_rate')}\")\n",
    "print(f\"Neurons L1: {best_hps.get('neurons_l1')}\")\n",
    "\n",
    "# Retrieve the best model found\n",
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58be87d2-2bc6-4625-9aff-ebdb8ed66fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "best_params = best_hp.get_config()['values']\n",
    "\n",
    "print(\"--- Best Hyperparameters ---\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1df4cc-4459-498a-a4e1-6bc8757c079b",
   "metadata": {},
   "source": [
    "#### Pretraining on the source domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b6deece6-fe3b-4e0e-beed-38de643fc029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN THIS LINE IF YOU DO THE HYPERPARAMETER TUNING\n",
    "best_params = {\n",
    "    'learning_rate' : 0.01,\n",
    "    'l2_reg' : 0.0001,\n",
    "    'neurons_l1' : 96,\n",
    "    'dropout_rate' : 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "90ed9878-a295-4f60-b9fb-cc2c0715e139",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ feature_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ feature_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,656</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ feature_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,176</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ feature_layer_1 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)                  │           \u001b[38;5;34m1,056\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ feature_layer_2 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)                  │           \u001b[38;5;34m4,656\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ feature_layer_3 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)                  │           \u001b[38;5;34m1,176\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ output_layer (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m25\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,913</span> (27.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,913\u001b[0m (27.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,913</span> (27.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,913\u001b[0m (27.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_mlp_model(input_shape, best_params):\n",
    "    neurons_l1 = best_params['neurons_l1']\n",
    "    dropout_rate = best_params['dropout_rate']\n",
    "    l2_reg = best_params['l2_reg']\n",
    "    learning_rate = best_params['learning_rate']\n",
    "\n",
    "    inputs = Input(shape=(input_shape,))\n",
    "    x = Dense(neurons_l1, activation='relu', name='feature_layer_1', kernel_regularizer=keras.regularizers.l2(l2_reg))(inputs)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = Dense(int(neurons_l1/2), activation='relu', name='feature_layer_2', kernel_regularizer=keras.regularizers.l2(l2_reg))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = Dense(int(neurons_l1/4), activation='relu', name='feature_layer_3', kernel_regularizer=keras.regularizers.l2(l2_reg))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    outputs = Dense(1, activation='sigmoid', name='output_layer')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', AUC(name='auc')]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# input_dim = X_pretrain_train_res.shape[1]\n",
    "input_dim = X_A_train.shape[1]\n",
    "mlp_pretrain = create_mlp_model(input_dim, best_params)\n",
    "mlp_pretrain.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "654d4553-dfdd-48c9-a387-9d970de06b70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.8390 - auc: 0.9066 - loss: 0.4111 - val_accuracy: 0.7869 - val_auc: 0.9048 - val_loss: 0.4519\n",
      "Epoch 2/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8610 - auc: 0.9274 - loss: 0.3553 - val_accuracy: 0.8361 - val_auc: 0.9026 - val_loss: 0.3843\n",
      "Epoch 3/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8847 - auc: 0.9483 - loss: 0.3060 - val_accuracy: 0.8525 - val_auc: 0.9178 - val_loss: 0.3508\n",
      "Epoch 4/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9000 - auc: 0.9607 - loss: 0.2710 - val_accuracy: 0.8197 - val_auc: 0.9093 - val_loss: 0.3510\n",
      "Epoch 5/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8949 - auc: 0.9660 - loss: 0.2541 - val_accuracy: 0.8361 - val_auc: 0.9105 - val_loss: 0.3537\n",
      "Epoch 6/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9169 - auc: 0.9719 - loss: 0.2257 - val_accuracy: 0.8525 - val_auc: 0.9178 - val_loss: 0.3558\n",
      "Epoch 7/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9085 - auc: 0.9673 - loss: 0.2449 - val_accuracy: 0.8361 - val_auc: 0.9122 - val_loss: 0.3574\n",
      "Epoch 8/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9102 - auc: 0.9672 - loss: 0.2418 - val_accuracy: 0.8197 - val_auc: 0.9037 - val_loss: 0.4030\n",
      "Epoch 9/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9169 - auc: 0.9734 - loss: 0.2205 - val_accuracy: 0.8361 - val_auc: 0.9082 - val_loss: 0.3764\n",
      "Epoch 10/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9119 - auc: 0.9716 - loss: 0.2286 - val_accuracy: 0.8525 - val_auc: 0.9048 - val_loss: 0.3878\n",
      "Epoch 11/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9271 - auc: 0.9795 - loss: 0.1978 - val_accuracy: 0.8197 - val_auc: 0.9065 - val_loss: 0.4371\n",
      "Epoch 12/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9305 - auc: 0.9813 - loss: 0.1848 - val_accuracy: 0.8361 - val_auc: 0.9003 - val_loss: 0.4631\n",
      "Epoch 13/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9237 - auc: 0.9794 - loss: 0.1957 - val_accuracy: 0.8197 - val_auc: 0.8958 - val_loss: 0.4307\n",
      "Epoch 14/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9237 - auc: 0.9802 - loss: 0.1965 - val_accuracy: 0.8689 - val_auc: 0.9071 - val_loss: 0.3907\n",
      "Epoch 15/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9373 - auc: 0.9838 - loss: 0.1723 - val_accuracy: 0.8197 - val_auc: 0.8975 - val_loss: 0.4766\n",
      "Epoch 16/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9153 - auc: 0.9712 - loss: 0.2424 - val_accuracy: 0.8361 - val_auc: 0.8998 - val_loss: 0.4132\n",
      "Epoch 17/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9305 - auc: 0.9843 - loss: 0.1851 - val_accuracy: 0.8525 - val_auc: 0.8964 - val_loss: 0.4654\n",
      "Epoch 18/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9339 - auc: 0.9825 - loss: 0.1830 - val_accuracy: 0.8361 - val_auc: 0.8829 - val_loss: 0.4717\n",
      "Epoch 19/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9390 - auc: 0.9877 - loss: 0.1561 - val_accuracy: 0.8525 - val_auc: 0.9003 - val_loss: 0.4798\n",
      "Epoch 20/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9458 - auc: 0.9869 - loss: 0.1554 - val_accuracy: 0.8525 - val_auc: 0.8902 - val_loss: 0.5141\n",
      "Epoch 21/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9424 - auc: 0.9906 - loss: 0.1459 - val_accuracy: 0.8525 - val_auc: 0.8857 - val_loss: 0.5877\n",
      "Epoch 22/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9424 - auc: 0.9853 - loss: 0.1678 - val_accuracy: 0.8689 - val_auc: 0.8863 - val_loss: 0.5396\n",
      "Epoch 23/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9339 - auc: 0.9856 - loss: 0.1749 - val_accuracy: 0.8361 - val_auc: 0.9127 - val_loss: 0.4649\n",
      "Epoch 24/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9339 - auc: 0.9857 - loss: 0.1749 - val_accuracy: 0.8033 - val_auc: 0.8784 - val_loss: 0.6050\n",
      "Epoch 25/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9288 - auc: 0.9863 - loss: 0.1682 - val_accuracy: 0.8361 - val_auc: 0.9127 - val_loss: 0.3991\n",
      "Epoch 26/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9424 - auc: 0.9855 - loss: 0.1731 - val_accuracy: 0.8361 - val_auc: 0.8896 - val_loss: 0.5692\n",
      "Epoch 27/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9322 - auc: 0.9865 - loss: 0.1729 - val_accuracy: 0.8852 - val_auc: 0.9020 - val_loss: 0.4327\n",
      "Epoch 28/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9424 - auc: 0.9918 - loss: 0.1407 - val_accuracy: 0.8525 - val_auc: 0.8598 - val_loss: 0.6826\n",
      "Epoch 29/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9373 - auc: 0.9898 - loss: 0.1544 - val_accuracy: 0.8525 - val_auc: 0.8727 - val_loss: 0.5366\n",
      "Epoch 30/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9542 - auc: 0.9915 - loss: 0.1375 - val_accuracy: 0.8033 - val_auc: 0.8761 - val_loss: 0.5823\n",
      "Epoch 31/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9525 - auc: 0.9909 - loss: 0.1519 - val_accuracy: 0.8033 - val_auc: 0.8801 - val_loss: 0.5385\n",
      "Epoch 32/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9576 - auc: 0.9921 - loss: 0.1439 - val_accuracy: 0.8197 - val_auc: 0.8801 - val_loss: 0.6465\n",
      "Epoch 33/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9644 - auc: 0.9930 - loss: 0.1378 - val_accuracy: 0.8361 - val_auc: 0.8688 - val_loss: 0.6626\n",
      "Epoch 34/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9407 - auc: 0.9889 - loss: 0.1654 - val_accuracy: 0.8361 - val_auc: 0.8756 - val_loss: 0.7134\n",
      "Epoch 35/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9458 - auc: 0.9861 - loss: 0.1783 - val_accuracy: 0.8033 - val_auc: 0.8818 - val_loss: 0.5366\n",
      "Epoch 36/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9525 - auc: 0.9922 - loss: 0.1420 - val_accuracy: 0.7869 - val_auc: 0.8874 - val_loss: 0.5792\n",
      "Epoch 37/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9593 - auc: 0.9954 - loss: 0.1200 - val_accuracy: 0.8197 - val_auc: 0.8666 - val_loss: 0.7156\n",
      "Epoch 38/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9593 - auc: 0.9957 - loss: 0.1161 - val_accuracy: 0.8525 - val_auc: 0.8615 - val_loss: 0.5993\n",
      "Epoch 39/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9661 - auc: 0.9950 - loss: 0.1130 - val_accuracy: 0.8033 - val_auc: 0.8716 - val_loss: 0.6950\n",
      "Epoch 40/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9610 - auc: 0.9957 - loss: 0.1147 - val_accuracy: 0.7705 - val_auc: 0.8637 - val_loss: 0.6916\n",
      "Epoch 41/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9661 - auc: 0.9972 - loss: 0.1005 - val_accuracy: 0.8361 - val_auc: 0.8620 - val_loss: 0.9687\n",
      "Epoch 42/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9441 - auc: 0.9912 - loss: 0.1455 - val_accuracy: 0.7869 - val_auc: 0.8756 - val_loss: 0.6954\n",
      "Epoch 43/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9339 - auc: 0.9839 - loss: 0.1953 - val_accuracy: 0.8361 - val_auc: 0.8823 - val_loss: 0.5677\n",
      "Epoch 44/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9458 - auc: 0.9906 - loss: 0.1574 - val_accuracy: 0.8197 - val_auc: 0.8699 - val_loss: 0.5582\n",
      "Epoch 45/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9746 - auc: 0.9971 - loss: 0.1154 - val_accuracy: 0.7869 - val_auc: 0.8767 - val_loss: 0.6702\n",
      "Epoch 46/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9661 - auc: 0.9956 - loss: 0.1128 - val_accuracy: 0.7213 - val_auc: 0.8508 - val_loss: 0.7743\n",
      "Epoch 47/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9593 - auc: 0.9936 - loss: 0.1365 - val_accuracy: 0.8197 - val_auc: 0.8497 - val_loss: 0.8568\n",
      "Epoch 48/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9559 - auc: 0.9920 - loss: 0.1472 - val_accuracy: 0.7869 - val_auc: 0.8682 - val_loss: 0.7185\n",
      "Epoch 49/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9644 - auc: 0.9952 - loss: 0.1229 - val_accuracy: 0.8033 - val_auc: 0.8649 - val_loss: 0.7662\n",
      "Epoch 50/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9678 - auc: 0.9956 - loss: 0.1151 - val_accuracy: 0.8197 - val_auc: 0.8604 - val_loss: 0.7977\n",
      "Epoch 51/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9644 - auc: 0.9951 - loss: 0.1226 - val_accuracy: 0.8033 - val_auc: 0.8446 - val_loss: 0.9931\n",
      "Epoch 52/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9661 - auc: 0.9957 - loss: 0.1187 - val_accuracy: 0.7869 - val_auc: 0.8519 - val_loss: 0.8276\n",
      "Epoch 53/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9627 - auc: 0.9923 - loss: 0.1393 - val_accuracy: 0.7541 - val_auc: 0.8452 - val_loss: 1.0105\n",
      "Epoch 54/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9661 - auc: 0.9967 - loss: 0.1095 - val_accuracy: 0.8361 - val_auc: 0.8671 - val_loss: 0.9159\n",
      "Epoch 55/150\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9390 - auc: 0.9852 - loss: 0.1948 - val_accuracy: 0.7541 - val_auc: 0.8559 - val_loss: 0.7587\n"
     ]
    }
   ],
   "source": [
    "pretrain_history = mlp_pretrain.fit(\n",
    "    # X_pretrain_train_res, \n",
    "    # y_pretrain_train_res,\n",
    "    X_A_train, \n",
    "    y_A_train,\n",
    "    epochs=150, # Start with a reasonable number of epochs\n",
    "    batch_size=32,\n",
    "    # validation_data=(X_pretrain_val, y_pretrain_val),\n",
    "    validation_data=(X_A_val, y_A_val),\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=52, restore_best_weights=True)\n",
    "    ],\n",
    "    shuffle=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the pre-trained weights using the required extension\n",
    "mlp_pretrain.save_weights('mlp_pretrain.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "78806114-d3ef-4ba3-b5c8-e51be309a11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "Optimal Threshold (Max F1 on Validation): 0.9077\n",
      "Max F1 Score at this threshold: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Use the fine-tune validation set (X_finetune_val) to find the threshold\n",
    "y_val_proba = mlp_pretrain.predict(X_B_val).ravel()\n",
    "precision, recall, thresholds = precision_recall_curve(y_B_val, y_val_proba)\n",
    "\n",
    "# Calculate F1 score for all thresholds\n",
    "fscore = (2 * precision * recall) / (precision + recall + 1e-6) # Added 1e-6 to prevent division by zero\n",
    "# Find the threshold that yields the maximum F1 score\n",
    "ix = np.argmax(fscore)\n",
    "best_threshold = thresholds[ix]\n",
    "\n",
    "print(f\"Optimal Threshold (Max F1 on Validation): {best_threshold:.4f}\")\n",
    "print(f\"Max F1 Score at this threshold: {fscore[ix]:.4f}\")\n",
    "\n",
    "# Now, use best_threshold instead of 0.5 for final testing (e.g., y_pred = (y_pred_proba > best_threshold).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2295ba14-c5ca-45c7-a0b9-064a2ecea201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Pre-trained MLP on Dataset A (Source Domain) ---\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Pretraining Test Accuracy on Source domain: 83.87%\n",
      "Pretraining Test F1 on Source domain: 86.84%\n",
      "Pretraining Test Recall (Sensitivity) on Source domain: 89.19%\n",
      "Pretraining Test Specificity on Source domain: 76.00%\n",
      "Pretraining Test ROC-AUC on Source domain: 0.93%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Testing Pre-trained MLP on Dataset A (Source Domain) ---\")\n",
    "\n",
    "y_pred_proba_a = mlp_pretrain.predict(X_A_test).ravel()\n",
    "\n",
    "\n",
    "# y_pred_a = (y_pred_proba_a > best_threshold).astype(int)\n",
    "\n",
    "y_pred_a = (y_pred_proba_a > 0.5).astype(int)\n",
    "\n",
    "\n",
    "# Calculate Metrics\n",
    "accuracy_a = accuracy_score(y_A_test, y_pred_a) * 100\n",
    "f1_a = f1_score(y_A_test, y_pred_a) * 100\n",
    "roc_auc_a = roc_auc_score(y_A_test, y_pred_proba_a) * 100\n",
    "recall_a = recall_score(y_A_test, y_pred_a) * 100\n",
    "precision_a = precision_score(y_A_test, y_pred_a) * 100\n",
    "specificity_a = recall_score(y_A_test, y_pred_a, pos_label=0) * 100\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Pretraining Test Accuracy on Source domain: {accuracy_a:.2f}%\")\n",
    "print(f\"Pretraining Test F1 on Source domain: {f1_a:.2f}%\")\n",
    "print(f\"Pretraining Test Recall (Sensitivity) on Source domain: {recall_a:.2f}%\")\n",
    "print(f\"Pretraining Test Specificity on Source domain: {specificity_a:.2f}%\")\n",
    "print(f\"Pretraining Test ROC-AUC on Source domain: {roc_auc_score(y_pretrain_test, y_pred_proba_a):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1a0ba920-aab0-487d-b8c5-58d22703692f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Pre-trained MLP on Dataset B (Zero-Shot Transfer) ---\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Pretraining Test Accuracy on target domain: 77.42%\n",
      "Pretraining Test F1 on target domain: 78.79%\n",
      "Pretraining Test Recall (Sensitivity) on target domain: 92.86%\n",
      "Pretraining Test Specificity on target domain: 64.71%\n",
      "Pretraining Test ROC-AUC on target domain: 0.83%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Testing Pre-trained MLP on Dataset B (Zero-Shot Transfer) ---\")\n",
    "\n",
    "# Get raw probability predictions\n",
    "y_pred_proba_b = mlp_pretrain.predict(X_B_test).ravel()\n",
    "\n",
    "# Convert probabilities to binary class predictions\n",
    "# y_pred_b = (y_pred_proba_b > best_threshold).astype(int)\n",
    "\n",
    "y_pred_b = (y_pred_proba_b > 0.5).astype(int)\n",
    "\n",
    "\n",
    "# Calculate Metrics\n",
    "accuracy_b = accuracy_score(y_B_test, y_pred_b) * 100\n",
    "f1_b = f1_score(y_B_test, y_pred_b) * 100\n",
    "roc_auc_b = roc_auc_score(y_B_test, y_pred_proba_b) * 100\n",
    "recall_b = recall_score(y_B_test, y_pred_b) * 100\n",
    "precision_b = precision_score(y_B_test, y_pred_b) * 100\n",
    "specificity_b = recall_score(y_B_test, y_pred_b, pos_label=0) * 100\n",
    "\n",
    "print(f\"Pretraining Test Accuracy on target domain: {accuracy_b:.2f}%\")\n",
    "print(f\"Pretraining Test F1 on target domain: {f1_b:.2f}%\")\n",
    "print(f\"Pretraining Test Recall (Sensitivity) on target domain: {recall_b:.2f}%\")\n",
    "print(f\"Pretraining Test Specificity on target domain: {specificity_b:.2f}%\")\n",
    "print(f\"Pretraining Test ROC-AUC on target domain: {roc_auc_score(y_finetune_test, y_pred_proba_b):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e01e12-434e-48a8-bd31-fe2a5ae61bbf",
   "metadata": {},
   "source": [
    "### Finetuning on the target domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0cdd7167-26dd-4d0c-9a56-ca4e44086c76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prosp\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ feature_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ feature_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,656</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ feature_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,176</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ feature_layer_1 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)                  │           \u001b[38;5;34m1,056\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ feature_layer_2 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)                  │           \u001b[38;5;34m4,656\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ feature_layer_3 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)                  │           \u001b[38;5;34m1,176\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ output_layer (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m25\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,913</span> (27.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,913\u001b[0m (27.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,857</span> (22.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,857\u001b[0m (22.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> (4.12 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,056\u001b[0m (4.12 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def setup_fine_tuning(input_shape, pretrain_weights_path, best_params, fine_tune_lr=1e-1):\n",
    "    # Create the model with exactly the same architecture as pre-training\n",
    "    model_ft = create_mlp_model(input_shape, best_params)\n",
    "    \n",
    "    # Load pre-trained weights\n",
    "    model_ft.load_weights(pretrain_weights_path)\n",
    "\n",
    "    # Freeze layers for fine-tuning\n",
    "    model_ft.get_layer('feature_layer_1').trainable = False\n",
    "    model_ft.get_layer('feature_layer_2').trainable = True\n",
    "    model_ft.get_layer('feature_layer_3').trainable = True\n",
    "    \n",
    "    # Recompile with lower learning rate for fine-tuning\n",
    "    model_ft.compile(\n",
    "        optimizer=Adam(learning_rate=fine_tune_lr),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', AUC(name='auc')]\n",
    "\n",
    "    )\n",
    "    \n",
    "    finetune_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "    \n",
    "    return model_ft, finetune_scheduler\n",
    "\n",
    "\n",
    "mlp_finetune, finetune_scheduler = setup_fine_tuning(\n",
    "    input_shape=input_dim,  # Use the input dimension from Dataset B\n",
    "    pretrain_weights_path='mlp_pretrain.weights.h5', \n",
    "    best_params=best_params,  # Add best_trial here\n",
    "    fine_tune_lr=0.05\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Save the fine-tuned model weights (optional)\n",
    "mlp_finetune.save_weights('mlp_finetune_weights.weights.h5')\n",
    "\n",
    "# Check the model summary to ensure layers are frozen correctly\n",
    "mlp_finetune.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3fc5d9c6-5dbc-425f-b6e0-5d5891ec0c0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.7901 - auc: 0.8504 - loss: 0.5963 - val_accuracy: 0.6000 - val_auc: 1.0000 - val_loss: 0.4510\n",
      "Epoch 2/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7443 - auc: 0.8408 - loss: 0.5430 - val_accuracy: 0.9333 - val_auc: 0.9955 - val_loss: 0.2813\n",
      "Epoch 3/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8130 - auc: 0.8780 - loss: 0.4971 - val_accuracy: 0.9667 - val_auc: 0.9955 - val_loss: 0.2628\n",
      "Epoch 4/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8359 - auc: 0.8979 - loss: 0.4474 - val_accuracy: 0.9000 - val_auc: 0.9821 - val_loss: 0.2467\n",
      "Epoch 5/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8473 - auc: 0.9096 - loss: 0.4180 - val_accuracy: 0.9000 - val_auc: 0.9821 - val_loss: 0.2538\n",
      "Epoch 6/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8244 - auc: 0.9013 - loss: 0.4466 - val_accuracy: 0.9000 - val_auc: 0.9821 - val_loss: 0.2624\n",
      "Epoch 7/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8244 - auc: 0.9147 - loss: 0.4056 - val_accuracy: 0.8667 - val_auc: 0.9799 - val_loss: 0.2480\n",
      "Epoch 8/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8588 - auc: 0.9142 - loss: 0.4003 - val_accuracy: 0.8667 - val_auc: 0.9710 - val_loss: 0.3462\n",
      "Epoch 9/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8435 - auc: 0.9016 - loss: 0.4705 - val_accuracy: 0.9333 - val_auc: 0.9866 - val_loss: 0.3158\n",
      "Epoch 10/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8435 - auc: 0.9055 - loss: 0.4457 - val_accuracy: 0.9667 - val_auc: 0.9955 - val_loss: 0.2213\n",
      "Epoch 11/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8435 - auc: 0.9200 - loss: 0.4159 - val_accuracy: 0.9667 - val_auc: 0.9844 - val_loss: 0.2714\n",
      "Epoch 12/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8550 - auc: 0.9265 - loss: 0.3932 - val_accuracy: 0.9000 - val_auc: 0.9911 - val_loss: 0.2869\n",
      "Epoch 13/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8435 - auc: 0.9293 - loss: 0.3776 - val_accuracy: 0.9000 - val_auc: 0.9821 - val_loss: 0.3297\n",
      "Epoch 14/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8588 - auc: 0.9303 - loss: 0.4187 - val_accuracy: 0.9000 - val_auc: 0.9576 - val_loss: 0.3500\n",
      "Epoch 15/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8779 - auc: 0.9277 - loss: 0.3911 - val_accuracy: 0.9333 - val_auc: 0.9844 - val_loss: 0.2364\n",
      "Epoch 16/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8359 - auc: 0.9321 - loss: 0.3872 - val_accuracy: 0.9333 - val_auc: 0.9911 - val_loss: 0.2549\n",
      "Epoch 17/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8664 - auc: 0.9256 - loss: 0.4252 - val_accuracy: 0.9000 - val_auc: 0.9353 - val_loss: 0.4417\n",
      "Epoch 18/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8588 - auc: 0.9381 - loss: 0.3790 - val_accuracy: 0.9000 - val_auc: 0.9821 - val_loss: 0.3602\n",
      "Epoch 19/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8702 - auc: 0.9417 - loss: 0.3691 - val_accuracy: 0.9333 - val_auc: 0.9598 - val_loss: 0.4053\n",
      "Epoch 20/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8473 - auc: 0.9386 - loss: 0.3788 - val_accuracy: 0.9000 - val_auc: 0.9308 - val_loss: 0.4971\n",
      "Epoch 21/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8817 - auc: 0.9438 - loss: 0.3750 - val_accuracy: 0.9000 - val_auc: 0.9353 - val_loss: 0.4733\n",
      "Epoch 22/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8779 - auc: 0.9421 - loss: 0.3599 - val_accuracy: 0.9000 - val_auc: 0.9241 - val_loss: 0.5510\n",
      "Epoch 23/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8969 - auc: 0.9528 - loss: 0.3464 - val_accuracy: 0.9000 - val_auc: 0.9062 - val_loss: 0.9215\n",
      "Epoch 24/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8626 - auc: 0.9412 - loss: 0.3775 - val_accuracy: 0.9333 - val_auc: 0.9866 - val_loss: 0.3601\n",
      "Epoch 25/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8702 - auc: 0.9403 - loss: 0.3732 - val_accuracy: 0.9000 - val_auc: 0.9754 - val_loss: 0.3149\n",
      "Epoch 26/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8588 - auc: 0.9270 - loss: 0.4244 - val_accuracy: 0.9000 - val_auc: 0.9308 - val_loss: 0.5528\n",
      "Epoch 27/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8779 - auc: 0.9447 - loss: 0.3926 - val_accuracy: 0.9000 - val_auc: 0.9353 - val_loss: 0.5248\n",
      "Epoch 28/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8588 - auc: 0.9432 - loss: 0.3757 - val_accuracy: 0.8000 - val_auc: 0.9241 - val_loss: 0.5827\n",
      "Epoch 29/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8817 - auc: 0.9397 - loss: 0.3767 - val_accuracy: 0.8333 - val_auc: 0.9420 - val_loss: 0.6549\n",
      "Epoch 30/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8435 - auc: 0.9227 - loss: 0.4438 - val_accuracy: 0.9000 - val_auc: 0.9688 - val_loss: 0.3895\n",
      "Epoch 31/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8779 - auc: 0.9502 - loss: 0.3762 - val_accuracy: 0.8667 - val_auc: 0.9420 - val_loss: 0.6688\n",
      "Epoch 32/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8817 - auc: 0.9533 - loss: 0.3643 - val_accuracy: 0.8333 - val_auc: 0.9219 - val_loss: 0.7966\n",
      "Epoch 33/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8855 - auc: 0.9487 - loss: 0.3693 - val_accuracy: 0.8667 - val_auc: 0.9241 - val_loss: 0.6468\n",
      "Epoch 34/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8817 - auc: 0.9485 - loss: 0.3689 - val_accuracy: 0.9000 - val_auc: 0.9509 - val_loss: 0.6879\n",
      "Epoch 35/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8817 - auc: 0.9355 - loss: 0.4180 - val_accuracy: 0.9000 - val_auc: 0.9821 - val_loss: 0.3766\n",
      "Epoch 36/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8740 - auc: 0.9422 - loss: 0.3971 - val_accuracy: 0.8667 - val_auc: 0.9330 - val_loss: 0.5357\n",
      "Epoch 37/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8893 - auc: 0.9546 - loss: 0.3583 - val_accuracy: 0.8667 - val_auc: 0.9554 - val_loss: 0.5579\n",
      "Epoch 38/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8626 - auc: 0.9441 - loss: 0.3845 - val_accuracy: 0.9000 - val_auc: 0.9420 - val_loss: 0.5242\n",
      "Epoch 39/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8893 - auc: 0.9532 - loss: 0.3577 - val_accuracy: 0.9000 - val_auc: 0.9330 - val_loss: 0.5133\n",
      "Epoch 40/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8855 - auc: 0.9534 - loss: 0.3532 - val_accuracy: 0.9000 - val_auc: 0.9286 - val_loss: 0.5908\n",
      "Epoch 41/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8626 - auc: 0.9447 - loss: 0.3883 - val_accuracy: 0.8333 - val_auc: 0.9509 - val_loss: 0.8977\n",
      "Epoch 42/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9046 - auc: 0.9506 - loss: 0.3555 - val_accuracy: 0.8667 - val_auc: 0.9509 - val_loss: 0.7109\n",
      "Epoch 43/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8893 - auc: 0.9524 - loss: 0.3637 - val_accuracy: 0.9000 - val_auc: 0.9442 - val_loss: 0.8091\n",
      "Epoch 44/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8931 - auc: 0.9483 - loss: 0.3496 - val_accuracy: 0.9000 - val_auc: 0.9554 - val_loss: 0.7594\n",
      "Epoch 45/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8855 - auc: 0.9478 - loss: 0.3615 - val_accuracy: 0.9000 - val_auc: 0.9353 - val_loss: 0.7673\n",
      "Epoch 46/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8740 - auc: 0.9208 - loss: 0.4188 - val_accuracy: 0.9333 - val_auc: 0.9263 - val_loss: 0.3271\n",
      "Epoch 47/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8626 - auc: 0.9361 - loss: 0.3808 - val_accuracy: 0.9000 - val_auc: 0.9554 - val_loss: 0.4646\n",
      "Epoch 48/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9008 - auc: 0.9509 - loss: 0.3865 - val_accuracy: 0.9000 - val_auc: 0.9375 - val_loss: 0.4637\n",
      "Epoch 49/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8779 - auc: 0.9501 - loss: 0.3586 - val_accuracy: 0.9000 - val_auc: 0.9487 - val_loss: 0.5196\n",
      "Epoch 50/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8626 - auc: 0.9338 - loss: 0.4084 - val_accuracy: 0.8667 - val_auc: 0.9420 - val_loss: 0.4721\n",
      "Epoch 51/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8779 - auc: 0.9480 - loss: 0.3648 - val_accuracy: 0.8667 - val_auc: 0.9576 - val_loss: 0.5833\n",
      "Epoch 52/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8740 - auc: 0.9473 - loss: 0.3689 - val_accuracy: 0.9000 - val_auc: 0.9621 - val_loss: 0.4325\n",
      "Epoch 53/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9084 - auc: 0.9573 - loss: 0.3249 - val_accuracy: 0.8667 - val_auc: 0.9554 - val_loss: 0.8380\n",
      "Epoch 54/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8817 - auc: 0.9518 - loss: 0.3637 - val_accuracy: 0.9333 - val_auc: 0.9509 - val_loss: 0.5173\n",
      "Epoch 55/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8931 - auc: 0.9552 - loss: 0.3456 - val_accuracy: 0.8667 - val_auc: 0.9464 - val_loss: 0.3691\n",
      "Epoch 56/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8817 - auc: 0.9474 - loss: 0.3871 - val_accuracy: 0.8667 - val_auc: 0.9509 - val_loss: 0.3867\n",
      "Epoch 57/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8855 - auc: 0.9547 - loss: 0.3406 - val_accuracy: 0.9000 - val_auc: 0.9487 - val_loss: 0.4622\n",
      "Epoch 58/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8664 - auc: 0.9451 - loss: 0.3741 - val_accuracy: 0.8667 - val_auc: 0.9487 - val_loss: 0.5852\n",
      "Epoch 59/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8511 - auc: 0.9332 - loss: 0.4063 - val_accuracy: 0.8667 - val_auc: 0.9665 - val_loss: 0.4221\n",
      "Epoch 60/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8664 - auc: 0.9342 - loss: 0.4060 - val_accuracy: 0.9000 - val_auc: 0.9688 - val_loss: 0.3695\n",
      "Fine-tuning complete. The 'mlp_finetune' model is your final transfer model.\n"
     ]
    }
   ],
   "source": [
    "history_finetune = mlp_finetune.fit(\n",
    "    # X_finetune_train_res, \n",
    "    # y_finetune_train_res,\n",
    "    X_B_train, \n",
    "    y_B_train,\n",
    "    epochs=300, # Fewer epochs are usually needed for fine-tuning\n",
    "    batch_size=16, # Smaller batch size often works better for smaller datasets\n",
    "    validation_data=(X_B_val, y_B_val),\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True), \n",
    "    ],\n",
    "    shuffle=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning complete. The 'mlp_finetune' model is your final transfer model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "194686bb-1fa6-4c31-afd9-23ebc85b3a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "Optimal Threshold (Max F1 on Validation): 0.3637\n",
      "Max F1 Score at this threshold: 0.9655\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Use the fine-tune validation set (X_finetune_val) to find the optimal threshold ---\n",
    "y_val_proba_ft = mlp_finetune.predict(X_B_val).ravel()  # Raw probability predictions\n",
    "\n",
    "# Calculate precision, recall, and thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(y_B_val, y_val_proba_ft)\n",
    "\n",
    "# Calculate F1 score for all thresholds\n",
    "fscore = (2 * precision * recall) / (precision + recall + 1e-6)  # Adding 1e-6 to prevent division by zero\n",
    "\n",
    "# Find the threshold that yields the maximum F1 score\n",
    "ix = np.argmax(fscore)\n",
    "best_finetune_threshold = thresholds[ix]\n",
    "\n",
    "print(f\"Optimal Threshold (Max F1 on Validation): {best_finetune_threshold:.4f}\")\n",
    "print(f\"Max F1 Score at this threshold: {fscore[ix]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5c47083e-617e-4efb-9637-cf8d8aa52515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Finetune Test Accuracy on source domain: 77.42%\n",
      "Finetune Test F1 on source domain: 82.50%\n",
      "Finetune Test Recall (Sensitivity) on source domain: 89.19%\n",
      "Finetune Test Specificity on source domain: 60.00%\n",
      "Finetune Test ROC-AUC on source domain: 0.84%\n"
     ]
    }
   ],
   "source": [
    "# Get raw probability predictions\n",
    "y_pred_proba_a_ft = mlp_finetune.predict(X_A_test).ravel()\n",
    "\n",
    "# Convert probabilities to binary class predictions\n",
    "y_pred_a_ft = (y_pred_proba_a_ft > best_finetune_threshold).astype(int)\n",
    "\n",
    "# Calculate Metrics\n",
    "accuracy_a_ft = accuracy_score(y_A_test, y_pred_a_ft) * 100\n",
    "f1_a_ft = f1_score(y_A_test, y_pred_a_ft) * 100\n",
    "roc_auc_a_ft = roc_auc_score(y_A_test, y_pred_proba_a_ft) * 100\n",
    "recall_a_ft = recall_score(y_A_test, y_pred_a_ft) * 100\n",
    "precision_a_ft = precision_score(y_A_test, y_pred_a_ft) * 100\n",
    "specificity_a_ft = recall_score(y_A_test, y_pred_a_ft, pos_label=0) * 100\n",
    "\n",
    "print(f\"Finetune Test Accuracy on source domain: {accuracy_a_ft:.2f}%\")\n",
    "print(f\"Finetune Test F1 on source domain: {f1_a_ft:.2f}%\")\n",
    "print(f\"Finetune Test Recall (Sensitivity) on source domain: {recall_a_ft:.2f}%\")\n",
    "print(f\"Finetune Test Specificity on source domain: {specificity_a_ft:.2f}%\")\n",
    "print(f\"Finetune Test ROC-AUC on source domain: {roc_auc_score(y_pretrain_test, y_pred_proba_a_ft):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "78e22800-926d-4819-ba69-7ca5bf77dfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "Finetune Test Accuracy on target domain: 83.87%\n",
      "Finetune Test F1 on target domain: 83.87%\n",
      "Finetune Test Recall (Sensitivity) on target domain: 92.86%\n",
      "Finetune Test Specificity on target domain: 76.47%\n",
      "Finetune Test ROC-AUC on target domain:0.93%\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba_b_ft = mlp_finetune.predict(X_B_test).ravel()\n",
    "\n",
    "# Convert probabilities to binary class predictions using a 0.5 threshold\n",
    "y_pred_b_ft = (y_pred_proba_b_ft > best_finetune_threshold).astype(int)\n",
    "\n",
    "# Calculate Metrics\n",
    "accuracy_b_ft = accuracy_score(y_B_test, y_pred_b_ft) * 100\n",
    "f1_b_ft = f1_score(y_B_test, y_pred_b_ft) * 100\n",
    "roc_auc_b_ft = roc_auc_score(y_B_test, y_pred_proba_b_ft) * 100\n",
    "recall_b_ft = recall_score(y_B_test, y_pred_b_ft) * 100\n",
    "precision_b_ft = precision_score(y_B_test, y_pred_b_ft) * 100\n",
    "specificity_b_ft = recall_score(y_B_test, y_pred_b_ft, pos_label=0) * 100\n",
    "\n",
    "\n",
    "print(f\"Finetune Test Accuracy on target domain: {accuracy_b_ft:.2f}%\")\n",
    "print(f\"Finetune Test F1 on target domain: {f1_b_ft:.2f}%\")\n",
    "print(f\"Finetune Test Recall (Sensitivity) on target domain: {recall_b_ft:.2f}%\")\n",
    "print(f\"Finetune Test Specificity on target domain: {specificity_b_ft:.2f}%\")\n",
    "print(f\"Finetune Test ROC-AUC on target domain:{roc_auc_score(y_finetune_test, y_pred_proba_b_ft):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa41a579-48bb-43e7-a376-f650c63a5f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.family': 'Times New Roman', 'font.weight': 'bold', 'font.size' : '16'})\n",
    "\n",
    "# Confusion matrix\n",
    "cm_mlp = confusion_matrix(y_finetune_test, y_pred_b_ft)\n",
    "# Unravel the confusion matrix to get TN, FP, FN, TP\n",
    "tn, fp, fn, tp = cm_mlp.ravel()\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "im = plt.imshow(cm_mlp, cmap='Blues', interpolation='nearest') \n",
    "\n",
    "vmax = cm_mlp.max()\n",
    "colorbar_ticks = np.arange(0, vmax + 10, 10)\n",
    "plt.colorbar(im, ticks=colorbar_ticks)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "# Set X-axis labels (Predicted: Disease then No Disease)\n",
    "plt.xticks([0, 1], ['Disease', 'No Disease'])\n",
    "# Set Y-axis labels (Actual: Disease then No Disease)\n",
    "plt.yticks([0, 1], ['No Disease', 'Disease'], rotation=90)  # Horizontal labels\n",
    "\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "\n",
    "# Dynamic text color for contrast\n",
    "norm = mcolors.Normalize(vmin=cm_mlp.min(), vmax=cm_mlp.max())\n",
    "cmap = plt.colormaps['Blues']\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        cell_value = cm_mlp[i, j]\n",
    "        cell_color = cmap(norm(cell_value))\n",
    "        luminance = 0.299 * cell_color[0] + 0.587 * cell_color[1] + 0.114 * cell_color[2]\n",
    "        text_color = 'white' if luminance < 0.5 else 'black'\n",
    "        \n",
    "        plt.text(j, i, f'{cell_value}', ha='center', va='center', color=text_color, fontsize=24, fontweight='bold')\n",
    "\n",
    "# Save the confusion matrix image\n",
    "plt.savefig('confusion_matrix_mlp.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e93627-3128-4f21-a053-1e4f5f70dedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_mlp = mlp_finetune.predict(X_finetune_test)\n",
    "y_pred_prob_tabnet = tabnet_finetuned.predict(X_B_test)\n",
    "y_pred_prob_xgb = xgb_finetune.predict_proba(X_finetune_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958f0f19-fcd3-4444-b6ee-8111823392c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.family': 'Times New Roman', 'font.weight': 'bold', 'font.size': 18})\n",
    "\n",
    "# Get predicted probabilities for each model\n",
    "y_pred_prob_mlp = mlp_finetune.predict(X_finetune_test)\n",
    "y_pred_prob_tabnet = tabnet_finetuned.predict(X_B_test)  # Probabilities for the positive class (ROC)\n",
    "y_pred_proba_xgb = xgb_finetune.predict(X_finetune_test) \n",
    "\n",
    "# Calculate ROC curve and AUC for each model\n",
    "fpr_mlp, tpr_mlp, _ = roc_curve(y_finetune_test, y_pred_prob_mlp)\n",
    "fpr_tabnet, tpr_tabnet, _ = roc_curve(y_B_test, y_pred_prob_tabnet)\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_finetune_test, Dataset_B_Finetune)\n",
    "\n",
    "# Calculate AUC for each model\n",
    "roc_auc_mlp = auc(fpr_mlp, tpr_mlp)\n",
    "roc_auc_tabnet = auc(fpr_tabnet, tpr_tabnet)\n",
    "roc_auc_xgb = auc(fpr_xgb, tpr_xgb)\n",
    "\n",
    "# fpr, tpr, _ = roc_curve(y_finetune_test, Dataset_B_Finetune)\n",
    "\n",
    "# Also calculate using roc_auc_score for verification\n",
    "roc_auc_score_mlp = roc_auc_score(y_B_test, y_pred_prob_mlp)\n",
    "roc_auc_score_tabnet = roc_auc_score(y_B_test, y_pred_prob_tabnet)\n",
    "roc_auc_score_xgb = roc_auc_score(y_B_test, y_pred_proba_xgb)\n",
    "\n",
    "print(f\"MLP AUC (auc function): {roc_auc_mlp:.4f}\")\n",
    "print(f\"MLP AUC (roc_auc_score): {roc_auc_score_mlp:.4f}\")\n",
    "print(f\"TabNet AUC: {roc_auc_tabnet:.4f}\")\n",
    "print(f\"XGBoost AUC: {roc_auc_xgb:.4f}\")\n",
    "\n",
    "# Plot combined ROC curve\n",
    "plt.figure(figsize=(7, 6))\n",
    "\n",
    "# Plot each ROC curve\n",
    "plt.plot(fpr_mlp, tpr_mlp, color='red', lw=2, label=f'MLP (AUC = {roc_auc_mlp:.2f})')\n",
    "plt.plot(fpr_tabnet, tpr_tabnet, color='green', lw=2, label=f'TabNet (AUC = {roc_auc_tabnet:.2f})')\n",
    "plt.plot(fpr_xgb, tpr_xgb, color='orange', lw=2, label=f'XGBoost (AUC = {roc_auc_xgb:.2f})')\n",
    "\n",
    "# Diagonal line for random classifier\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', lw=2, linestyle='--')\n",
    "\n",
    "# Enable minor ticks for more grid lines\n",
    "plt.minorticks_on()\n",
    "\n",
    "# Customize grid settings for major and minor grid lines\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=1)\n",
    "\n",
    "# Adjust plot settings\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# plt.title('Combined ROC Curve for Fine-Tuned Models')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True, alpha=0.4)\n",
    "\n",
    "\n",
    "# Save the combined ROC curve\n",
    "plt.savefig('roc_curve_combined.png', dpi=300, bbox_inches='tight')\n",
    "# Show the plot\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
